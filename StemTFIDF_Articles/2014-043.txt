BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2014-043
Semiparametric Estimation with Generated Covariates
Enno Mammen* Christoph Rothe** Melanie Schienle***
* University of Mannheim, Germany ** Columbia University, United States of America
*** Leibniz Universität Hannover, Germany This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Semiparametric Estimation with Generated
Covariates
Enno Mammen, Christoph Rothe, and Melanie Schienle
University of Mannheim, Columbia University, and Leibniz University Hannover
Abstract We study a general class of semiparametric estimators when the infinite-dimensional nuisance parameters include a conditional expectation function that has been estimated nonparametrically using generated covariates. Such estimators are used frequently to e.g. estimate nonlinear models with endogenous covariates when identification is achieved using control variable techniques. We study the asymptotic properties of estimators in this class, which is a non-standard problem due to the presence of generated covariates. We give conditions under which estimators are root-n consistent and asymptotically normal, derive a general formula for the asymptotic variance, and show how to establish validity of the bootstrap.
JEL Classification: C14, C31 Keywords: Semiparametric estimation, generated covariates, profiling, propensity score
First Version: January 10, 2010. This Version: September 2, 2014. Enno Mammen, Department of Economics, University of Mannheim, D-68131 Mannheim, Germany. E-mail: emammen@rumms.uni-mannheim.de. Christoph Rothe, Department of Economics, Columbia University, 420 W 118th Street, New York, NY 10027, USA. E-mail: cr2690@columbia.edu. Melanie Schienle, School of Economics and Management, Leibniz University Hannover, Ko¨nigsworther Platz 1, D-30167 Hannover, Germany. E-mail: schienle@ewifo.uni-hannover.de. M. Schienle thanks Deutsche Forschungsgemeinschaft for research support via grant SCHI-1127 and for support of various research stays at the SFB 649 "Economic Risk".
1

1. Introduction
In this paper, we study the theoretical properties of semiparametric estimators when estimation of the nonparametric component requires the use of generated covariates. Such "three-step" estimators are for example frequently used to estimate nonlinear models with endogenous covariates when identification is achieved using control variable techniques. Here we consider a general class of semiparametric optimization estimators with a criterion function that depends on a conditional expectation function that has been estimated nonparametrically using generated covariates. The nonparametric component may be profiled and thus depend on unknown finite-dimensional parameters. Generated covariates may originate from an either parametric, semiparametric or nonparametric first step, and we allow the function that generates them to also serve some other purpose within the model.
Deriving asymptotic properties of estimators in this class is a non-standard problem due to the presence of generated covariates. The contribution of this paper is to give conditions on the primitives of the model under which these semiparametric "three-step" estimators are root-n consistent and asymptotically normal, to derive a general formula for the asymptotic variance, and to show how to establish validity of the bootstrap. We apply our methods to two substantial examples: estimation of average treatment effects via regression on the propensity score (Rosenbaum and Rubin, 1983), and estimation of production functions in the presence of serially correlated technology shocks (Olley and Pakes, 1996; Levinsohn and Petrin, 2003). In both cases, our paper is the first to give explicit conditions for root-n consistency and asymptotic normality of the respective estimator, and to establish validity of the bootstrap.
Semiparametric estimation problems involving both finite- and infinite-dimensional parameters are central to econometrics, and are studied extensively under general conditions by e.g. Newey (1994), Andrews (1994), Chen and Shen (1998), Ai and Chen (2003, 2007), Chen, Linton, and Van Keilegom (2003), Chen and Pouzo (2009), or Ichimura and Lee (2010). None of these papers explicitly considers the case of generated covariates in the nonparametric component, but one can easily see that the "high-level" conditions they employ are typically sufficiently abstract to
2

encompass the generation step. What needs to be adapted substantially, however, are the methods used to verify some of these abstract conditions in the context of a specific model. Compared to a standard analysis of a setting without generated covariates, the main difficulties occur when establishing a uniform rate of consistency for the nonparametric component (e.g. Newey, 1994, Assumption 5.1(ii); or Chen, Linton, and Van Keilegom, 2003, Condition (2.4)), and an asymptotic normality result for a linearized version of the objective function (e.g. Newey, 1994, Assumption 5.3 and Lemma 1; or Chen, Linton, and Van Keilegom, 2003, Condition (2.6)).
The main technical contribution of our paper is to provide two new stochastic expansions to verify such conditions. It thus establishes a connection between the extensive literature on estimation and inference in semiparametric models and the one on applications with generated covariates. Our expansions characterize the influence of generated covariates in the model's nonparametric component. We then show how to use this result to verify the above-mentioned uniform consistency and asymptotic normality conditions. Alternatively, our expansion could also be directly applied to a linearized version of the estimator. The expansions, which are proven using techniques from empirical process theory (e.g. Van der Vaart and Wellner, 1996; van de Geer, 2009), are related to results in Mammen, Rothe, and Schienle (2012) for purely nonparametric regression problems with generated covariates. The main difference is that in the present paper we derive sufficiently sharp bounds on weighted integrals of the remainder term instead of controlling its supremum norm. This requires substantially different mathematical methods. The new error bounds shrink at a considerably faster rate than those obtained in Mammen, Rothe, and Schienle (2012), which is critical for our development of a general theory of semiparametric estimation with generated covariates.
As a byproduct of the verification of the asymptotic normality condition mentioned above, we also obtain an explicit formula for the asymptotic variance of semiparametric estimators contained in the general class we consider. Compared to an infeasible procedure that uses the true values of the covariates, the influence function of such an estimator generally contains two additional terms: one that accounts for using generated covariates to estimate the nonparametric component, and one that accounts for the direct influence of generated covariates in other parts of the model, e.g.
3

through determining the point of evaluation of the infinite-dimensional parameter. Additionally, we obtain a characterization of cases under which these two adjustment terms exactly offset each other, and thus do not affect first-order asymptotic theory. Our methods can also be used to verify conditions under which a bootstrap procedure leads to asymptotically valid inference. The latter aspect can be important in many applications where the asymptotic variance is difficult to estimate.
Our paper is related to an extensive literature on models with generated covariates. To the best of our knowledge, Newey (1984) and Murphy and Topel (1985) were among the first to study the theoretical properties of such two-step estimators in a fully parametric setting. Pagan (1984) and Oxley and McAleer (1993) provide extensive surveys. Nonparametric regression with (possibly nonparametrically) generated covariates is studied by Mammen, Rothe, and Schienle (2012) under general conditions. See their references for a list of examples, and Andrews (1995), Song (2008) and Sperlich (2009) for related results. Examples of semiparametric applications with generated covariates include Olley and Pakes (1996), Heckman, Ichimura, and Todd (1998), Li and Wooldridge (2002), Levinsohn and Petrin (2003), Blundell and Powell (2004), Linton, Sperlich, and Van Keilegom (2008), Rothe (2009) and Escanciano, Jacho-Ch´avez, and Lewbel (2012), among many others. Song (2013) studies a class of semiparametric models with generated covariates that have a single-index structure, and shows that adaptive estimation is possible in such models under weak conditions (see also Song (2012)). Hahn and Ridder (2013) study the form of the influence function of semiparametric GMM-type estimators with generated covariates. They start with the
 assumption that the estimator is n-consistent and asymptotically linear (without explicitly specifying it), and then use arguments adapted from Newey (1994) to argue what the asymptotic variance of such a hypothetical estimator should be. In contrast, the focus of this paper is on giving explicit conditions that ensure that a concrete estimator is root-n consistent and asymptotic normal in the first place, and on showing how to establish validity of the bootstrap. Both aspects are important for implementing an estimator in practice. Escanciano, Jacho-Ch´avez, and Lewbel (2013) provide stochastic expansions for sample means of weighted residuals of semiparametric regressions with generated covariates, and certain uniform convergence results. Their results are useful for deriving
4

asymptotic properties of certain semiparametric regression-type estimators, where the nonparamet-

ric component affects the final estimator solely through its value at the generated covariates. They

also require a particular "index" condition, which can imply strong restrictions on the underlying

economic model and affect the form of the asymptotic variance. No such restriction is necessary

for our results

The remainder of the paper is structured as follows: In Section 2, we describe the class of

models and estimators we consider, and outline how to establish their asymptotic properties. In

Section 3, we present our main technical results: stochastic expansions that characterize the influ-

ence of generated covariates in the model's nonparametric component. Section 4 shows how these 
expansions can be used to verify classic conditions for n-consistency and asymptotic normality of

semiparametric estimators. In Section 5, we further study the asymptotic variance of our estima-

tors, and show how to establish validity of the bootstrap. In Section 6, we discuss two econometric

applications that make use of our results. All proofs and further details on the applications are

collected in the Appendix.

Throughout the paper, we use the notation that for any vector a  Rd the values amin =

min1jd aj and amax = max1jd aj denote the smallest and largest of its elements, respectively,

a+ =

d j=1

aj

denotes

the

sum

of

its

elements,

a-k

=

(a1,

.

..,

ak-1, ak+1,

.

. . , ad)

denotes

the

(d-1)-

dimensional subvector of a with the kth element removed, a! =

d j=1

aj

!

denotes

the

product

of

the factorial of the elements of the vector (in case the latter are all non-negative integers), and

ab =

d j=1

ajbj

for

any

vector

b



Rd.

Except

where

specifically

indicated

otherwise,

we

denote

the

cumulative distribution function (CDF) and the density function of a generic random vector X by

FX and fX , respectively.

2. Model, Estimation Procedure
We consider a general class of semiparametric optimization estimators where the criterion function depends on two types of infinite dimensional nuisance parameters: a conditional expectation function that has been estimated nonparametrically using generated covariates, and another estimated

5

function that is used to compute the generated covariates in a first step. We allow the criterion function to depend on the latter estimated function directly to accommodate settings where it also serves another purpose other than determining the shape of the conditional expectation function. Our setup covers both parametrically and nonparametrically generated covariates, as well as intermediate cases. It also allows for non-smooth criterion functions and profiled estimation of the nonparametric components.

2.1. Model. Let Z = (Y, X, W )  RdZ be a random vector defined on a complete probability space. Let   Rd denote a finite dimensional parameter space with generic element , and  = M × R be an infinite dimensional parameter space with generic element  = (m, r). Denote by 0   and 0(·, ) = (m0(·, ), r0(·))   the true values of the finite and infinite dimensional parameter, respectively.1 We assume that there exists a nonrandom function q : supp(Z)××  Rdq such that

Q(, 0(·, )) = E(q(Z, , 0(·, ))) = 0 if and only if  = 0.

The parametric component of our semiparametric model is thus identified via a moment condition. For simplicity, we also assume that for every    the objective function Q(, (·, )) depends on the nuisance parameter  through its value over some compact set IT × IR only, which is useful to later accommodate "fixed trimming" schemes into the estimation procedure. We also impose certain restrictions on the nature of the infinite dimensional parameter 0(·, ) = (m0(·, ), r0(·)). First, we assume that r0 can be identified from the distribution of a random subvector W of Z alone. This allows for a consistent estimate of r0 to be computed without knowledge of either 0 and m0. Note that while in many applications r0 will be a conditional expectation function, our setup does not require such a structure. Second, we assume that m0(·, ) is a conditional expectation function that depends on    and the true value r0 through the relationship

m0(·, ) = E(Y |T (X, , r0) = ·)

(2.1)

1Note that the notation 0(·, ) = (m0(·, ), r0(·)) is understood to mean that 0(a, b, ) = (m0(a, ), r0(b)) for

every conformable (a, b).

6

where T (X, , r) = t(X, r(Xr), ) is a random vector of dimension dT , Xr is a random subvector of X that contains the covariates entering the function r, and t : RdX × Rdr ×   RdT is a known function. The primary role of r0 is thus to generate (some of) the covariates used to compute the function m0. By allowing the shape of m0 to depend on X and r0(Xr) through a known transformation indexed by , our setup includes a broad class of index models that require profiling of the nonparametric component. Note that we allow r0 to enter the objective function Q(, 0(·, )) = Q(, (m0(·, ), r0(·))) directly to accommodate settings where r0 also serves a purpose other than determining the shape of m0. The fact that m0 is a functional of r0 is not imposed at the level of the criterion function Q. That is, the expression Q(, (m0(·, ), r(·))) is understood to mean Q(, (m¯ (·, r0, ), r(·))) and not Q(, (m¯ (·, r, ), r(·))), where m¯ (·, r, ) = E(Y |T (X, , r) = ·).
To make the notation more compact, we usually suppress the arguments of the infinite dimensional nuisance parameters, writing (, ) = (, m, r)  (, m(·, ), r(·)), (, 0) = (, m0, r0)  (, m0(·, ), r0(·)), and (0, 0) = (0, m0, r0)  (0, m0(·, 0), r0(·)). We also write T (, r)  T (X, , r), T ()  T (, r0), T (r)  T (0, r) and T  T (0, r0). We also write B = (tr(B AB))1/2 for any matrix B, where we suppress the dependence of the norm on the fixed symmetric positive definite matrix A for notational convenience.
2.2. Estimation Procedure. Given an i.i.d. sample {Z1, . . . , Zn} from the distribution of Z, a three-step semiparametric extremum estimator  of 0 can be constructed as follows. In the first step, we compute a (possibly nonparametric) consistent estimate r of r0. We do not require a specific procedure for this step, but will only impose certain "high-level" restrictions below that cover a wide range of methods. These include nonparametric estimators based on either kernels or sieves, and fully parametric procedures, as well as intermediate cases. Given an estimate of r0, we then compute an estimate of m0(·, ) for every    through a nonparametric regression of Y on the generated covariates T () = t(X, r(Xr), ) using p-th order local polynomial smoothing. Our
7

estimator is thus given by m(t, ) = , where


n

2

(, ) = argmin Yi -  -

u(Ti() - t)u Kh(Ti() - t) .

, i=1

1u+p

(2.2)

Here Kh(v) =

dT j=1

K(vj

/hj

)/hj

is

a

dT -dimensional

product

kernel

built

from

the

univariate

kernel function K, h = (h1, ..., hdT ) is a vector of bandwidths that tend to zero as the sample size n

tends to infinity, and 1u+p denotes the summation over all vectors u = (u1, . . . , up) of positive

integers with 1  u+  p. For p = 1, we get the usual local linear estimator, but our setup also

allows for uneven orders p > 1 for the purpose of bias control.2 We focus on local polynomial

estimation for m0(·, ) in this paper because the particular structure of the estimator facilitates

controlling the presence of generated covariates (see Mammen, Rothe, and Schienle, 2012), and

does not require a separate treatment of boundary regions. While it might be possible to conduct

a similar analysis for other nonparametric procedures, such as orthogonal series estimators, we

conjecture that this would require substantially more involved technical arguments. Finally, writing

(, ) = (, m(·, ), r(·)), we define the estimator  of 0 as any approximate solution to the problem

of minimizing a semiparametric GMM-type objective function:



Qn(, )

= inf


Qn(, )

+ oP (1/

n),

(2.3)

where Qn(, ) = (1/n)

n i=1

q(Zi,

,

).

Our estimator is a semiparametric procedure involving

generated covariates, in the sense that a preliminary estimate r of the nuisance parameter r0 is used

to compute the covariates entering the nonparametric regression procedure to estimate m0(·, ).

For the later asymptotic analysis, it will be useful to also consider an infeasible estimation

procedure that uses the true value r0 instead of an estimate r. Such an estimator  of 0 can be
2Note that the definition of the estimator m(·, ) in (2.2) implicitly requires the generated covariates to be continuously distributed (see also Assumption 1(ii) below). This is not restrictive, however, as it would be straightforward to modify the estimator m(·, ) by the usual frequency method if some covariates are in fact discrete. Also note that for the special case that the objective function Qn depends on m(·, ) through its values at the Ti() only, one could slightly simplify some technical arguments later by directly considering a "leave-one-out" version of m(·, ). Since our setup does not require such a structure, we proceed with the definition in (2.2).

8

obtained by first computing an estimate m(·, ) of m0(·, ) via nonparametric regression of Y on

T () for every   . That is, it is given by m(t, ) = , where


n

2

(, ) = argmin Yi -  -

u(Ti() - t)u Kh(Ti() - t).

, i=1

1u+p

One then defines  as an approximate minimizer of an infeasible version of the objective function:

 Qn(, ) = inf Qn(, ) + oP (1/ n)


(2.4)

where (, ) = (, m(·, ), r0(·)). In order to distinguish the two procedures, we refer to  and m in the following as the real estimators of 0 and m0, respectively, and to  and m as the corresponding oracle estimators.

2.3. General Approach for Asymptotic Analysis. We now describe our general strategy of our asymptotic analysis (a formal result is given in Theorem 5 at the end of Section 4). Our approach
 builds on an extensive literature that has given "high-level" sufficient conditions for n-consistency and asymptotic normality of semiparametric estimators. Examples include Newey (1994), Andrews (1994), Chen and Shen (1998), Ai and Chen (2003), Chen, Linton, and Van Keilegom (2003), or Ichimura and Lee (2010). A closer inspection of these conditions reveals that many of them can be verified irrespective of the presence of generated covariates, and thus through well-established (although still potentially highly involved) arguments. On the other hand, some of these sufficient conditions are strongly affected by the presence of generated covariates, and cannot be verified by standard techniques. Our main technical contribution in this paper is to develop stochastic expansions of nonparametric regression estimators for exactly this purpose.
We develop two types of results, which can be used to verify sufficient conditions for consistency and asymptotic normality of semiparametric estimators, respectively. For establishing consistency, the main problem that arises from the presence of generated covariates is to show that the nonparametric first-stage estimates are uniformly consistent. Under certain sufficient conditions that can be verified irrespective of the question whether generated covariates are present or not (see,

9

e.g., Newey (1994) or Chen, Linton, and Van Keilegom (2003); see also Appendix C), one can show that  p 0 if

 - 0  = oP (1),

(2.5)

where ·  denotes the pseudo-norm induced by the sup-norm on a class of continuous and bounded
functions, i.e. we have   = sup supx |m(x, )|+supxr |r(xr)|. In the following section, we provide new tools to verify a condition like (2.5), and then illustrate their application in Section 4.
 In comparison, establishing n-consistency and asymptotic normality of  turns out to be

more involved in the presence of generated covariates, as it requires stronger conditions on the

nonparametric first stage. Under certain sufficient conditions that can be verified irrespective of

the question whether generated covariates are present or not (see, e.g., Newey (1994) or Chen,

Linton, and Van Keilegom (2003); see also Appendix C), the estimator  satisfies the following

representation:

 n(

-

0)

=

-(Q0

AQ0)-1Q0

A

1 n

n

q(Zi, 0, 0) + nQ0[ - 0]

i=1

 + OP ( n

 - 0

2) + oP (1).

Here Q0 = Q(0, 0) denotes the ordinary derivative of Q(, ) with respect to  evaluated at

(0, 0), and for any ¯ such that 0 +  ¯   for | | sufficiently small we put Q0[¯] = Q(0, 0)[¯] =

lim0(Q(0, 0 +  ¯) - Q(0, 0))/ as the pathwise derivative of Q(0, ) at 0 in the direction

¯.3

This

representation

implies

that



is

 n-consistent and

asymptotically

normal

with

asymptotic

variance  = (Q0 AQ0)-1Q0 AV AQ0(Q0 AQ0)-1 if

 - 0  = oP (n-1/4)

(2.6)

3To make the notation clear, define m¯ (·, r, ) = E(Y |T (, r) = ·), and note that m0(·, ) = m¯ (·, r0, ). Then for any  = (m, r) the pathwise derivative Q0[] is defined as Q0[] = lim0(Q(0, m¯ (·, r0, 0) +  m(·), r0 +  r) - Q(0, m¯ (·, r0, 0), r0))/ , and not as Q0[] = lim0(Q(0, m¯ (·, r0 +  r, 0) +  m(·), r0 +  r) - Q(0, m¯ (·, r0, 0), r0))/ . See also Linton, Sperlich, and Van Keilegom (2008).

10

and

1 n

n

q(Zi, 0, 0) + nQ0[ - 0] -d N (0, V )

i=1

(2.7)

for some positive definite variance matrix V . Verifying these two conditions is technically much

more challenging than verifying (2.5). The estimate of the nuisance functions (·, ) = (m(·, ), r(·))

is required to be uniformly consistent with a particular rate, and a particular linear functional of

 - 0 needs to satisfy an asymptotic normality condition. In the following section, we provide new

tools for verifying both (2.6) and (2.7), and we illustrate their application in Section 4.

3. Stochastic Expansions of the Nonparametric Component
This section contains our main technical results. We consider a stochastic expansion of nonparametrically estimated regression functions under very general conditions, deriving a bound on both weighted averages and the supremum norm of the remainder term that is sufficiently sharp for our purposes. These are the key tools for verifying conditions (2.7) and (2.5)­(2.6), respectively, in applications.
3.1. Assumptions. We now state our assumptions on the data generating process and the preliminary estimator r of r0. We begin by defining the generalized regression residual () = Y -E(Y |T ()). This definition allows us to write the dependent variable Y as Y = m0(T (), )+() with E(()|T ()) = 0.
Assumption 1 (Regularity). We assume the following properties for the data distribution, the bandwidth, and kernel function K.
(i) The sample observations {Z1, . . . , Zn} are an independent and identically distributed sample from the distribution of Z.
(ii) The parameter space  is compact. For every   , the random vector T () = t(X, r0(Xr), ) is continuously distributed with support IT, satisfying IT  int(IT,) with IT compact. The
11

corresponding density function fT (x, ) has a continuous partial derivative with respect to x, and inf,xIT fT (x, ) > 0.
(iii) The function m0(u, ) has continuous partial derivatives of order p+1 with respect to u for all   .
(iv) There exist a constant C > 0 and some constant l > 0 small enough such that for every    the residuals () satisfy the inequality E(exp(l|()|)|T ())  C.
(v) The function K is twice continuously differentiable and satisfies the following conditions: K(u)du = 1, uK(u)du = 0, and K(u) = 0 for values of u not contained in some compact
interval, say [-1, 1].
(vi) The bandwidth h = (h1, . . . , hdT ) satisfies hj  n-j for all j = 1, . . . , dT , and (1 - +)/2 > max.
Most restrictions imposed in Assumption 1 are standard for nonparametric kernel-type estimators of nuisance functions in semiparametric models. Part (i) is not necessary and could be relaxed to allow for certain forms of temporal dependence, albeit at the cost of substantially more involved theoretical arguments. Part (ii) states that the covariates T () are continuously distributed, and that the density is bounded away from zero over some compact set IT . The latter condition ensures that m(·, ) is a stable estimate over IT . If it is known that the estimator  is consistent, the parameter set  can be replaced in the assumptions by a local neighborhood of the true parameter. Then assumption (ii) is reasonable if the support IT, smoothly changes with . The differentiability conditions in (iii) are used to control the magnitude of bias terms. Assuming subexponential tails of () conditional on T () in part (iv) is a regularity condition that is necessary to apply certain results from empirical process theory in our proofs. Note that conditions (ii)­(iv) involve the true function r0 only. Unlike Escanciano, Jacho-Cha´vez, and Lewbel (2013, Assumption 3), we do not assume that e.g. the vector T (, r) or the conditional expectation E(Y |T (, r)) have particular distributional or smoothness properties for values of r  R other than r0. Part (v) describes
12

a standard kernel function with compact support. Finally, the restrictions on the bandwidth in (vi) imply that the smoothing bias of the nonparametric regression estimator will be dominated by certain stochastic terms. As we will see from the next assumption, allowing the components of h to tend to zero at different rates can be useful in applications with multiple generated covariates that have different rates of convergence.
We remark that our setting can easily be extended to allow for random, data-dependent bandwidths. Allowing for a random bandwidth would only require to control the behavior of the mapping (t, )  m(t, ) as a function of h uniformly over some grid of bandwidth values that expands at a polynomial rate (Einmahl and Mason, 2005). To account for the presence of generated covariates, we are going to control the mapping (t, )  m(t, ) as a function of r uniformly over a much bigger space (see Assumption 3 below). Hence the extension to data-dependent bandwidths would cause no particular technical difficulties.
Assumption 2 (Accuracy). We assume the following properties of the estimator r: (i) sups |rj(s) - r0,j(s)| = OP (n-j) for some j > 0 and all j = 1, . . . , dr, and (ii) sup,x |Tj(x, , r) - Tj(x, , r0)| = oP (n-j ) for some j > j and all j = 1, . . . , dT ,
where in both cases the subscript j denotes the j-th component of the respective object.
Assumption 2 imposes restrictions on the accuracy of the first-step estimator r. Clearly, part (i) is a necessary condition for equation (2.5) to hold, and part (i) with j > 1/4 is necessary for condition (2.6). Part (ii) ensures that the difference between the respective components of T () and T () tend to zero in probability at a rate at least as fast as the corresponding bandwidth in the second stage of the estimation procedure, uniformly in . Such conditions can be verified for a wide range of nonparametric estimators (e.g. Masry (1996), Newey (1997)), and they trivially hold for regular parametric estimators.
Assumption 3 (Complexity). For every j = 1, . . . , dT , there exist a sequence of sets of functions Tn,j that satisfies the following properties:
13

(i) Pr((x, )  Tj(x, , r)  Tn,j)  1 as n  .
(ii) For some function rn satisfying sup,x |Tj(x, , rn) - Tj(x, , r0)| = o(n-j ) there exists a constant CT > 0 such that the set Tn,j = Tn,j  {Tj(·, r) : sup,x |Tj(x, , r) - Tj(x, , rn)|  n-j and r  R} can be covered by at most CT exp(-j nj ) balls with · -radius  for all   n-j , where 0 < j  2, j  R and ·  denotes the supremum norm.
Assumption 3 restricts the complexity of the function space in which the mapping (x, )  T (x, , r) takes its values by imposing constraints on the cardinality of the covering sets. Since we have that T (x, , r) = t(x, r(xr), ) for some known function t which, by Assumption 1(iii), is continuously differentiable with respect to its second component, the condition imposes implicit restrictions on the complexity of the first-stage estimator r. Indeed, if the function t has a partial derivative with respect to its second argument that is bounded and bounded away from zero, we could equivalently state a restriction similar to Assumption 3 on the set Rn = {r  R : Tj(·, r)  Tn,j for all j = 1, . . . , dT }.
Restrictions on covering numbers are a common requirement in the literature on empirical processes, that is typically fulfilled under suitable smoothness assumptions. Suppose for example that Rn is the set of smooth functions defined on the convex set IR  RdXr , whose partial derivatives up to order k exist and are uniformly bounded by some multiple of nj for some j  0, that lTj(x, r(xr), )/xr - lTj(x, r(xr), )/xr  Cl  -  for every , , every value of x and r, and every l  {0, . . . , k}, and that t has a absolutely bounded partial derivative with respect to its second argument. Then the set Tn,j = {(x, )  Tj(x, , r) : r  Rn} satisfies Assumption 3(ii) with j = dXr /k and j = jj (Van der Vaart and Wellner, 1996, Theorem 2.7.1). The same entropy bound applies if Rn consists of the sum of one fixed function and a smooth function from a respective smoothness class. This extension is useful if one chooses the fixed function as equal to the sum of r0 and the bias of r, and thus does not require the bias term to be a smooth function. For further discussion of entropy bounds and additional references we refer to van de Geer (2009).
For kernel-based estimators of r0, one can then verify Assumption 3(i) by explicitly calculating
14

the derivatives. Consider e.g. the one-dimensional Nadaraya-Watson estimator rn,j with bandwidth of order n-1/5 over some compact subset of the interior of the covariate's support where its density

is bounded away from zero. Choose rn,j equal to r0,j plus asymptotic bias term. Then one can

check

that

the

second

derivative

of

rn,j - rn,j

is

absolutely

bounded

by

 OP ( log n)

=

oP (nj )

for all j > 0 over some compact set in the interior of the support of the respective conditioning

variables. For sieve and orthogonal series estimators, Assumption 3(i) immediately holds when the

set Tn,j is chosen as the image of the sieve set or a subset of the linear span of an increasing number of basis functions, respectively, under the functional T (x, , ·). That is, if r(·) = Pk(n)(·)  for some   Rk(n) and Pk(n)(·) = (p1(·), . . . , pk(n)(·)) with a {pj}j=1 a complete basis for the space R, then Assumption 3(i) holds if we define Tn,j = {(x, )  T (x, , Pk(·) ) :   Rk(n)}. Note that in

settings where r0 is estimated by parametric or semiparametric methods verifying Assumption 3 is

generally much more simple, and substantially smaller values can be established for the constants

j and j. To state our final assumption, we define the "index bias" (X, ) = E(Y |X) - E(Y |T ()), which

is the difference between the conditional expectations of Y given the underlying dX -dimensional

covariate vector X and the dT -dimensional "index" T (), respectively.

Assumption 4 (Continuity). We assume that the elements of Rn = {r  R : Tj(·, r)  Tn,j for all j = 1, . . . , dT } satisfy the following properties for n large enough:

(i) For all r  Rn and    the function  B(t, , r) = E((X, )|T (r) = t) is p + 1 times differentiable with respect to its first argument, and the derivatives are uniformly bounded in absolute value.

(ii) For a constant CB > 0 and for r1, r2  Rn ,    it holds that

| B(T (r1), , r1) -  B(T (r2), , r2)|  CB T (r1) - T (r2) a.s.

15

(iii) For a constant CB > 0 and all r1, r2  Rn ,    and t  IT it holds that
E (T (, r1) - t)uh-uKh(T (, r1) - t) - E (T (, r2) - t)uh-uKh(T (, r2) - t)  CBn-min
for 0  u+  p.
Assumption 4(i)­(ii) are technical conditions which ensure that a conditional expectation of the "index bias" (X, ) satisfies certain smoothness restrictions. Under the additional assumption that the mapping r  T (r) is smooth, one important implication of these conditions and our previous assumptions is that the functional r  E(Y |T (X, , r) = ·) is uniformly continuous in a neighborhood of r0. To see this, drop the dependence on  for a moment, put  =  - (X), and write
E(Y |T (X, r)) = E(m0(T (r0)) - m0(T (r))|T (r)) + E(m0(T (r))|T (r)) + E(|T (r)) + E((X)|T (r)) = E(m0(T (r0)) - m0(T (r))|T (r)) + m0(T (r)) + E((X)|T (r)).
Continuity of this expression with respect to r then follows from our assumptions on smoothness of the mappings t  m0(t) and r  E((X)|T (r)). Note that our assumptions do not require the functional r  E(Y |T (X, , r) = ·) to be pathwise differentiable.
It is difficult to give more "low-level" conditions for Assumption 4(i)­(ii) in general, but there are certain settings where (X, ) = 0 and thus these conditions trivially hold . Examples of such settings include many instrumental variable models. In general, however, it is undesirable to impose that (X, ) = 0, and we do not require such a condition for our analysis. See our Section 6.1 below for an application where this flexibility is important.
Assumption 4(iii) is a further smoothness condition. If the random vector T (, r) is continuously distributed for every r, conditions (ii) and (iii) hold if one has appropriate bounds for f1 - f2  for r1, r2  Rn where for j = 1, 2 the term fj denotes the density function of either T (, rj) or of ((X, ), T (, rj)), and the density function T (, rj) is bounded away from zero uniformly over  on its support.
16

3.2. Main Results. Under the assumptions described in the previous subsection, we can

now derive a stochastic approximation of the nonparametric estimator m. To state the result, we

require some further notation. For any s  {0, 1, . . . , p} let ns =

s+dT -1 dT -1

be the number of distinct

dT -tuples u with u+ = s. Arrange these dT -tuples as a sequence in a lexicographical order with the

highest priority given to the last position, so that (0, . . . , 0, s) is the first element in the sequence

and (s, 0, . . . , 0) the last element. Let s denote this 1-to-1 mapping, i.e. s(1) = (0, . . . , 0, s), . . . , s(ns) = (s, 0, . . . , 0). For each s  {0, 1, . . . , p} we also define a ns ×1 vector wi,s(t, , r) with its kth element given by ((Ti(, r) - t)/h)s(k), and write wi(t, , r) = (1, wi,1(t, , r) , . . . , wi,p(t, , r) ) . Next, define Nh(t, , r) = E(wi(t, , r)wi(t, , r) Kh(Ti(, r)-t)) and let mpol(a, t, ) be the following

polynomial approximation of m0(a, ) in a neighborhood of t:

mpol(a,

t,

)

=

0u+p

1 u!

um0(t, ) tu11 . . . tudTdT

(a

-

t)u.

Finally, let mpol(a, t, ) denote the vector of partial derivatives of mpol(a, t, ) with respect to the

components of its first argument, write e1 = (1, 0, . . . , 0) for the first unit vector in RN , where N =

p s=0

ns,

put

Kh(v)

=

(Kh,1(v),

.

.

.

,

Kh,dT

(v))

with elements Kh,j(v) = (K (vj/hj)/h2j )

j=j K(vj /hj )/hj ,

and K derivative of K, and recall that (X, ) = E(Y |X) - E(Y |T ()). With this notation, we can

then define the approximating function m by

m(t, ) = m(t, ) + An (t, , r) + Bn (t, , r),

(3.1)

where

nA(t, , r) = e1 Nh(t, )-1E wi(t, , r)Kh(Ti() - t)mpol(Ti(r), t, )(Ti(, r) - Ti()) , Bn (t, , r) = e1 Nh(t, )-1E wi(t, , r)Kh(Ti() - t) (Ti(, r) - Ti())(Xi, )
for any r  Rn . The function m consists of two components: the term m(·, ) is the oracle estimator of m0(·, )
introduced above, whereas nA(t, , r)+Bn (t, , r) is an adjustment term that captures the additional uncertainty due to the presence of generated covariates. Note that the generated covariates enter

17

the expansion only through smoothed versions of the estimation error T (, r) - T (, r0). Since this additional smoothing typically improves the rate of convergence of the stochastic part of the

first-step estimator (although it does not improve the order of the bias component), we generally

expect the adjustment term to have a faster rate of convergence. Hence the dimensionality of the

generation step should play a less pronounced role in this context. Our main result concerns the

accuracy of using m as an approximation of m.

Theorem 1. Suppose that Assumption 1, 2(ii), 3 and 4 hold. Then uniformly for   , we have

(m(t, ) - m(t, ))(t)dt = oP (n-)

(3.2)

for any weight function  : RdT  R whose partial derivatives of order one are uniformly absolutely

bounded, and that satisfies (x) = 0 for all x / IT , and  < min{1, . . . , 4} with

1 =

1 2

+

(1

-

max 2

)min

-

(

+ )max , 2

2 = (p + 1)min + ( - )min,

3 = (2 -

max 2

)min

+

1 (1
2

-

+)

-

(

+ )max , 2

4 = 2min.

The theorem provides a bound on weighted averages of the approximation error m(t, ) -

m(t, ). We focus on such weighted averages of the approximation error because they are helpful when it comes to verifying conditions of the type (2.7). In particular, they can be shown to vanish faster than n-1/2 under reasonable conditions on the primitives of the model. On the other

hand, bounds on the supremum norm of the approximation error, as studied Mammen, Rothe, and Schienle (2012), typically vanish at a rate slower than n-1/2, and are thus not useful to establish

the "asymptotic normality" condition. They can however, with some adaptation, be employed to

verify the conditions (2.5) and (2.6), as explained below. For this purpose, we state the following

theorem, which is a variation of an earlier result in Mammen, Rothe, and Schienle (2012) that gives

a uniform rate of consistency of the estimator m(t, ). See also Escanciano, Jacho-Cha´vez, and

Lewbel (2013, Appendix B) for a related result.

Theorem 2 (Uniform Consistency). Suppose Assumption 1, 2(ii), 3 and 4(i)­(ii) hold. Then

sup |m(t, ) - m0(t, )| = OP n-(p+1)min + log(n)n-(1-+) + n-min + n- ,
tIT ,

18

where  < min{1, ..., 3} with 11
1 = 2 (1 - +) + ( - )min - 2 ( + )max, 2 = (p + 1)min + ( - )min, 3 = min + ( - )min.
Note that the first two terms in the error bound on the right hand side follow from a standard uniform consistency result of the oracle estimator m (Masry, 1996), whereas the remaining two terms are due to the presence of generated covariates. We remark that the rates given in Theorem 2 could be improved under additional restrictions on the form of the estimator r, such as those given in Assumption 5 below. See the remark at the end of the proof of Theorem 2 in Appendix A for details.

4. Application to Semiparametric Estimation
 In this section, we show how to use the technical results of the previous section to establish nconsistency and asymptotic normality of the semiparametric estimator  defined in (2.3). In particular, we show how to verify the uniform consistency conditions given in (2.5) and (2.6), and the asymptotic normality condition in (2.7). We begin with the former two uniform consistency conditions, as they are conceptually simpler to establish. Recall that our aim is to show that

 - 0  := sup |m(t, ) - m0(t, )| + sup |r(s) - r0(s)| = oP (an),

tIT ,

sIR

(4.1)

with either an = 1 (as in (2.5)) or an = n-1/4 (as in (2.6)). While a bound on the second supremum

term in the above equation is standard, a bound on first one follows from our Theorem 2.

Theorem 3. (i) Suppose that Assumption 1­3 and Assumption 4(i)­(ii) hold, and that + < min(1, 1 + 2( - )min - ( + )max). Then condition (2.5) holds. (ii) Suppose that the conditions of part (i) hold with j, j > 1/4 for all j, that min > 1/(4(p + 1)), and that + < min(1/2, 1/2 + 2( - )min - ( + )max). Then condition (2.6) holds.
We remark that part (i) of the theorem could also be shown if one would weaken Assumption 2 and allow for 0 < j < j, j = 1, . . . , dT , if one in turn ensures that -( - )min < min(min, (p +
19

1)min). The various conditions of the theorem involve a tradeoff between the complexity of the first

and second estimation step for the nonparametric component. They can be shown to be satisfied

when r0 is "sufficiently regular" (i.e. the j and j are small) and m0(·, ) is "sufficiently smooth"

(i.e. p is large and thus the j can be chosen small). Exact conditions are difficult to give in general,

but are easy to check for a specific application, where specific values for the j and j are available.

See the discussion after Assumption 3 above for an example.

To verify the asymptotic normality condition (2.7), we make use of the stochastic expansion

derived in Theorem 1. Recall that our aim is to show that

1 n

n

q(Zi, 0, 0) + nQ0[ - 0] d N (0, V )

i=1

for some positive definite variance matrix V . Given a specific estimator r of r0, the term m(t, )

defined in (3.1) can usually be calculated more explicitly, and then be used to verify this condition,

and to obtain a general formula for the variance matrix V . To illustrate this idea in a general setting,

suppose that the estimator used to generate the covariates satisfies the following asymptotically

linear representation, which is similar to conditions used e.g. in Rothe (2009) or Ichimura and

Lee (2010). The assumption can be shown to be satisfied for a wide range of nonparametric,

semiparametric, and fully parametric estimation procedures (we also discuss two representative

examples below).4

Assumption 5 (Linear Representation). The estimator r of r0 satisfies

1 r(s) - r0(s) = n

n

rni(s) + Rnr (s)

i=1

(4.2)

with rni(s) = Hn(Si, s)(Wi) for some Si, a random subvector of Wi, and supsIR |Rnr (s)| = oP (n-1/2). The term (Wi) satisfies E((Wi)|Si) = 0 and E((Wi)(Wi) ) < , and Hn is a

weighting function satisfying E( Hn(Si, Sj) 2) = o(n) for i = j.
4Note that Assumption 5 is typically not satisfied for estimators that are not asymptotically Gaussian, such as the

Maximum Score estimator of a single-index binary choice model, or other estimators that follow so-called cube-root

asymptotics. See Song (2013) for a further discussion of this point.

20

To see how this additional structure can be utilized for our purposes, recall that it follows from elementary rules for pathwise derivatives that
Q0[ - 0] = Qm(0, 0)[m - m0] + Qr(0, 0)[r - r0],
where for any (, r) the functional Qm(, )[m¯ ] is the pathwise derivative of Q(, (m, r)) at m in the direction m¯ , and similarly for Qr. As noted in Section 2, the notation is such that computing Qr does not involve computing the pathwise derivative of the functional r  E(Y |T (, r) = ·). In most applications, the structure of the criterion function Q(, ) = E(q(Z, , m, r)) is such that (with some abuse of notation) we have q(Z, 0, m0, r0) = q(Z, , m0(Zm), r0(Zr)). That is, the term q(Z, , m, r) only depends on the functions m and r through their value when evaluated at some random vectors Zm and Zr. Here Zm and Zr could be subvectors of the data Z, or known transformations thereof that might even involve m, r and  (for example, we could have Zr = Xr and Zm = T (X, r(Xr), )). All econometric applications we consider in Section 6 below exhibit this structure. Its most important implication is that the pathwise derivatives of the criterion function are of the form

with

Qm(0, 0)[m - m0] = m(z)(m(z) - m0(z))dFZm(z), Qr(0, 0)[r - r0] = r(z)(r(z) - r0(z))dFZr (z).

(4.3) (4.4)

m(zm) = E(q(Z, , m0, r0)/m0(Zm, 0)|Zm = zm) r(zr) = E(q(Z, , m0, r0)/r0(Zr)|Zr = zr).

Note that a representation like (4.3)­(4.4) with square integrable functions m and r also follows from the Riesz representation theorem under more general conditions (e.g. Newey, 1994).
If m and r are sufficiently smooth, one can use Assumption 5 together with our main stochastic expansion to show that there exist fixed functions j with E(j(Z)) = 0 and E(j(Z)j(Z) ) < 

21

for j = 1, 2, 3 such that

1 m(z)m(z, 0)dFZm(z) = n

n

1(Zi) + oP (n-1/2)

i=1

m(z)

nA(z, 0, r) + nB(z, 0, r)

1 dFZm(z) = n

n

2(Zi) + oP (n-1/2),

i=1

1 r(z) n

n

rni(z)dFZr (z)

=

1 n

n

3(Zi) + oP (n-1/2).

i=1 i=1

Moreover, the properties of the remainder term Rnm(t) = m(t, 0) - m(t, 0) established in Theo-

rem 1 ensure, under suitable regularity conditions, that

m(z)Rnm(z)dFZm (z) = oP (n-1/2).

If we now put 0(Zi) = q(Zi, 0, 0) and (z) =

3 j=0

j (z ),

the

above

statements

imply

that

1 n

n

q(Zi, 0, 0)

+

nQ0[

-

0]

=

1 n

n

(Zi) + oP (1)

i=1 i=1

Together with the Central Limit Theorem, the previous equation then implies that condition (2.7)

holds with V = E((Z)(Z) ). The following theorem formalizes this argument, and provides a general formula to compute the variance matrix V (we study the question how to explicitly

compute V in the following section). To state the result, define G(t) = m(t)fZm(t)/fT (t) and let G (t) = G(t)/t put T (r)(x) = t(x, 0, r0(xr))/r0(xr) and m(xr) = E(T (r)(X)((X)G (T ) + m0(T )G(T ))|Xr = xr).

Theorem 4. Suppose Assumption 1­ 5 hold with p + 1 > dT ,

(

+ )max 2

<

min{(1

-

max 2

)min,

(2

-

max 2

)min

+

1 2 (1 - +)},

(4.5)

the criterion function satisfies (4.3)­ (4.4) with m(·) and r(·) being (p + 1)-times continuously

differentiable, (2p + 2)-1 < j < (2dT )-1 for j = 1, . . . , dT , and let

0(Zi) = q(Zi, 0, 0)

1(Zi) = im(Ti)fZm (Ti)/fT (Ti)

2(Zi)

=

-

(Wi)

lim
n

E(m (Xr

)Hn

(Si

,

Xr

)|Si)

3(Zi)

=



(Wi

)

lim
n

E(r

(Zr

)Hn(Si

,

Zr

)|Si),

22

Then condition (2.7) holds with V = E((Z)(Z) ), where (z) =

3 j=0

j

(z).

Restriction (4.5) involves a tradeoff between the complexity of the first and second estimation step for the nonparametric component that is analogous to the one discussed after the statement of Theorem 3.
With the results of this section, and a result from Chen, Linton, and Van Keilegom (2003), we are now ready to state the following theorem, which formally states the asymptotic properties of our semiparametric two-step estimator with generated covariates.

Theorem 5. (i) Suppose that the conditions of Theorem 3(i) and Assumption C.1 in Appendix C

hold. Then  p 0. (b) Suppose that the conditions of Theorems 3(ii) and 4 and Assumption C.2 in

Appendix

C

hold.

Then

 n(

- 0)

d

N (0, ),

where



=

(Q0

AQ0 )-1 Q0

AV AQ0(Q0

AQ0)-1.

5. The Asymptotic Variance and the Bootstrap
In this section, we first provide some intuition for the form of the asymptotic variance of the estimator , and illustrate how to evaluate the general formulas in Theorem 4 and 5 for several settings. We then discuss conditions under which valid inference on 0 can be conducted via the bootstrap.

5.1. The Asymptotic Variance. The argument in the previous subsection conveys some important intuition for the form of the asymptotic variance of . Recall that under the conditions of Theorem 1 this variance is given by

 = (Q0 AQ0)-1Q0 AV AQ0(Q0 AQ0)-1

with V = E((Z)(Z) ) and (z) =

3 j=0

j

(z)

as

described

in

Theorem

4.

In contrast, the

asymptotic variance of the oracle estimator  can be shown to be

 = (Q0 AQ0)-1Q0 AV AQ0(Q0 AQ0)-1

with V = E((0(Z)+1(Z))(0(Z)+1(Z)) ), by simply setting r = r0. The presence of generated covariates thus affects the asymptotic variance only through the additional summands 2(Z) and

23

3(Z) used to calculate V , as the weight matrix A is chosen by the econometrician and Q0 is simply a population quantity. In particular, the term 2(Z) captures the additional uncertainty due to using generated covariates when estimating the function m0, whereas the term 3(Z) accounts for directly using the generated covariates in other parts of the model, e.g. as a point of evaluation of an estimated function. A simple condition for the presence of generated covariates to be asymptotically negligible, i.e. that  = , is then of course that 2(Z) = -3(Z) with probability one. See Hahn and Ridder (2013) for a similar result based on different arguments.
An important practical issue is how to explicitly calculate V in the context of a concrete semiparametric model. It seems difficult to construct an estimator of V based on the formula in Theorem 4 alone due to its high level of generality. However, in the context of a specific model a more explicit formula can usually be derived, and then used to construct a consistent sample analogue estimator of V (and thus of ). This requires explicit expressions for the various terms introduced in Assumption 5. We now give two examples for which this assumption is satisfied: the case where r0 is a conditional expectation function estimated by nonparametric regression, and the case where r0(xr) = r¯(xr, 0) is a function known up to a finite dimensional parameter 0, for which there exists a regular asymptotically linear estimator. These are arguably the most important cases from an applied point of view. We refer to Kong, Linton, and Xia (2010) for general results on kernel-based M-estimators.

Example 1 (Nonparametric Regression). Suppose that W is partitioned as W = (D, S), and

we have that D = r0(S) +  with E(|S) = 0. Consider a kernel-based nonparametric regression estimator r of r0, such as the Nadaraya-Watson or a local polynomial estimator. Then one can show that Assumption 5 holds under suitable smoothness conditions and choice of IR with (Wi) = i and Hn(Si, s) = fS,n(s)-1Lg(Si - s), where L is a kernel function and g is a bandwidth that tends to zero at an appropriate rate, and some fS,n(s)  fS(s) as n  . We then find that

2(Zi)

=

-im(Si

)

fXr (Si) fS (Si )

and

3(Zi)

=

i r (Si )

fZr (Si) fS (Si )

.

The form of 0(·) and 1(·) remains unchanged.

24

Example 2 (Nonlinear Parametric Estimation). Assume that r0(s) = r¯(s, 0) is a parametrically

specified function (not necessarily a conditional expectation) known up to the finite dimensional

parameter 0. Suppose there exists an estimator  of 0 that satisfies

1  - 0 = n

n

(Wi) + oP (n-1/2),

i=1

where E((W )) = 0, E((W )(W ) ) < , that r¯(xr, ) is continuously differentiable in its

second argument with derivative r¯ (xr, ) = r¯(xr, )/. Then Assumption 5 is satisfied with (Wi) = (Wi) and Hn(Si, s) = r¯ (s, 0), and thus

2(Zi) = -(Wi)E(T (r)(X)r¯ (Xr, 0)((X)G (T ) + m0(T )G(T ))) 3(Zi) = (Wi)E(r(Zr)r¯ (Zr, 0)),

with G(t) = m(t)fZm(t)/fT (t) and G (t) = G(t)/t. An important special case of this setting is the one where W is partitioned as W = (D, S), we have that D = r¯(S, 0) +  with E(|S) = 0 and E(2|S) < , and  is the nonlinear least squares estimator of 0. In such a setting, we would have that (Wi) = E(r¯ (S, 0)r¯ (S, 0) )-1r¯ (Si, 0)(Di - r0(Si)), under the usual regularity conditions.

Using results like those in Example 1­2, one can then derive the asymptotic variance  of a wide range of semiparametric estimators by calculating the functions m, m, and r. The following two examples give an explicit formula for  in particular classes of semiparametric estimators. These examples illustrate two important issues. First, they give some insight under which conditions the presence of generated covariates can be asymptotically negligible. Second, they show that the "index bias" (X) = E(Y |X) - E(Y |T ) appears explicitly in the asymptotic variance of a large class of estimators, and thus assuming that (X) = 0 as in Escanciano, Jacho-Ch´avez, and Lewbel (2013) can be restrictive.

Example 3 (Linear Estimator). Consider a setup where T (X, , r) = (X1, r(Xr)) and the parame-

ter of interest is 0 = E(s(m0(T ))) for some known function s, and thus the criterion function is of

the form Qn(, m, r) = n-1

n i=1

s(m((X1i,

r(Xri))))

-

.

This

setting

is

also

considered

in

Hahn

25

and Ridder (2013, Theorem 3). Suppose that r0 is a nonparametric regression function satisfying D = r0(Xr) +  with E(|Xr) = 0. Applying Theorem 4 as in Example 1 above, we find that the asymptotic variance of the estimator  is given by

 = E((1 + 2)(1 + 2) )

where, writing T = (X1, r0(Xr)),

1 = s(m0(T )) -  + s (m0(T )), 2 = -E(s (m0(T ))m(02)(T )(Y - E(Y |T ))|Xr)
with m(02)(t) the partial derivative of m0(t) with respect to the second component of t. In this simple setting, it is easy to give intuitive conditions under which the presence of generated covariates is asymptotically negligible. Note that the term 2 = 2(Z) + 3(Z) accounts for the estimation error from using an estimate of r0 instead of the actual function. This term is easily seen to be equal to zero if either s(·) is a linear function or if the index restriction E(Y |X) = E(Y |T ) holds.

Example 4 (Semiparametric Regression). Consider a setup where the objective function is of the

form Qn(, m, r) = n-1

n i=1

(Yi

-

m(T (Xi, , r), ))s(Xi)

for

some

known

function

s.

This type

of objective function occurs in many semiparametric regression problems, such as the estimation

of single- or multi-index models with generated covariates by semiparametric maximum likelihood

or semiparametric least squares (e.g. Rothe, 2009). Suppose again that the function r0 is a nonparametric regression function that satisfies D = r0(Xr) +  with E(|Xr) = 0 and E(2|Xr) < .

Applying Theorem 4 as in Example 1, we find that the asymptotic variance of the estimator  is

equal to

 = (Q0)-1E((1 + 2 + 3)(1 + 2 + 3) )(Q0)-1,

26

where, writing u(t) = E(s(X)|T = t) and u (t) = u(t)/t,
1 = (s(X) - E(s(X)|T )) 2 = -E((s(X) - E(s(X)|T ))m0(T )T (r)(X)|Xr) 3 = E(u (T )T (r)(X)(E(Y |X) - E(Y |T ))|Xr).
The terms 2 and 3 account for the estimation error from using an estimate of r0 instead of the actual function. In this setting there are generally no simple conditions under which the presence of generated covariates is asymptotically negligible. Still, the form of the asymptotic variance simplifies considerably if the index restriction E(Y |X) = E(Y |T ) holds, as 3 = 0 in this case.

5.2. Validity of the Bootstrap. In practice, inference based on combining an asymptotic

normality result with an estimate the asymptotic variance can be very complicated. Both V

and  could be difficult to estimate since they depend on the nonparametrically estimated com-

ponents of the model in a potentially nontrivial fashion. In such cases, resampling techniques

like the ordinary nonparametric bootstrap can be used for tasks like obtaining confidence re-

gions for the parameters of interest or critical values for certain hypothesis tests. Using re-

sults from Chen, Linton, and Van Keilegom (2003), our techniques can be used to establish

the validity of such an approach. Consider for example a setting where the sample and pop-

ulation objective function are of the form Qn(, ) = n-1

n i=1

q(Zi,

,

m(Zm,i,

),

r(Zr,i))

and

Q(, ) = E(q(Z, , m(Zm, ), r(Zr))), respectively. Let {Z1, . . . , Zn} be drawn with replacement

from the original sample {Z1, . . . , Zn}, let  be the same estimator as  but based on the boot-

strap data, and put Qn (, ) = n-1

n i=1

q(Zi,

,

m(Zm ,i,

),

r(Zr,i)).

Next, define the bootstrap

estimator  as any sequence that minimizes a GMM-type criterion function based on a recentered

moment condition:

Qn(, ) - Qn(, )

= inf


Qn(, ) - Qn(, )

 + oP (1/ n).

Sufficient conditions for the asymptotic validity of this bootstrap procedures were studied by Chen,

Linton, and Van Keilegom (2003). These conditions are mostly minor strengthenings of those in

27

Appendix C, that can be verified irrespective of the presence of generated covariates. However, there are also two conditions that are affected by the presence of generated covariates, which are the following variants of (2.6) and (2.7), respectively:

 -   = oP  (n-1/4)

(5.1)

and

1 n n

q(Zi, , ) - q(Zi, , ) + nQ0[ - 0] d N (0, V )

i=1

(5.2)

under the probability measure P  implied by bootstrap sampling. By adapting the discussion after

Theorem B in Chen, Linton, and Van Keilegom (2003) in an obvious fashion, and applying a result

from Gin´e and Zinn (1990), these two conditions can be verified in the same way we establish (2.6)

and (2.7) above, and are thus immediate for a wide range of applications. We thus obtain the

following result.

Theorem 6. (a) Suppose that (5.1)­(5.2) and Assumption C.3 in Appendix C hold. Then n( - ) converges in distribution to N (0, ) under the probability measure P  implied by bootstrap sam-

pling. (b) Under the conditions of Theorem 3(ii) and 4, the conditions (5.1) and (5.2) are fulfilled.

6. Econometric Applications
Semiparametric estimation problems with generated covariates occur in various fields of econometrics. In this subsection, we discuss two applications in greater detail: estimation of average treatment effects via regression on the propensity score, and estimation of production functions in the presence of serially correlated technology shocks. To save space, we only sketch the construction of estimators, and refer to Appendix B for details and regularity conditions. We also restrict attention to deriving asymptotic normality results, as showing consistency of the respective estimators only requires arguments that are very similar to those that would be used in the absence of generated covariates.

28

6.1. Regression on the Propensity Score. Consider the potential outcomes framework, which is commonly used in the extensive literature on program evaluation (Imbens, 2004): Let Y1 and Y0 be the potential outcomes with and without program participation, respectively, D  {0, 1} an indicator of program participation, Y = Y1D + Y0(1 - D) be the observed outcome, X a vector of exogenous covariates, and let (x) = Pr(D = 1|X = x) be the propensity score. A typical object of interest in this context is the average treatment effect (ATE), defined as

0 = E(Y1 - Y0).

Since selection into the program may be nonrandom, this object cannot be obtained by simply comparing the average outcomes of treated and untreated individuals. However, when selection into the treatment is unconfounded, biases due to nonrandom selection into the program can be removed by conditioning on the propensity score (Rosenbaum and Rubin, 1983). That is, the condition that Y1, Y0D|X implies that Y1, Y0D|(X). Moreover, writing d() = E(Y |D = d, (X) = ), we have that d() = E(Yd|(X) = ), and thus by the law of iterated expectations, the ATE is identified through the relationship

0 = E(1((X)) - 0((X))).

(6.1)

Similar arguments can be made for other measures of program effectiveness (e.g. Heckman, Ichimura, and Todd, 1998). Estimating the ATE by a sample analogue of (6.1) requires nonparametric estimation of the functions 1() and 0(). Since the propensity score is generally unknown and has to be estimated in a first stage, this fits into our framework with Z  (Y, X, (D, X)), r0(Xr)  (X), t(X, r0(Xr), )  (D, (X)), m0(z1)  d(p) and q(z, , m0, r0)  1((x)) - 0((x)) - .
Using the path-derivative approach of Newey (1994), Hahn and Ridder (2013) derived the form of the influence function of a hypothetical estimator in this problem that is assumed to satisfy an asymptotic linearity condition. Here we complement their result by giving explicit conditions for root-n consistency and asymptotic normality of a concrete estimator, which were thus far not known (Imbens, 2004). In particular, we consider the following sample version of (6.1) as a natural

29

estimate of the ATE:

1 =
n

n
(1((Xi)) - 0((Xi))),

i=1

where (x) is the q-th order local polynomial estimator of (x), and d() is the local linear

estimator of d(), computed using the first-stage estimates of the propensity score (alternatively,

we could consider a parametric estimator for the propensity score, such as Probit). Here the binary

covariate D is accommodated via the usual frequency method, i.e. the estimate d is computed

by local linear regression of Yi on (Xi) using the nd =

n i=1

I{Di

=

d}

observations

with

D

=

d

only. The following proposition gives the asymptotic properties of the estimator.

Proposition 1. Suppose that the regularity conditions given in Appendix B.1 hold. Then we have

that



p

0

and

 n(

-

0)

d

N (0, E((Y, D, X)2),

where

(Y,

D,

X)

=

µ1 (X )

-

µ0 (X )

+

D(Y

- µ1(X)) (X )

-

(1

-

D)(Y - µ0(X)) 1 - (X)

-

0

is the influence function, and µd(x) = E(Y |D = d, X = x) for d = 0, 1.

Under the conditions of the proposition the asymptotic variance of  equals the semiparametric efficiency bound for estimating 0, which was obtained by Hahn (1998). The estimator obtained via regression on the estimated propensity score thus has the same first-order limit properties as other popular efficient estimators of the ATE under unconfoundedness, such as the propensity score reweighting estimator of Hirano, Imbens, and Ridder (2003) or the estimator in Hahn (1998). Note that the flexibility of our Assumption 4 plays an important role for deriving this result. If we were to assume that the "index bias" is equal to zero in this application, we would in fact impose the restriction that d(x) = µd(x), and thus restrict the distribution of potential outcomes.

6.2. Estimation of Production Functions. When estimating the parameters of production functions, a simultaneity problem arises if there is contemporaneous correlation between a firm's inputs and shocks to productivity. In a highly influential paper, Olley and Pakes (1996) propose a methodology to address this issue, which can be seen as a control function approach. Here we consider a simplified version of their method, as described in Levinsohn and Petrin (2003). This
30

setting assumes that firms do not age and cannot be closed. The Cobb-Douglas model for log output Yt of a firm in period t is given by

Yt = 0 + LLt + K Kt + t + t,

(6.2)

where Lt and Kt are labor and capital inputs, respectively, t is a productivity index that follows a first-order Markov process, and t is an i.i.d. productivity shock. Here t and t are both unobserved. The main difference is that t is a state variable, and hence impacts the firm's input choices, while t has no impact on firm behavior. In particular, the firms' investment It in the capital stock is a function of t and Kt: It = t(t, Kt). Under suitable conditions, firms that choose to invest have investment functions that are strictly increasing in the unobserved productivity index, and hence by invertability t can be written as function of capital and investment

t = (Kt, It).

Substituting this relationship into (6.2), we find that

Yt = LLt + t + t,

(6.3)

where t = (Kt, It) = 0 + KKt + (Kt, It). Equation (6.3) is a standard partially linear model, and thus L and the function (·) are identified under standard regularity conditions. To identify the coefficient K, it is assumed that capital does not immediately respond to innovations in the productivity index t, which together with the Markov assumption implies that

t = (t-1) + t with E(t|t-1, Kt) = 0.

We can thus rewrite the output net of labor's contribution Yt = Yt - LLt as

Yt = K Kt + (t-1 - K Kt-1) + t,

(6.4)

with (x) = (x) + 0 and t = t + t. Note that while equation (6.4) resembles a partially linear model, its structure is actually somewhat different, as the coefficient K appears both in the linear part and inside the unknown function . It is thus not feasible to estimate K by some version of

31

the estimator proposed by Robinson (1988). Still, the parameter K can be characterized as the solution to a profiled nonlinear least squares problem:

K = argmin E(Yt - LLt - bKt - (t-1 - bKt-1|b))2,
b

(6.5)

where (c|b) = E(Yt - LLt - bKt|t-1 - bKt-1 = c) for any b  R. Implementing a sample analogue of (6.5) to estimate K requires nonparametric estimation of the function (·|b), which in turn requires an estimates of the coefficient L and the function (·). The latter two estimates can both obtained by estimating (6.3) in a first stage. This problem fits into our framework with

Z  (Yt, Lt, Kt, It, Kt-1, It-1), 0  K , r0(Xr)  (L, t-1), T (X, , r0)  t-1 -bKt-1, m0(·, )  (·|b) and q(Z, , m0, r0)  (Yt - LLt - bKt - (t-1 - bKt-1|b))(Kt - b(t-1 - bKt-1|b)Kt-1).
To give an explicit expression for an estimator K of K, let L and (·) be estimates of L and (·), respectively, obtained via the method in Robinson (1988). For every b  R, let (·|b) be an

estimate of (·|b), computed by local linear regression of Yit - LLit - bKit on (Ki,t-1, Ii,t-1) -

bKi,t-1. Then we can define the final estimator as

K

=

argmin
b

1 n

n
(Yit
i=1

-

LLit

-

bKit

-

((Ki,t-1, Ii,t-1)

-

bKit-1|b))2.

(6.6)

Note that computing both (·|b) and (·) involves a nonparametric regression with a generated

dependent variable. However, compared to the problems arising from the presence of generated

covariates, this issue is straightforward to address for linear smoothers like local linear regression.

To simplify the expression for the influence function, we introduce the following notation: Let

(b)(c|b) = a(a|b)|a=c + a(c|a)|a=b be the total derivative of (b|b) with respect to b, and  (c|b) = c(c|b) the ordinary derivative with respect to the first component. We also define Git = Kit - (b)(i,t-1 - K Ki,t-1|K )Ki,t-1 and the "projection residuals" Gt = Gt - E(Gt|t-1 - K Kt-1) and Lt = Lt - E(Lt|t-1 - K Kt-1).

Proposition 2. Suppose that the regularity conditions given in Appendix B.2 hold. Then we have

that

K

p

K

and

 n(K

- K )

d

N (0, )

with

 = Q0-1E (0 + 1 + 2)(0 + 1 + 2) Q0-1,

32

where
0 = Gtt 1 = -E(Gt |Kt-1, It-1) (t-1 - K Kt-1|K )t-1 2 = -E(Gt(Lt - E(Lt|Kt-1, It-1) (t-1 - K Kt-1|K )))
× E((Lt - E(Lt|Kt, It))2)-1(Lt - E(Lt|Kt, It))t.
Asymptotic properties of a somewhat more general version the above estimation procedure were first studied in Pakes and Olley (1995). Our expression for the influence function given in Proposition 2 differs from their result, even when taking into account that we only consider a simplified version of their model. This discrepancy seems to be due to a mistake in one of their derivations. Our formula for the asymptotic variance accounts both for the estimation error from using an estimate of (·) when estimating (·|b), and for the estimation error from using an estimate of (·) when evaluating (·|b). In our Proposition 2, both contributions are collected in the term 1. In contrast, the formula given in Pakes and Olley (1995) only accounts for the estimation error from using an estimate of (·) when evaluating (·|b).5

A. Proofs of Main Results

A.1. Proof of Theorem 1. To simplify notation, we give a detailed proof only for the special case

dT = 1, i.e. T = T (X, , r) is a univariate random variable, but we will shortly comment how rates change

for dT > 1. The proof for higher-dimensional T is conceptually similar. The following notation is used

throughout our proofs (some of it is simply a restatement of notation that we introduced before for the

special case dT = 1). The unit vector (1, 0, . . . , 0) in RN , where N =

p s=0

ns,

is

denoted

by

e1.

We

write

wi(t, , r) = (1, (Ti(r, ) - t)/h, ..., (Ti(r, ) - t)p/hp) ,

Mh(t, , r)

=

1 n

n

wi(t, r, )wi(t, r, )

Kh(Ti(r, ) - t),

i=1

m0(t, ) = (m0(t, ), hm0(t, )/2, ..., hpm0p(t, )/p!) .

5Hahn and Ridder (2013) mentioned a similar application in an early working paper version of their paper, but to

the best of our knowledge they did not derive an explicit expression for the influence function, or make any comparison

with Pakes and Olley (1995).

33

We also set wi(t, ) = wi(t, , r0) and wi(t, ) = wi(t, , r), and define Mh(t, ) and Mh(t, ) analogously. Finally, we put Nh(t, ) = E(Mh(t, )). Using () = () - (X, ), we can write
Yi = m0(Ti(), ) + i () + (Xi, ) .
Note that E(()|X) = 0 for any   . With this representation of the dependent variable, we define the following decompositions of both the real and the oracle estimator:

m(t, ) = mA(t, ) + mB(t, ) + mC (t, ) + mD(t, ) + mE(t, )
m(t, ) = mA(t, ) + mB(t, ) + mC (t, ) + mD(t, ) + mE(t, ),
with respective components mj(t, ) = e1 j(, r) and mj(t, ) = e1 j(, r0) defined for j  {A, B, C, D, E}
as follows:
n
A(t, , r) = argmin (i () -  wi(t, , r))2Kh(Ti(, r) - t),
 i=1 n
B(t, , r) = argmin (m0(Ti(, r0), ) - m0(t, ) wi(t, , r0) -  wi(t, , r))2Kh(Ti(, r) - t),
 i=1 n
C (t, , r) = argmin (m0(t, ) wi(t, , r0) - m0(t, ) wi(t, , r) -  wi(t, , r))2Kh(Ti(, r) - t),
 i=1 n
D(t, , r) = argmin (m0(t, ) wi(t, , r) -  wi(t, , r))2Kh(Ti(, r) - t),
 i=1 n
E(t, , r) = argmin ((Xi, ) -  wi(t, , r))2Kh(Ti(, r) - t).
 i=1
We also denote the component-wise differences between the real and the oracle estimator by

Rj,n(t, ) = mj(t, ) - mj(t, ) for j  {A, B, C, D, E}.

(A.1)

Finally, recall the definition that m(t, ) = m(t, ) + An (t, , r) + nB(t, , r) given in (3.1). The statement of the theorem follows if for any    the term Rn(t, ) = m(t, ) - m(t, ) satisfies
Rn(t, )(t) dt = oP (n- ) .

This is what we show in the following. To simplify the notation, we fix  = 0 for the rest of the proof and we omit  as an argument of functions. The proof can be easily extended to show that the results hold uniformly over   . To see this note that we show at several places that expansions hold uniformly over function classes. This can be easily extended to uniformity over the function class and   .

34

We will show that

RA,n(t)(t) dt = OP (n-1 ), RB,n(t)(t) dt = OP (n-2 ), RC,n(t)(t) dt = nA(t, r)(t) dt + OP (n-3 + n-4 ), RE,n(t)(t) dt = Bn (t, r)(t) dt + OP (n-1 + n-2 ).

(A.2) (A.3) (A.4) (A.5)

where the terms Rj,n are defined in (A.1) above. This directly implies the statement of the theorem since

(m(t) - m(t))(t) dt =
j{A,...,E}
and RD,n(t)  0 by construction.

Rj,n(t)(t) dt,

(A.6)

We start with the proof of (A.2). In the following, let c > 0 be some generic constant which can take differ-

ent values at each appearance. Furthermore Vn is a generic sequence of stochastically bounded random variables, again with different meaning at different appearances. Denote i(t, r) = e1 Mh(t, r)-1wi(t, r)Kh(Ti(r)- t) and write i(r) = i(t, r)(t) dt. Furthermore let Lh(Ti(r)-t) = Kh(Ti(r)-t)wi(t, r) be a vector-valued

kernel type function. Then it holds that

1 RA,n(t) = n

n

(i(t, r0) - i(t, r)) i .

i=1

Using elementary arguments, one can show that

Mh(Ti(r1), r1) - Mh(Ti(r2), r2) = Vnn|Ti(r1) - Ti(r2)| for r1, r2  Rn and 1  i  n. With the help of this bound, we find that, for r1, r2  Rn and 1  i  n,

|i(r1) - i(r2)|  e1 Mh(t, r1)-1Lh(Ti(r1) - t) - e1 Mh(t, r2)-1Lh(Ti(r2) - t) (t)dt = e1 Mh(Ti(r1) - hu, r1)-1(Ti(r1) - hu)
- e1 Mh(Ti(r2) - hu, r2)-1(Ti(r2) - hu) L(u)du ,  Vnn|Ti(r1) - Ti(r2)|.

For the case dT > 1, one gets by similar arguments that |i(r1) - i(r2)|  Vn max nj |Ti,j (r1) - Ti,j (r2)|.
1jdT

(A.7)

35

This last bound can be used to calculate a rough bound on the entropy Hn() of the class of functions Xi 

i(r). Here, exp(Hn()) denotes the number of balls with radius  that are necessary to cover the functions

Xi  i(r). Using Assumption 3, the class of functions Tj(·, r) can be covered by c exp((Vn-1n-j )-j nj )

balls of radius Vn-1n-j . Thus we find that the entropy Hn()  c

dT j=1

-j

Vnj

nj

j

+j

 c max1jdT -j njj+j for some constant c > 0. This implies

Cn

Hn1/2()d



V n-(1-max/2)min+(+)max/2
n

0

for Cn = n-min .

We now apply Theorem 8.13 in van de Geer (2009) with Z¯ = n-1

n i=1

Zi, ,

Zi,

=

i(r)i,



=

r,

R = Cn = n-min , and a is the entropy bound above. Conditional on observations X1, ..., Xn, we obtain an

exponential

bound

for

Z¯

uniformly

in

Rn

since

1 n

n i=1

E[exp(

|i|)|Ti]



C

with

probability

tending

to

one, for some constants C,  > 0 due to Assumption 1 (iv). With standard arguments this yields

sup
r1 ,r2 Rn

1 n

n
(i(r1)
i=1

-

i(r2))i

=

OP

n-(1/2)-(1-max /2)min +( +)max /2

.

(A.8)

Equation (A.8) then implies the desired result (A.2) because r^n  Rn with probability tending to one and

thus

P

RA,n(t)(t)dt



sup
r1 ,r2 Rn

1 n

n
(i(r1) - i(r2))i
i=1

 1 as n  .

For the proof of (A.3), note that for some non-negative integers a, b and constants C1, C2 > 0 it holds that m0(Ti(r)) - m0(t) wi(t, r)  C1n-(p+1)min and

1 n

n

Kh(Ti(r1) - t)wia,k(t, r1)wib,l(t, r1) - Kh(Ti(r2) - t)wia,k(t, r2)wib,l(t, r2)  C2n-(-)min

i=1

for components l, k and all t  IT and r, r1, r2  Rn. These two statements directly imply (A.3).

For the proof of (A.4), note that uniformly over 1  i  n, t  IT and r  Rn it holds that

m0(t) wi(t, r0) - m0(t) wi(t, r) = mpol(Ti(r), t)(Ti(r) - Ti(r0)) + OP (n-2min ).

Substituting this expression into RC,n, we find that

1 RC,n(t)(t)dt = n

n

i(r)(Ti(r) - Ti(r0)) + OP (n-2min ),

i=1

where

i (r) = e1 Mh(t, r)-1Lh(Ti(r) - t)mpol(Ti(r), t)(t)dt.

36

Furthermore, we have that

An (t, r)(t)dt

=

1 n

n

i (r0)(Ti(r) - Ti(r0)) + oP (n-1/2).

i=1

Thus, for (A.4) we have to show that

1 n

n
(i(r) - i(r0))(Ti(r) - Ti(r0)) = OP (n-3 + n-4 ).

i=1

(A.9)

Since |Ti(r) - Ti(r0)| = OP (n-min ) uniformly over r  Rn and 1  i  n, for (A.9) one only has to prove

that

|i(r) - i (r0)| = OP (nmin-3 + n )min-4

uniformly for r  Rn and 1  i  n. To see why the last claim holds, note that we can write:

i (r) - i (r0) = e1 [Mh(t, r)-1Lh(Ti(r) - t)mpol(Ti(r), t) - Mh(t, r0)-1Lh(Ti(r0) - t)mpol(Ti(r0), t)](t)dt
= e1 [Mh(Ti(r) - hu, r)-1(Ti(r) - hu)mpol(Ti(r), Ti(r) - hu) - Mh(Ti(r0) - hu, r0)-1(Ti(r0) - hu)mpol(Ti(r0), Ti(r0) - hu)]L(u)du.

First, it is easy to see that

max sup sup |(Ti(r) - t) - (Ti(r0) - t)| = OP (n-min ) and
1in rRn tIT

max
1in

sup
rRn

sup
tIT

|mpol(Ti(r),

Ti(r)

-

t)

-

mpol(Ti(r0),

Ti(r0)

-

t)|

=

OP

(n-min )

due to the smoothness of the functions involved. It thus remains to consider the elements of the matrix

Mh(Ti(r) - t, r) - Mh(Ti(r0) - t, r0). Any such element is of the form

1n n

(Ti(r) - t)uh-uKh(Ti(r) - t) - (Ti(r0) - t)uh-uKh(Ti(r0) - t)

i=1

for some 0  u+  p. We thus show that

1n n

(Ti(r) - t)uh-uKh(Ti(r) - t)

i=1

- (Ti(r0) - t)uh-uKh(Ti(r0) - t) = OP (nmin-3 + nmin-4 ).

(A.10)

uniformly over r  Rn. Because of Assumption 4(iii), we have that E (Ti(r) - t)uh-uKh(Ti(r) - t) - E (Ti(r0) - t)uh-uKh(Ti(r0) - t) = OP (n-min )

37

uniformly over r  Rn . Thus, for a proof of (A.10) it suffices to establish that

1n n

(Ti(r) - t)uh-uKh(Ti(r) - t) - E (Ti(r) - t)uh-uKh(Ti(r) - t)

i=1

- (Ti(r0) - t)uh-uKh(Ti(r0) - t) - E (Ti(r0) - t)uh-uKh(Ti(r0) - t)

= OP (nmin-3 + nmin-4 ).

The last claim follows from the same type of arguments used in the treatment of RA,n(t). Taken together, the above derivation shows that
RC,n(t)(t) dt = An (t, r)(t) dt + oP (n-3 + n-4 ),

as claimed

It remains to show (A.5). Note that

1 RE,n(t)(t) dt = n

n
[i(r) - i(r0)](Xi).

i=1

Using the same reasoning as in the treatment of RA,n(t), and Assumption 4(i)­(ii), we find that

1 n

n

i(r)((Xi) - E[(Xi)|Ti(r)]) - i(r0)((Xi) - E[(Xi)|Ti(r0)]) = OP (n-1 )

i=1

uniformly for r  Rn. Note that E[(Xi)|Ti(r0)] = 0. We now use that

1 n

n

i(r)E[(Xi)|Ti(r)]

=

1 n

n

i=1 i=1

e1 Mh(t, r)-1Lh(Ti(r) - t)E[(Xi)|Ti(r)](t)dt

= nB(t)(t)dt + OP (n-2 )

uniformly over r  Rn, and thus (A.5) holds. This concludes the proof of Theorem 1.

A.2. Proof of Theorem 2. First, standard results in e.g. Masry (1996), imply that the oracle estimator

m satisfies sup |m(t, ) - m0(t, )| = OP n-(p+1)min +
tIT ,
under the conditions of the theorem. Second, one can show that

log(n)n-(1-+)

.

sup |m(t, ) - m(t, )| = oP (n-).
tIT ,

(A.11)

The statement (A.11) is an extension of Theorem 1 in Mammen, Rothe, and Schienle (2012), which gives

a stochastic expansion of a local linear estimator regression estimator with generated covariates, and the

38

special case that T (x, r, ) = r(xr). Generalizing this result to higher order local polynomials and more general forms of T is conceptually straightforward, and thus a proof is omitted. With (A.11), the statement of the Theorem follows from a trivial bound on m(t, ) - m(t, ).

Remark 1. One could use the additional structure implied by Assumption 5 to prove a somewhat better

uniform rate of consistency under some minor additional regularity conditions. In particular, one can show

that

sup |m(t, ) - m(t, )| = OP (n-min n-(1-+) log n + n-2min ),
tIT ,

(A.12)

which is better than the rate of OP (n-min ) obtained from a crude bound that appears in Theorem 2.

A.3. Proof of Theorem 3 We only show part b) explicitly. Calculations for part a) are conceptually the same. By assumption, we have that ||r(s) - r0(s)|| = oP (n-1/4). Thus, it only remains to be shown that
||m(t, ) - m0(t, )|| = oP (n-1/4).
This is fulfilled if all three remaining terms in the error bound in Theorem 2 are of smaller order than n-1/4. For the two terms corresponding to the rate of convergence of the oracle estimator m, this is directly achieved for bandwidths larger than the stated lower bound, and such that + < 1/2. Such a bandwidth exists under sufficient smoothness conditions. The restriction that 1 > 1/4 imposes a binding restriction on the complexity of the sets Tn,j. It can be satisfied if + < 1/2 + 2( - )min - ( + )max.

A.4. Proof of Theorem 4. To prove this result, we first establish a linear stochastic expansion for

the oracle estimator m. Using arguments in Masry (1996), Kong, Linton, and Xia (2010) or Ichimura and

Lee (2010), one can show that

1 m(t, ) =
n

n

nmi(t, ) + O(n-(p+1)min ) + OP (log(n)n-(1-+)),

i=1

uniformly over t  IT and   , where

mni(t, ) = e1 Nh(t)-1w(Ti() - t)Kh(Ti() - t)i().

with w(t) = (1, t, ..., tp) and Nh(t, ) = E(w((Ti() - t)/h, )w((T () - t)/h, ) Kh(T () - t)). Next, note that the conditions of the Theorem imply that that O(n-(p+1)min ) = o(n-1/2) and OP (log(n)n-(1-+)) =

39

oP (n-1/2) and O(n-2min ) = oP (n-1/2). Applying Theorem 1, we therefore find that Q0 can be decomposed as follows:
Q0[ - 0] = A1 + A2 + A3 + A4 + oP (n-1/2),

where

A1 = A2 =

1 m(zm) n

n

mni(zm, 0)fZm (zm)dzm,

i=1

m(zm)nA(zm, 0, r)fZm (zm)dzm,

A3 = m(zm)nB(zm, 0, r)fZm (zm)dzm A4 = r(zr)rni(zr, 0, r)fZr (zr)dzr,

We deal with each of these four terms separately. First, applying standard arguments from kernel smoothing

theory, we find that

1n A1 = n i
i=1

e1 Nh(zm)-1w(Ti - zm)Kh(Ti - zm)m(zm)fZm (zm)dzm

1n

= n

i

i=1

e1 Nh(Ti - th)-1w(t)K(t)m(Ti - th)fZm (Ti - th)dt

1 =
n

n

im(Ti)fZm (Ti)/fT (Ti) + O(n-(p+1)min )

i=1

1 =
n

n

1(Zi) + oP (n-1/2)

i=1

For the second term, first note that it follows from standard bias calculations for kernel-type estimators that

m(zm)nA(zm, 0, r)fZm (zm)dzm

= -E

Ti(r)

(X

)(r(Xri

)

-

r0

(Xri

))m

(Ti

)m0

(Ti

)

fZm (Ti) fT (Ti)

+ OP (hp+1)

uniformly for fixed functions r  Rn . Substituting the expansion for r - r0 from Assumption 5 we then directly find that

A2

=

-1 n

n



(Wi

)

lim
n

E

i=1

T

(r)

(X

)m

(T

)m0

(T

)

fZm (T ) fT (T )

Hn

(Si

,

Xr

)

Si

+ OP (n-(p+1)min + n-2min ) + oP (n-1/2)

1 =
n

n

2A(Zi) + oP (n-1/2).

i=1

40

Concerning the term A3, we have that

A3 = =

m(zm) fT (zm)

Kh

(T

(x)

-

zm

)(T

(x)

-

T

(x))(x)fZm

(zm

)fX

(x)

dxdzm

1 h

K (t)G(T (x) + th) dt(T (x) - T (x))(x)fX (x) dx

= G (T (x))(T (x) - T (x))(x)fX (x) dx + O(hp+1)

=

G (T (x))T (r)(x)

1 n

n

Hn(Si, xr)(Wi)

(x)fX (x) dx + OP (hp+1 + n-2min )

i=1

=1 n

n



(Wi)

lim
n

E(G

(T )T (r)(X)Hn(Si, Xr)(X)|Si)

+

OP (n-(p+1)min

+

n-2min )

i=1

1 =
n

n

2B(Zi) + oP (n-1/2)

i=1

with G(t) = m(t)fZm (t)fT (t)-1 and G (t) = tG(t) using integration by parts to obtain the fourth equality.

Finally, we have

A4

=

(Wi)

lim
n

E(r (Xr )Hn (Si ,

Xr )|Si )

+

oP

(n-1/2)

1 =
n

n

3(Zi) + oP (n-1/2)

i=1

using the same type of arguments as the ones applied above. The statement of the Theorem then follows

since 2 = 2A + 2B.

A.5. Proof of Theorem 5. The statement of the theorem follows from the results on consistency and asymptotic normality of semiparametric two-step estimators in Chen, Linton, and Van Keilegom (2003).

A.6. Derivation of Example 1. Suppose that r0 is a q +1-times continuously differentiable regression function estimated by qth order local polynomial regression using a bandwidth g and a kernel function L.

Assume that S is continuously distributed with compact support IS, and that the corresponding density fS

is q-times continuously differentiable, bounded, and bounded away from zero on IS. Then it follows under some further standard regularity conditions (e.g. Kong, Linton, and Xia, 2010) that

r(s)

-

r0(s)

=

1 n

n

e1 NgS(s)-1w(Si - s)Lg(Si - s)i + OP (gq+1 + log(n)/(ngdS ))

i=1

uniformly over s  IS, w(t) = (1, t, ..., tq) as above and NgS(s) = E(w((Si - s)/g)w((Si - s)/g) Lg(Si - s)). The remainder term in the last equation can be made as small as oP (n-1/2) by choosing an appropriate

bandwidth if q is sufficiently large. It follows that Assumption 5 is satisfied with (Wi) = i and Hn(Si, s) =

41

e1 NgS(s)-1w(Si - s)Lg(Si - s). The condition that E( Hn(Si, Sj) 2) = o(n) holds if ngdS  . To obtain the explicit expressions for 2 and 3, we insert the above relation into the expression from Theorem 4 and apply standard U-Statistics arguments (e.g. Powell, Stock, and Stoker, 1989).
A.7. Derivation of Example 2. It easy to see that Assumption 5 is satisfied with (Wi) = (Wi) and Hn(Si, s) = r (s, 0) under the conditions given in the example. By substituting these expression into the general formulas in Theorem 4, one directly obtains the specific expressions for 2 and 3 given in the main text.

B. Details on Econometric Applications

B.1. Regression on the Propensity Score. In this section, we give details on the construction

of the estimator , and the regularity conditions under which Proposition 1 is valid. The data consist of a

sample {(Yi, Di, Xi), i = 1, . . . , n} from the distribution of (Y, D, X). The estimator of the propensity score

(x) = E(D|X = x) is given by (x) = , where

n

(, ) = argmin (Di -  -

u (Xi - x)u)2Lg(Xi - x)

, i=1

1u+ q

and Lg(s) =

dX j=1

L(sj /g)/g

is

a

dX -dimensional

product

kernel

built

from

the

univariate

kernel

L,

g

is

a

bandwidth, which for simplicity is assumed to be the same for all components, and 1u+q denotes the summation over all u = (u1, . . . , uq) with 1  u+  q. Next, for d  {0, 1} the estimate of d() = E(Y |D =

d, (X) = ) is given by the third-order local polynomial estimator: we set d() = d, where

n

(d, d) = argmin I{Di = d}(Yi -  -

v ((Xi) - )v)2Kh((Xi) - ) ,

, i=1

1v3

with Kh(u) = K(u/h)/h, K a one-dimensional kernel function and h a bandwidth that tends to zero as the

sample size n tends to infinity. The final estimator of 0 is then given by

1 =
n

n
(1((Xi)) - 0((Xi))).

i=1

To prove Proposition 1, we make the following assumptions.

Assumption 6. The sample observations {(Yi, Di, Xi), i = 1, . . . , n} are i.i.d.

Assumption 7. (i) The random vector X is continuously distributed with compact support IX . Its density function fX is bounded and bounded away from zero on IX , and is also q +1-times continuously differentiable

42

for some uneven number q  dX . (ii) The function (x) is bounded away from zero and one on IX , and is also q + 1-times continuously differentiable. (iii) For any d  {0, 1}, the random variable (X) is continuously distributed conditional on D = d, with compact support I. Its conditional density function f|D(·, d) is bounded and bounded away from zero on I, and is also four times continuously differentiable. (iv) For any d  {0, 1}, the function d() is four times continuously differentiable on I.
Assumption 8. The residual  = Y - E(Y |(X)) satisfies E[exp(l||)|X]  C almost surely for a constant C > 0 and l > 0 small enough.
Assumption 9. (i) The function K is twice continuously differentiable and satisfies the following conditions: K(u)du = 1, uK(u)du = 0, |u2K(u)|du < , and K(u) = 0 for values of u not contained in some
compact interval, say [-1, 1]. (ii) The function L is k-times continuously differentiable for some natural number k  max{2, dX/2}, and satisfies the following conditions: L(u)du = 1, uL(u)du = 1, and L(u) = 0 for values of u not contained in some compact interval, say [-1, 1].
Assumption 10. The bandwidths satisfy h  n- and g  n- with  = 1/(2q + 1) and 1/8 <  < (q + 2)/(8q + 4).
Proof of Proposition 1. The proof uses the same arguments as that of Theorem 4 and Example 1, and thus the details are omitted. The only issue is to show that we can choose  > 1/2. To see this, note that the conditions of the Proposition imply that Assumption 2 holds with  = (q + 1)/(4q + 2) > 1/4, and that Assumption 3 holds with   q/(q+1) and  = 0. The restrictions on  then ensure that - > (1/2)(+) and (1 - )/2 -  > (1/2)( + ). We then easily see that  > 1/2 can be chosen.

B.2. Estimation of Production Functions. In this section, we give details on the construction

of the estimator , and the regularity conditions under which Proposition 2 is valid. The data consist of a

sample {(Yit, Lit, Kit, Iit, Kit-1, Iit-1), i = 1, . . . , n} from the distribution of (Yt, Lt, Kt, It, Kt-1, It-1). As a

first step, we obtain an estimator L of L using the method in Robinson (1988). Under regularity conditions

given in that paper,

 n(L

-

L)

=

E((Lt

-

E(Lt|Kt,

It))2)-1

1 n

n
(Lit - E(Lit|Kit, Iit))it + oP (1).

i=1

Next, the estimator of (·) is given by (a, b) = , where

n

(, ) = argmin ((Yit - LLit) -  -

u ((Kit, Iit) - (a, b))u)2Lg((Kit, Iit) - (a, b)),

, i=1

1u+ q

43

and Lg(s) =

dX j=1

L(sj /g)/g

is

a

dX -dimensional

product

kernel

built

from

the

univariate

kernel

L,

g

is

a

bandwidth, which for simplicity is assumed to be the same for all components, and 1u+q denotes the

summation over all u = (u1, . . . , uq) with 1  u+  q. To simplify the exposition below, we also define an

infeasible estimator of (·) that uses the true value of the dependent variable. We set (a, b) = , where

n

(, ) = argmin ((Yit - LLit) -  -

r ((Kit, Iit) - (a, b))u)2Lg((Kit, Iit) - (a, b)).

, i=1

1u+ q

We also define t = (Kt, Lt). Next, for every b the estimator of (·|b) is given by the third-order local polynomial estimator (c|b) = , where

n

(, ) = argmin ((Yit - LLit - bKit) -  -

v(it-1 - bKit-1 - c)v)2Kh(it-1 - bKit-1 - c) ,

, i=1

1v3

with Kh(u) = K(u/h)/h, K a one-dimensional kernel function, and h a bandwidth that tends to zero as the sample size n tends to infinity. Again, we also define an infeasible estimator that uses the true value of the dependent variable. We set (c|b) = , where

n

(, ) = argmin ((Yit - LLit - bKit) -  -

v(it-1 - bKit-1 - c)v)2Kh(it-1 - bKit-1 - c) ,

, i=1

1v3

Our final estimator is then given as a solution to an empirical moment condition. Let

1n Mn(b) = n (Yit - LLit - bKit - (it-1 - bKit-1|b))(Kit - b(it-1 - bKit-1|b)Kit-1)
i=1

Then the final estimator K satisfies Mn(K ) = 0. To prove Proposition 2, we make the following assumptions. We remark that Assumption 12 follows

under standard regularity conditions for the estimation of partially linear models, see e.g. Robinson (1988).

Assumption 11. The sample observations {(Yit, Lit, Kit, Iit, Kit-1, Iit-1), i = 1, . . . , n} are i.i.d.

Assumption 12. The estimator L satisfies

 n(L

-

L)

=

E((Lt

-

E(Lt|Kt,

It))2)-1

1 n

n
(Lit - E(Lit|Kit, Iit))it + oP (1).

i=1

Assumption 13. (i) The random vector St-1 = (Kt-1, It-1) is continuously distributed with compact

support IS. Its density function fS is bounded and bounded away from zero on IS, and is also q + 1-times

continuously differentiable for some uneven number q  3. (ii) The function (s) is q + 1-times continuously

differentiable. (iii) Suppose that K  int(B) for some known compact set B. For any b  B, the random

variable Tt-1(b) = (St-1) - bKt-1 is continuously distributed with compact support IT . Its density function

44

fT (·, b) is bounded and bounded away from zero on IT , uniformly over b  B. The density is also four times continuously differentiable. (iv) For any b  B, the function (·, b) is four times continuously differentiable on IT .
Assumption 14. For any b  B, the residual (b) = (Yt-LLt-bKt)-(Tt-1(b)|b) satisfies E[exp(l|(b)|)|St-1]  C almost surely for a constant C > 0 and l > 0 small enough.
Assumption 15. Assumptions 9(i) and 10 hold. Furthermore, Assumption 9(ii) holds for all values k  2.
Proof of Proposition 2. Again, we can use the same arguments as that of Theorem 4 and Example 1 to show this result. To show that  > 1/2 under the conditions of the proposition is possible, we proceed as in the proof of Proposition 1. To derive the influence function, it is useful to note that (4.3)­(4.4) hold with
m(c) = -E(Gt|Tt-1 = c) r(c1, c2) = -(E( (Tt-1)Gt|St-1 = c1), E(Gt|Lt = c2)) .
Moreover, the proof uses that
t-1 = t-1 - E(Lt-1|Kt-1, It-1)(L - 0) + oP (n-1/2) (c|b) = (c|b) - (L - L)E(Lt|t-1 - bK,t-1 = c) + oP (n-1/2).
This follows directly from the linearity of the local polynomial smoothing operator.
C. Additional Assumptions
In this section, we state Assumption C.1­C.3, which collect those conditions of Theorem 5 and Theorem 6 that can be verified irrespective of the question whether the function m0 is estimated using generated covariates or not. The assumptions are all minor variations of those given in Chen, Linton, and Van Keilegom (2003), and
 could be replaced by similar conditions considered in other papers studying n-consistency and asymptotic normality of semiparametric "plug-in" estimators, such as Newey (1994).
Throughout the section, we use the following notation. For some small  > 0, we define  = {   : -0  } and  = {   :  -0   }. For any (, )  ×, we also denote the ordinary derivative of Q(, ) with respect to  by Q(, ). For any   , we say that Q(, ) is pathwise differentiable at    in the direction ¯ if there exists a continuous linear functional Q(, ) :  ×   Rl such that Q(, )[¯] = lim0(Q(,  +  ¯) - Q(, ))/ . The functional Q(, ) is called the pathwise derivative of Q(, ).
45

Assumption C.1. Suppose that:

(C1) For all  > 0, there exists an > 0 such that inf -0 > Q(, 0)  . (C2) Uniformly over   , Q(, ) is continuous in  at  = 0 with respect to the metric · .

(C3) It holds that

sup
, -0 n

Qn(, ) - Q(, ) - Qn(0, 0) 1 + n( Qn(, ) + Q(, ) )

= oP (1)

for all positive sequences n = o(1).

Assumption C.2. Suppose that:

(N1) 0  int() satisfies Q(0, 0) = 0.

(N2) (i) the ordinary derivative Q(, 0) of Q(, 0) in  exists for    and is continuous at  = 0; (ii) the matrix Q0 = Q(0, 0) is of full rank.

(N3) For all    the pathwise derivative Q(, 0)[-0] of Q(, 0) exists in all directions (-0)  ; and

for all (, )  n ×n with a positive sequence n = o(1): (i) Q(, )-Q(, 0)-Q(, 0)[ -0] 

c

 - 0

2 

for

a

constant

c



0;

(ii)

Q(, 0)[ - 0] - Q0[ - 0]

 o(1)n, where Q0[ - 0] =

Q(0, 0)[ - 0].

(N4)    with probability tending to one.

(N5) It holds that



sup n Qn(, ) - Q(, ) -0 n, -0 n 1 + Qn(, ) + Q(, )

= oP (1)

for any positive sequence n = o(1).

Assumption C.3. Suppose that: (B1) 0  int() satisfies Q(0, 0) = 0, and  -a.s. 0.
 (B2) Qn(, ) = inf Qn(, ) + oa.s.(1/ n) (B3) (i)    almost surely, (ii)    with P  probability tending to one, and (iii) -0  = oa.s.(n-1/4). (B4) (i) For all   , the ordinary derivative Q(, ) of Q(, ) in  exists for    and is continuous
at  = 0; (ii) the matrix Q0 = Q(0, ) is of full rank.
46

(B5) For all    and   n with a positive sequence n = o(1), the pathwise derivative Q(, )[¯ - ]

of Q(, ) exists in all directions (¯ - )  ; and for all (, ¯)  n × n : (i) Q(, ¯) - Q(, ) -

Q(, )[¯ - ]

c

¯ - 

2 

for a constant c  0; (ii)

Q(, )[¯ - ] - Q(0, )[¯ - ]

 o(1)n.

(B6) It holds that sup -0 n, -0 n Qn(, ) - Q(, ) - Qn(0, 0) = oa.s.(n-1/2) for any positive sequence n = o(1).

(B7) It holds that sup -0 n, -0 n Qn (, ) - Qn(, ) - (Qn (0, 0) - Qn(0, 0)) = oP  (n-1/2) for any positive sequence n = o(1).

References
Ai, C., and X. Chen (2003): "Efficient estimation of models with conditional moment restrictions containing unknown functions," Econometrica, 71(6), 1795­1843.
(2007): "Estimation of possibly misspecified semiparametric conditional moment restriction models with different conditioning variables," Journal of Econometrics, 141(1), 5­43.
Andrews, D. (1994): "Asymptotics for semiparametric econometric models via stochastic equicontinuity," Econometrica, 62(1), 43­72.
(1995): "Nonparametric kernel estimation for semiparametric models," Econometric Theory, 11(03), 560­586.
Blundell, R., and J. Powell (2004): "Endogeneity in semiparametric binary response models," The Review of Economic Studies, 71(3), 655­679.
Chen, X., O. Linton, and I. Van Keilegom (2003): "Estimation of semiparametric models when the criterion function is not smooth," Econometrica, 71(5), 1591­1608.
Chen, X., and D. Pouzo (2009): "Efficient estimation of semiparametric conditional moment models with possibly nonsmooth residuals," Journal of Econometrics, 152(1), 46­60.
Chen, X., and X. Shen (1998): "Sieve extremum estimates for weakly dependent data," Econometrica, 66(2), 289­314.
Einmahl, U., and D. Mason (2005): "Uniform in bandwidth consistency of kernel-type function estimators," Annals of Statistics, 33(3), 1380­1403.
47

Escanciano, J., D. Jacho-Cha´vez, and A. Lewbel (2012): "Identification and Estimation of Semiparametric Two Step Models," Unpublished manuscript.
(2013): "Uniform Convergence of Weighted Sums of Non- and Semi-parametric Residuals for Estimation and Testing," Journal of Econometrics.
Gine´, E., and J. Zinn (1990): "Bootstrapping general empirical measures," The Annals of Probability, pp. 851­869.
Hahn, J. (1998): "On the role of the propensity score in efficient semiparametric estimation of average treatment effects," Econometrica, 66(2), 315­331.
Hahn, J., and G. Ridder (2013): "Asymptotic Variance of Semiparametric Estimators With Generated Regressors," Econometrica, 81(1), 315­340.
Heckman, J., H. Ichimura, and P. Todd (1998): "Matching as an econometric evaluation estimator," Review of Economic Studies, 65(2), 261­294.
Hirano, K., G. Imbens, and G. Ridder (2003): "Efficient estimation of average treatment effects using the estimated propensity score," Econometrica, 71(4), 1161­1189.
Ichimura, H., and S. Lee (2010): "Characterization of the asymptotic distribution of semiparametric M-estimators," Journal of Econometrics, 159(2), 252­266.
Imbens, G. (2004): "Nonparametric estimation of average treatment effects under exogeneity: A review," Review of Economics and Statistics, 86(1), 4­29.
Kong, E., O. Linton, and Y. Xia (2010): "Uniform Bahadur representation for local polynomial estimates of M-regression and its application to the additive model," Econometric Theory, 26(05), 1529­1564.
Levinsohn, J., and A. Petrin (2003): "Estimating production functions using inputs to control for unobservables," Review of Economic Studies, 70(2), 317­341.
Li, Q., and J. Wooldridge (2002): "Semiparametric estimation of partially linear models for dependent data with generated regressors," Econometric Theory, 18(03), 625­645.
Linton, O., S. Sperlich, and I. Van Keilegom (2008): "Estimation of a semiparametric transformation model," Annals of Statistics, 36(2), 686­718.
48

Mammen, E., C. Rothe, and M. Schienle (2012): "Nonparametric Regression with Nonparametrically Generated Covariates," Annals of Statistics, 40, 1132­1170.
Masry, E. (1996): "Multivariate local polynomial regression for time series: uniform strong consistency and rates," Journal of Time Series Analysis, 17(6), 571­599.
Murphy, K. M., and R. H. Topel (1985): "Estimation and Inference in Two-Step Econometric Models," Journal of Business and Economic Statistics, 3, 370­379.
Newey, W. (1984): "A method of moments interpretation of sequential estimators," Economics Letters, 14(2-3), 201­206.
Newey, W. (1994): "The Asymptotic Variance of Semiparametric Estimators," Econometrica, 62, 1349­ 1382.
Newey, W. (1997): "Convergence rates and asymptotic normality for series estimators," Journal of Econometrics, 79(1), 147­168.
Olley, G., and A. Pakes (1996): "The dynamics of productivity in the telecommunications equipment industry," Econometrica, 64(6), 1263­1297.
Oxley, L., and M. McAleer (1993): "Econometric issues in macroeconomic models with generated regressors," Journal of Economic Surveys, 7(1), 1­40.
Pagan, A. (1984): "Econometric issues in the analysis of regressions with generated regressors," International Economic Review, 25(1), 221­247.
Pakes, A., and S. Olley (1995): "A limit theorem for a smooth class of semiparametric estimators," Journal of Econometrics, 65(1), 295­332.
Powell, J., J. Stock, and T. Stoker (1989): "Semiparametric estimation of index coefficients," Econometrica, 57(6), 1403­1430.
Robinson, P. (1988): "Root-N-consistent semiparametric regression," Econometrica, 56(4), 931­954.
Rosenbaum, P., and D. Rubin (1983): "The central role of the propensity score in observational studies for causal effects," Biometrika, 70(1), 41­55.
49

Rothe, C. (2009): "Semiparametric estimation of binary response models with endogenous regressors," Journal of Econometrics, 153(1), 51­64.
Song, K. (2008): "Uniform convergence of series estimators over function spaces," Econometric Theory, 24(6), 1463­1499.
Song, K. (2012): "On the smoothness of conditional expectation functionals," Statistics & Probability Letters, 82(5), 1028­1034.
Song, K. (2013): "Semiparametric models with single-index nuisance parameters," Working Paper. Sperlich, S. (2009): "A note on non-parametric estimation with predicted variables," Econometrics Jour-
nal, 12(2), 382­395. van de Geer, S. (2009): Empirical Processes in M-Estimation. Cambridge University Press. Van der Vaart, A., and J. Wellner (1996): Weak convergence and empirical processes: with applications
to statistics. Springer Verlag.
50

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Principal Component Analysis in an Asymmetric Norm" by Ngoc Mai Tran, Maria Osipenko and Wolfgang Karl Härdle, January 2014.
002 "A Simultaneous Confidence Corridor for Varying Coefficient Regression with Sparse Functional Data" by Lijie Gu, Li Wang, Wolfgang Karl Härdle and Lijian Yang, January 2014.
003 "An Extended Single Index Model with Missing Response at Random" by Qihua Wang, Tao Zhang, Wolfgang Karl Härdle, January 2014.
004 "Structural Vector Autoregressive Analysis in a Data Rich Environment: A Survey" by Helmut Lütkepohl, January 2014.
005 "Functional stable limit theorems for efficient spectral covolatility estimators" by Randolf Altmeyer and Markus Bibinger, January 2014.
006 "A consistent two-factor model for pricing temperature derivatives" by Andreas Groll, Brenda López-Cabrera and Thilo Meyer-Brandis, January 2014.
007 "Confidence Bands for Impulse Responses: Bonferroni versus Wald" by Helmut Lütkepohl, Anna Staszewska-Bystrova and Peter Winker, January 2014.
008 "Simultaneous Confidence Corridors and Variable Selection for Generalized Additive Models" by Shuzhuan Zheng, Rong Liu, Lijian Yang and Wolfgang Karl Härdle, January 2014.
009 "Structural Vector Autoregressions: Checking Identifying Long-run Restrictions via Heteroskedasticity" by Helmut Lütkepohl and Anton Velinov, January 2014.
010 "Efficient Iterative Maximum Likelihood Estimation of HighParameterized Time Series Models" by Nikolaus Hautsch, Ostap Okhrin and Alexander Ristig, January 2014.
011 "Fiscal Devaluation in a Monetary Union" by Philipp Engler, Giovanni Ganelli, Juha Tervala and Simon Voigts, January 2014.
012 "Nonparametric Estimates for Conditional Quantiles of Time Series" by Jürgen Franke, Peter Mwita and Weining Wang, January 2014.
013 "Product Market Deregulation and Employment Outcomes: Evidence from the German Retail Sector" by Charlotte Senftleben-König, January 2014.
014 "Estimation procedures for exchangeable Marshall copulas with hydrological application" by Fabrizio Durante and Ostap Okhrin, January 2014.
015 "Ladislaus von Bortkiewicz - statistician, economist, and a European intellectual" by Wolfgang Karl Härdle and Annette B. Vogt, February 2014.
016 "An Application of Principal Component Analysis on Multivariate TimeStationary Spatio-Temporal Data" by Stephan Stahlschmidt, Wolfgang Karl Härdle and Helmut Thome, February 2014.
017 "The composition of government spending and the multiplier at the Zero Lower Bound" by Julien Albertini, Arthur Poirier and Jordan RoulleauPasdeloup, February 2014.
018 "Interacting Product and Labor Market Regulation and the Impact of Immigration on Native Wages" by Susanne Prantl and Alexandra SpitzOener, February 2014.
SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasrecahrcwhaws assupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
019 "Unemployment benefits extensions at the zero lower bound on nominal interest rate" by Julien Albertini and Arthur Poirier, February 2014.
020 "Modelling spatio-temporal variability of temperature" by Xiaofeng Cao, Ostap Okhrin, Martin Odening and Matthias Ritter, February 2014.
021 "Do Maternal Health Problems Influence Child's Worrying Status? Evidence from British Cohort Study" by Xianhua Dai, Wolfgang Karl Härdle and Keming Yu, February 2014.
022 "Nonparametric Test for a Constant Beta over a Fixed Time Interval" by Markus Reiß, Viktor Todorov and George Tauchen, February 2014.
023 "Inflation Expectations Spillovers between the United States and Euro Area" by Aleksei Netsunajev and Lars Winkelmann, March 2014.
024 "Peer Effects and Students' Self-Control" by Berno Buechel, Lydia Mechtenberg and Julia Petersen, April 2014.
025 "Is there a demand for multi-year crop insurance?" by Maria Osipenko, Zhiwei Shen and Martin Odening, April 2014.
026 "Credit Risk Calibration based on CDS Spreads" by Shih-Kang Chao, Wolfgang Karl Härdle and Hien Pham-Thu, May 2014.
027 "Stale Forward Guidance" by Gunda-Alexandra Detmers and Dieter Nautz, May 2014.
028 "Confidence Corridors for Multivariate Generalized Quantile Regression" by Shih-Kang Chao, Katharina Proksch, Holger Dette and Wolfgang Härdle, May 2014.
029 "Information Risk, Market Stress and Institutional Herding in Financial Markets: New Evidence Through the Lens of a Simulated Model" by Christopher Boortz, Stephanie Kremer, Simon Jurkatis and Dieter Nautz, May 2014.
030 "Forecasting Generalized Quantiles of Electricity Demand: A Functional Data Approach" by Brenda López Cabrera and Franziska Schulz, May 2014.
031 "Structural Vector Autoregressions with Smooth Transition in Variances ­ The Interaction Between U.S. Monetary Policy and the Stock Market" by Helmut Lütkepohl and Aleksei Netsunajev, June 2014.
032 "TEDAS - Tail Event Driven ASset Allocation" by Wolfgang Karl Härdle, Sergey Nasekin, David Lee Kuo Chuen and Phoon Kok Fai, June 2014.
033 "Discount Factor Shocks and Labor Market Dynamics" by Julien Albertini and Arthur Poirier, June 2014.
034 "Risky Linear Approximations" by Alexander Meyer-Gohde, July 2014 035 "Adaptive Order Flow Forecasting with Multiplicative Error Models" by
Wolfgang Karl Härdle, Andrija Mihoci and Christopher Hian-Ann Ting, July 2014 036 "Portfolio Decisions and Brain Reactions via the CEAD method" by Piotr Majer, Peter N.C. Mohr, Hauke R. Heekeren and Wolfgang K. Härdle, July 2014 037 "Common price and volatility jumps in noisy high-frequency data" by Markus Bibinger and Lars Winkelmann, July 2014 038 "Spatial Wage Inequality and Technological Change" by Charlotte Senftleben-König and Hanna Wielandt, August 2014 039 "The integration of credit default swap markets in the pre and postsubprime crisis in common stochastic trends" by Cathy Yi-Hsuan Chen, Wolfgang Karl Härdle, Hien Pham-Thu, August 2014
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
040 "Localising Forward Intensities for Multiperiod Corporate Default" by Dedy Dwi Prastyo and Wolfgang Karl Härdle, August 2014.
041 "Certification and Market Transparency" by Konrad Stahl and Roland Strausz, September 2014.
042 "Beyond dimension two: A test for higher-order tail risk" by Carsten Bormann, Melanie Schienle and Julia Schaumburg, September 2014.
043 "Semiparametric Estimation with Generated Covariates" by Enno Mammen, Christoph Rothe and Melanie Schienle, September 2014.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

