BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2015-050
Nonparametric Estimation in case of Endogenous Selection
Christoph Breunig* Enno Mammen*² Anna Simoni*³
* Humboldt-Universität zu Berlin, Germany *² Heidelberg University, Germany and
Higher School of Economics Moscow, Russian Federation *³ CNRS and CREST, France
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Nonparametric Estimation in case of Endogenous Selection 

Christoph Breunig

Enno Mammen 1

Humboldt-Universita¨t zu Berlin

Heidelberg University and

Higher School of Economics, Moscow

Anna Simoni 2

CNRS and CREST

November 10, 2015

This paper addresses the problem of estimation of a nonparametric regression function from selectively observed data when selection is endogenous. Our approach relies on independence between covariates and selection conditionally on potential outcomes. Endogeneity of regressors is also allowed for. In both cases, consistent two-step estimation procedures are proposed and their rates of convergence are derived. Also pointwise asymptotic distribution of the estimators is established. In addition, we propose a nonparametric specification test to check the validity of our independence assumption. Finite sample properties are illustrated in a Monte Carlo simulation study and an empirical illustration.
Keywords: Endogenous selection, instrumental variable, sieve minimum distance, regression estimation, convergence rate, asymptotic normality, hypothesis testing, inverse problem.
JEL classification: C14, C26
This work was supported by the DFG-SNF research group FOR916. The authors gratefully thank the CoEditor Jianqing Fan, an Associate Editor, and two anonymous referees for their many constructive comments on the previous version of the paper. The authors are also grateful to Timothy Armstrong, Xiaohong Chen, Victor Chernozhukov, Elise Coudin, Laurent Davezies, Kirill Evdokimov, Xavier D'Haultfoeuille, Aureo de Paula, Christoph Rothe, Bernard Salanie´ and seminar participants at Bristol, Columbia University, CREST, Mannheim University, MIT and Yale University for useful comments and to the GIP team at SFB 884-Mannheim for providing data of the German Internet Panel. Christoph Breunig was supported by the DFG postdoctoral fellowship BR 4874/1-1. The author is also grateful for support and hospitality of the Cowles Foundation. Anna Simoni gratefully acknowledges financial support from ANR-13-BSH1-0004 (IPANEMA) and hospitality from University of Mannheim and Boston College.
Department of Economics, Humboldt-Universita¨t zu Berlin, Spandauer Straße 1, 10178 Berlin, Germany, e-mail: christoph.breunig@hu-berlin.de
1Institute for Applied Mathematics, Heidelberg University, Im Neuenheimer Feld 294, 69120 Heidelberg, Germany and Higher School of Economics, Pokrovskii Boulevard 11, 103012 Moscow, Russia, e-mail: emammen@rumms.uni-mannheim.de
2CREST, 15, Boulevard Gabriel Pe´ri, 92240 Malakoff, France, e-mail: simoni.anna@gmail.com
1

1. Introduction
This paper addresses the problem of estimation of a regression function from selectively observed data. To explain the problem at stake, consider a partially observed dependent variable Y, a vector of covariates X and a binary indicator . The econometrician observes a realization of  and X for each individual in the random sample but only observes a realization of Y when  = 1. In many applications it is important to learn about E[Y|X] which, by the law of total expectation, can be written as
E[Y|X] = E[Y|X,  = 1]P( = 1|X) + E[Y|X,  = 0]P( = 0|X).
The difficulty arises because the data available cannot identify E[Y|X,  = 0] nor E[Y|X]. In this paper, we address this lack of identification by assuming independence between the regressors X and the selection mechanism  conditionally on the selectively observed outcome Y. Relying on this assumption we propose a new methodology to consistently estimate the regression function (·)  E[Y|X = ·].
We also consider endogeneity of covariates by extending the nonparametric instrumental variable models of Newey and Powell [2003] and Darolles et al. [2011] to allow for selection. That is, we propose a method to estimate a structural function  which satisfies
Y = (Z) + U
for some unobservables U, where Z is endogenous in the sense that E[U|Z] 0 and an additional instrumental variable X is available such that E[U|X] = 0. If the instrument X is independent of the selection given potential outcome Y, we show that  is identified and can be consistently estimated under commonly imposed assumptions.
Previous literature has proposed different solutions to overcome the problem of lack of identification of E[Y|X]. One solution consists in assuming missing-at-random (MAR), namely, independence between the selection variable and the outcome conditional on the observed covariates, see Rubin [1976]. MAR implies E[Y|X] = E[Y|X,  = 1] = E[Y|X,  = 0]. Unfortunately, the plausibility of this assumption may be questioned in many economic examples where missing observations arise due to self-selection, nonresponse or because counterfactual variables are unobservable (see the examples given in Heckman [1979]).
In his seminal work, Heckman [1974, 1979] relies on instruments that determine selection but not the outcome and proposes a consistent parametric estimation method. Point-identification comes from parametric restrictions. Ahn and Powell [1993] and Das et al. [2003] extend Heckman's approach to a semiparametric and nonparametric framework, respectively.
An alternative strategy relies on "identification at infinity", namely, on the fact that the selection problem becomes negligible for large values of the covariates. This strategy requires the existence of a covariate with a large support, see Chamberlain [1986]. Based on this idea Lewbel [2007] and D'Haultfoeuille and Maurel [2013] propose alternative identification strategies.
A completely different approach was proposed by Manski [1989] who poses, as the only restriction, a bound on the support of Y conditional on X. This implies a bound on E[Y|X]. While such a weak restriction has the advantage of ensuring robust inference, only partial identification of E[Y|X] can be achieved. Following Manski [1989], an extensive literature on bounds and partial identification in econometrics has flourished (see e.g. Chernozhukov
2

et al. [2013] and Tamer [2010] for a review).
In this paper, we solve the problem of endogenous selection by using a different type of instrumental strategy. We assume independence between selection  and instruments X, conditional on the outcome Y (and possibly additional covariates), namely

 X | Y.

(1.1)

This assumption is suitable when selection is driven by the outcome Y. For example, if Y
denotes income and X expenditure then, typically in survey data, whether people report
their income or not is primarily determined by the level of their income. Assumption (1.1)
has been used in the previous literature on missing data (see e.g. Chen [2001], Tang et al.
[2003] in the statistics literature and D'Haultfoeuille [2010], Davezies and D'Haultfoeuille
[2013], Ramalho and Smith [2013] in the econometrics literature). This type of assumption
is common in the finite mixtures literature (e.g. Henry et al. [2014]) as well as in the
nonclassical measurement error literature (see e.g. Hu and Schennach [2008b]). Moreover,
it is a particular case of Assumption (41) in Manski [1994]. Assumption (1.1) alone is not sufficient for nonparametric identification of the regression function  or the selection probability P( = 1|Y). In this paper, however, we show that the function  is identified if Y is complete for X. Moreover, Assumption (1.1) can be tested, see Theorem 2.4 in
D'Haultfoeuille [2010].
Our paper builds on these results and goes beyond what has been proposed in the literature so far. We focus on E[Y|X] which is point-identified despite the presence of
endogenous selection. Then, we contribute to the literature in many directions. First, we propose a two-step estimator for E[Y|X] that does not require P( = 1|Y) to belong to a parametric family. Under our assumptions, P( = 1|Y) is identified through a conditional moment restriction. Because P( = 1|Y) is nonparametric, its estimation requires solving
a linear inverse problem which is ill-posed in general. In the first step, we use a constraint sieve-type estimator for P( = 1|Y) which has to account for the ill-posedness of the inverse
problem (see e.g. Newey and Powell [2003], Ai and Chen [2003] for sieve estimators for
inverse problems in econometrics). In the second step, we use this estimator to construct a plug-in series estimator for E[Y|X].
Second, we recover the rate of convergence of the estimator for the nonparametric regression function E[Y|X] and show that it does not suffer from the ill-posedness of the
underlying inverse problem. In contrast to the classical nonparametric rate of convergence,
we get an additional bias due to estimation of the selection probability in the first step.
Also Das et al. [2003] obtain an additional bias term in their convergence rate but which
it due to estimation of a propensity score. Under additional smoothness conditions, this
bias is asymptotically negligible and the usual nonparametric regression rate is obtained. The fact that the rate does not suffer from the ill-posedness of the estimation problem in
the first step is because the second step of our estimation procedure performs a smoothing through integration with respect to the conditional distribution of Y given X.
We also study the case where regressors are potentially endogenous. In contrast to
Das et al. [2003], who used a control function method to correct for endogeneity and
selection, we rely on a nonparametric instrumental regression model where the dependent
variable is selectively observed. In this model, we propose a two step estimator of the
structural function and obtain a convergence rate that deviates from the usual ill-posed

3

inverse problem rate by an extra bias term. Third, we establish asymptotic normality for our estimator of the regression function
E[Y|X = ·] evaluated at some point in the support of X. We point out that, due to the nature of our estimation problem, both the rate of convergence and asymptotic normality are new and cannot be derived from the results in Newey [1997] and the related literature. Indeed, we derive an asymptotic variance formula that involves an additional part due to correction for sample selection in the first step.
Forth, we propose a testing procedure to test the identifying assumption (1.1). At the best of our knowledge a testing procedure for this assumption in the continuous-(Y, X) case has not been developed yet in the literature. Some ideas about how to construct a test for assumption (1.1) are outlined in D'Haultfoeuille [2010] but without formal implementation. Our testing procedure is based on a different intuition.
The remainder of this paper is organized as follows. In Section 2, we present the setup and discuss identification. In Section 3, we present our two-step estimator for E[Y|X], we give rates of convergence of the integrated squared error of our estimator and establish pointwise asymptotic normality of our estimator. Section 4 deals with possible endogenous covariates. Our testing procedure to test the instrumental variable restriction (1.1) is presented in Section 5. Its finite sample properties are investigate through a Monte Carlo experiments whose results are reported in Section 6. Section 7 presents an empirical application of our method to estimate the propensity to work in the German speaking population by using "German Internet Panel" data. All proofs are postponed to the appendix.
2. Identification
In this section, we provide assumptions under which the selection probability function P( = 1|Y = ·) and the regression function E[Y|X = ·] are identified. We further motivate our estimation procedure.
2.1. Setup and Main Assumptions
Let (, Y, Xt) be a jointly distributed random vector where (Y, Xt) is a random vector which takes values in R1+dx and  is a random variable which takes values in {0, 1}. A realization of (, Xt) is observed for each individual in the random sample while a realization of the dependent variable Y is observed when  = 1 and missing when  = 0. We write Y = Y.1 We assume that the marginal distribution of Y (resp. X) admits a probability density function pY (resp. pX) with respect to the Lebesgue measure. The following three assumptions are sufficient to identify the joint distribution of (, Y, Xt). Assumption 1. It holds that
 X | Y.
1In our setting, Y is assumed to be a scalar. Our results would still hold if we extended this framework to allow for a p-dimensional vector Y of selectively observed variables. In this case  = ((j))1 j p and the j-th component of Y would be observed when (j) = 1 and missing when (j) = 0. This extension would require little modifications of our method but would burden the notation and the presentation. For this reason we do not consider it.
4

Assumption 1 states an exclusion restriction of the random vector X with respect to the selection variable  given potential outcomes Y. The vector X is referred to as the vector
of instruments. This assumption can be justified in many settings. An example is provided by measurement error models where Y is observed with error for some individuals. Then, for some error , X = Y + can be interpreted as a proxy for Y and satisfies Assumption 1 if  , see e.g. Chen et al. [2011]. Other examples are given by data with nonresponse. For instance, consider the case where Y is income and X is expenditure. It could be that people
with high income are less likely to report it. Examples of such type of incomplete data sets
are the French "Enque^te Budget de famille" of INSEE or the British "Family expenditure
Survey". For further illustrations of Assumption 1 we refer to Ramalho and Smith [2013].
Assumption 2. For every function  that is bounded from below almost surely and satisfies E |(Y)| <  it holds that E[(Y)|X] = 0 implies (Y) = 0.2
Assumption 2 is weaker than L1­completeness but stronger than bounded­completeness.
Completeness conditions have been largely used in econometrics as identification assumptions, see e.g. Darolles et al. [2011], Newey and Powell [2003], Blundell et al. [2007], Hu
and Schennach [2008a], D'Haultfoeuille [2011] and Hoderlein et al. [2012].
Assumption 3. It holds P( = 1|Y) > 0.
This assumption can rule out a selection when it is a deterministic function of Y such as
P( = 1|Y = ·) = 1{· c} for some constant c belonging to the interval (min(Supp(Y)), ) where Supp(Y) denotes the support of Y. Here 1 denotes the indicator function. To
understand Assumption 3, consider the example where  = (Y, ) for some function (·) and a random variable . Then, Assumption 3 is verified if the distribution of  is such that the set {; (y, ) = 1} has positive probability for every y in the support of Y except, possibly, for a set of y with probability 0.

2.2. Identification and idea of the estimator

Our object of interest is E[Y|X] while the selection probability P( = 1|Y) is a nuisance
(functional) parameter. However, knowledge of the latter allows us to identify and estimate E[Y|X] in a way that we now explain. Let us introduce the function g(·)  1/P( = 1|Y = ·) for the inverse selection probability. Under Assumptions 1­3, the function g is identified
through the conditional moment restriction

E g(Y) X = 1,

(2.1)

see Theorem 2.3 in D'Haultfoeuille [2010]. In the first step of our two-step procedure, we
make use of (2.1) to estimate the inverse selection probability function g.
Since the function g is identified by equation (2.1), identification of the conditional expectation E[Y|X] follows from

E[Y|X] = E[Y P( = 1|Y)g(Y)|X] = E E[Yg(Y)|Y] X = E[Yg(Y)|X] = E[Yg(Y)|X] (2.2)
where the first equality follows from Assumption 3 and second to last equality follows from Assumption 1 and the fact that g(Y) = g(Y) whenever Y  Y differs from zero. This
2Since conditional expectations are defined only up to equality a.s., all (in)equalities with conditional expectations and/or random variables are understood as (in)equalities a.s., even if we do not say so explicitly.

5

result shows that E[Y|X] can be written as a weighted average of the observed Y where
the weight is equal to the inverse selection probability function. We use equation (2.2) to construct an estimator for E[Y|X] in the second step of our estimation procedure.

Remark 2.1 (Including additional covariates). In empirical applications, only a subset of the covariates might be independent of selection given potential outcome. We can cover this case by slightly extending Assumption 1. More precisely, suppose that X = (X1, X2) and that Assumption 1 is modified as  X1|(Y, X2) and hence, X2 can be correlated to . Under this assumption, E[Y|X] = E[Yg(Y, X2)|X] where g(y, x) = 1/P( = 1|Y = y, X2 = x). Moreover, if Assumptions 2 and 3 are modified with (Y) replaced by (Y, X2) and P( = 1|Y) replaced by P( = 1|Y, X2) > 0, respectively, then g is identified by E[g(Y, X2)|X] = 1.

2.3. Notation

For a random vector V we use the corresponding calligraphic capital letter V to denote

its support. Let LV2 = { :



2 V



E |(V)|2

<

}

denote

the

space

of

square

integrable

functions of V with respect to the distribution of V. We denote by ·, · V the inner product

in LV2 that induces

·

2 V

.

Moreover,

  := supvV |(v)| denotes the sup norm and

·

is

the usual Euclidean norm.

The regression function of interest is denoted by:

(·) = E[Y|X = ·]

but we may use both notations depending on the context. Let { fj}j 1 (resp. {ej}j 1) be

a sequence of approximating functions in L2X (resp. LY2 ). Then, we denote by fmn(X) = ( f1(X), . . . , fmn(X))t (resp. ekn(Y) = (e1(Y), . . . , ekn(Y))t) a vector of functions which are used to approximate the conditional expectation E[Y|X] (resp. the inverse selection probability

g(Y)) and by Xmn = fmn(X1), . . . , fmn(Xn) t (resp. Ykn = (1ekn(Y1), . . . , nekn(Yn))t) the n × mn

(resp. n × kn) matrix obtained by putting together the n vectors fmn(Xi), i = 1, . . . , n (resp.

iekn(Yi), i = 1, . . . , n, where iekn(Yi) denotes the product of i and the vector ekn(Yi)).

We denote by Fmn = (·) =

mn j=1



j

f

j

(·)

:





Rmn

the linear sieve space of dimension

mn <  that becomes dense in LX2 as n tends to infinity. For a matrix A we denote by A- its generalized inverse. For a function  defined on Y, we denote by M : LY2  LY2 the multiplication operator M =  which is bounded if  is bounded on Y. Then

(Mid)(y) = y(y) for all y  Y where id denotes the identity function.

3. Nonparametric Regression with Sample Selection
In this section, we consider estimation of the regression function . The first step estimation procedure for the inverse selection probability g is based on constrained sieve minimum distance. In the second step, we use a plug-in series estimator of the conditional expectation (·) = E[Yg(Y)|X = ·]. We derive the rate of convergence in mean square error and the asymptotic distribution of our estimator of .

6

3.1. The Estimator and its Rate of Convergence

Let G denote a separable Banach space which is endowed with a norm · G and let g  G.
Equation (2.1) can be written in a more compact form by using the following notation. Let T : G  LX2 be the linear operator T  E[(Y)|X]. Thereby, equation (2.1) can be equivalently written as the operator equation

Tg = 1

(3.1)

where the function g is identified under Assumptions 1­3. Note that since E[g(Y)] = E[g(Y)] = 1 + g(0)P( = 0) the function g belongs to LY1 if g(0) < . In the following we assume without loss of generality that g(0) <  and hence G = L1Y.
For every   LY1 , denote (·, ) = E[(Y) - 1|X = ·]. The least squares estimator of (·, ) is given by

n

n(·, ) = fmn (·)t(Xtmn Xmn )-

i(Yi) - 1 fmn (Xi)

i=1

(3.2)

for some integer mn which increases with the sample size n. Under conditions given below, Xmt nXmn will be nonsingular with probability approaching one and hence its generalized
inverse will be the standard inverse. We now introduce some assumptions.

Assumption 4. (i) We observe a sample ((1, X1, Y1), . . . , (n, Xn, Yn)) of independent and identical distributed (iid.) copies of (, X, Y) where Y = Y. (ii) There exists a constant C > 0 and a

sequence of positive integers (mn)n 1 satisfying supxX fmn(x) 2 Cmn such that m2n/n = o(1). (iii) The smallest eigenvalue of E[ fm(X) fm(X)t] is bounded away from zero uniformly in m. (iv) Let
  L2X and there is Fmn  Fmn such that Fmn -  X = O(m-n/dx) for some constant  > 0.

Assumption 4 (ii) - (iii) restricts the magnitude of the approximating functions { fj}j 1

and impose nonsingularity of their second moment matrix. It is a standard assumption

for series estimators (cf. e.g. Assumption 2 in Newey [1997]). Assumption 4 (ii) holds for

instance for polynomial splines, Fourier series and wavelet bases but rules out orthogonal

polynomials and power series sieves. Assumption 4 (iv) determines the sieve approxima-

tion error which in turn characterizes the bias of the estimated regression function . In

the following, we consider the linear sieve space Gn = (·) =

kn j=1



j

e

j

(·)

:





Rkn

of

dimension kn <  that becomes dense in the function space L1Y as n tends to infinity. We

propose the following sieve minimum distance estimator

n

gn  arg min

2n(Xi, ).

{Gn: (·) 1} i=1

(3.3)

The constraint (·) 1 imposed on the sieve space Gn ensures that the estimated conditional probability of observing Y belongs to the unit interval. This estimator of g corresponds to
the penalized sieve minimum distance estimator suggested by Chen and Pouzo [2012].
If no constraint is imposed then the sieve estimator gn has an explicit solution given by

gn(·)  ekn (·)t kn and k = YtkXm(XtmXm)- Xmt Yk -YtkXm(Xmt Xm)- Xtm1n

(3.4)

where 1n is a n-dimensional vector of ones and for some integer kn mn which increases with the sample size n.

7

The second step of our estimation procedure consists in using the estimator gn in (3.3)
to construct an estimator for . Let Gn = Y1 gn(Y1), . . . , Yn gn(Yn) . Then, our estimator of the nonparametric regression function (·) is given by

n(·)  fmn (·)t mn where mn = (Xmt n Xmn )- Xtmn Gn.

(3.5)

In the following, the sequence (Rn)n 1 denotes the rate of convergence of the estimator gn w.r.t. to the norm · 2G. The next assumption is used to recover the rate of convergence of
n.
Assumption 5. (i) For all   Gn there exists FmnT  Fmn such that FmnT - T  = O(mn-/dx). (ii) There exists a sequence of positive integers (n)n 1 satisfying supyY ekn(y) 2 2n such that kn2n/n = o(1). (iii) It holds kn2Rn = O(1). (iv) TMid X/ T X is bounded uniformly over all   G with T X 0.
Assumption 5 (i) holds true, for example, for splines or power series if the family of
functions T :   Gn contains only functions which are at least ­times continuously differentiable (see also Assumption 3 of Blundell et al. [2007]). Assumption 5 (ii) is satisfied with n = kn when the approximating functions are for instance B-splines, Fourier series and wavelet bases. For Legendre polynomials this assumption is satisfied with n = kn. Assumption 5 (iii) is a mild restriction on the rate of convergence of gn which we illustrate below. Instead of a bound on Y, Assumption 5 (iv) restricts the size of the multiplication operator Mid in the norm induced by T. Otherwise stated, it requires that the norms of the operators TMid and T are equivalent. Assumption 5 (iv) is satisfied for instance under an additional link condition, like Assumption 6 (iii) below, if the basis functions coincide with Legendre polynomials or cardinal B-splines (see Example 3.1 below).

Estimation of g requires to "solve" a conditional moment restriction that is different from (2.2), namely E[g(Y)|X] = 1. From Blundell et al. [2007] and Chen and Pouzo [2012] we obtain the rate of convergence of T(gn - g) X. Their result, however, is not enough to obtain the rate of convergence of n as our case requires to determine the rate
of TMidgn - TMidg X where T is a series least square estimator of T. Thereby, additional arguments are needed to obtain the rate of n -  X given in the following theorem. In the following, for any  in G let Ekn  Gn be such that Ekn -  G = o(1).
Theorem 3.1. Let Assumptions 1 ­ 5 hold true. Then we have

n - 

2 X

=

Op

max

m-n 2/dx ,

mn n

,

T(Ekn g -

g)

2 X

.

As we see from Theorem 3.1, the rate of convergence of our estimator n depends

on both parameters kn and mn which correspond to the first and second estimation step,

respectively. In addition to the usual nonparametric rate we obtain an additional bias term

T(Ekn g

-

g)

2 X

which

is

due

to

the

sieve

approximation

of

the

inverse

selection

probability

function g. An additional bias occurs also in the convergence rate for estimating regression

functions in Theorem 4.1 of Das et al. [2003]. In their case, however, the additional bias arises

from nonparametric estimation of a propensity score. From Theorem 3.1 we see that n attains the optimal nonparametric rate of convergence if T(Ekn g - g) X = O(m-n/dx). If the inverse selection probability g is sufficiently smooth in the sense that Ekn g-g G = O(mn-/dx)
then, by Jensen's inequality, the optimal nonparametric rate is obtained. In the following,

8

we provide a more general treatment by incorporating mapping properties of the operator

T in the Hilbert space case where G = LY2 .

Assumption 6. (i) E[Y2] <  and E[g2(Y)] < . (ii) Assume Ekn g - g Y = O(kn-) for some

constant  > 0. (iii) There exists a sequence of non-increasing positive real numbers (j)j 1 such

that

T

2 X

c

 j=1



j

, ej

2 Y

and

T

2 X

C

 j=1



j

, ej

2 Y

for

some

constant

C

>

0

and

all

  L2Y. (iv) The largest eigenvalue of

1j /2-l 1/2

Midej, el Y

j,l

is bounded away from infinity.
1

Assumption 6 (i) ensures that g belongs to the Hilbert space LY2 and Y has finite second moment while (ii) determines the sieve approximation error for estimating the function g.

Assumption 6 (iii) is also known as a link condition and commonly used in the analysis of

inverse problems (see, e.g. Chen and Reiß [2011]).

Corollary 3.2. Let Assumptions 1­ 4, 5 (i)­(iii), and 6 hold true. Then we have

n - 

2 X

=

Op

max

m-n 2/dx ,

mn n

,

kn kn-2

.

(3.6)

Remark 3.1. In the mildly ill-posed case where j  j-2t, t mn  ndx/(2+dx). Hence,

0, let kn  n1/(2t+2+1) and

n - 

2 X

= Op

max

n-(2t+2)/(2t+2+1), n-2/(2+dx)

which is Op(n-2/(2+dx)) if  dx(t+). In case of trigonometric basis functions, the operator

T acts like integrating (t)­times which then automatically implies  = dx(t + ) (cf. page

12 in Breunig and Johannes [2011]).

Also it holds

gn

-g

2 Y

=

Op(n-2/(2t+2+1))

and

in

particular we have kn2Rn = O(n(2-2)/(2t+2+1)) = o(1) if  > 1. In the severely ill-posed case

where j  exp(- j2t), t > 0, we let kn  (log n)1/2t and obtain

n - 

2 X

= Op

n-2/(2+dx)

.

In this case,

gn

-

g

2 Y

=

Op(log(n)-2/t)

and

in

particular,

kn2 Rn

=

O(log(n)(2-2)/t)

=

o(1)

again if  > 1. We conclude that under mild conditions on the smoothness of  the optimal

nonparametric rate of regression in mean squared error is obtained.

The following example illustrates that Assumption 6 (iv) is automatically satisfied when
{ej}j 1 coincides with Legendre polynomials. Similarly to this example, Assumption 6 (iv) can be verified for cardinal B-splines (cf. De Boor [1978]).

Example 3.1. Assume that Y is contained in [-1, 1] and consider the Hilbert space LY2 = L[2-1,1] enowed with the usual norm. Let {ej}j 1 be the Legendre polynomials. That is, for y  [-1, 1] we define e1  1, e2(y) = y, and

j ej+1(y) = (2 j - 1)y ej(y) - ( j - 1)ej-1(y)

for j 3. This recursion formula is equivalent to

y ej(y)

=

j ej+1(y) + ( j - 1)ej-1(y) 2j - 1

9

which implies

12/2-1M1/i2deM...1, eid1e2Y, e1 Y

11/22-1/2 Mide1, e2 Y Mide2, e2 Y ...



.
. .

. . .

. ..

=



0
221/2 311/2
0
...

211/2 321/2
0
431/2
15...12/2

0
431/2 1514/2
0
...



0
0 ... ...

.
.
. .

. . . .

....

.

The norm of the right hand side matrix is bounded by its Frobenius norm which is

j

j2 4 1 (2 j+1)2 (2( j-1)+1)2

within

a

constant.

This series is bounded and thereby Assumption

6 (iv) holds true for either the mildly ill-posed case or the severely ill-posed case.

3.2. Pointwise Asymptotic Distribution of the Estimator
This subsection is about inference on the regression function  evaluated at some point of the support of X. To establish the asymptotic distribution of our estimator we require the following additional assumptions. Let us introduce the matrices Qn = E[ fmn(X) fmn(X)t], Tn = E[ fmn (X)ekn (Y)t], and TYn = E[Y fmn (X)ekn (Y)t].
Assumption 7. (i) Let E[|Yg(Y) - (X)|4|X] C and Var(Yg(Y)|X) c for some constants c, C > 0. (ii) The matrix Tnt Tn has full rank. (iii) The function g is uniformly bounded away from 1. (iv) There exist some constants c, C > 0 such that c ln/kn C, where ln is the minimum eigenvalue of TtnQn-1Tn. (iv) It holds Ekn g - g  = O(kn-+1/2) for  > 1/2 and Fmn  -   = O(m-n/dx+1/2) for  > dx/2.
A bounded fourth moment of the error was also assumed by Newey [1997] to establish asymptotic normality of series estimators in the regression context. The consistency result established in Theorem 3.1 together with Assumption 7 (iv) imply that the constraint is not binding asymptotically and hence the estimator gn given in (3.4) conicides with the one in (3.3). To prove consistency of the asymptotic variance formula we require uniform convergence of the approximation biases at a certain rate. Examples of approximating functions that satisfy this condition are Fourier series, splines series and wavelet series (see also Belloni et al. [2015]).
For the asymptotic distribution result we introduce the asymptotic variance

Vn(x) = Var fmn(x)tQn-1 fmn(X)(Yg(Y) - (X)) + Var fmn (x)tQn-1TYn TtnQ-n1Tn -1Tnt Qn-1 fmn (X) - E[ fmn (X)]

.

In contrast to Newey [1997], the asymptotic variance is driven by an additional summand that arises from the first step estimation of the inverse selection probability g. An additional part in the asymptotic variance formula, due to sample selection correction, is obtained also in Das et al. [2003] but which is due to the estimation of a propensity score. In the next

10

result we replace the variance Vn(x) by the estimator

n
Vn(x) = n-1

fmn (x)t(Xmt n Xmn /n)- fmn (Xi) Yi gn(Yi) - n(Xi)

2
+

i=1

n

n-1

fmn (x)t(Xmt n Xmn )-Xmt n diag(Y)Ykn

Ytkn Xmn (Xmt n Xmn )-Xtmn Ykn

-
×

i=1

Yktn Xmn (Xtmn Xmn /n)- fmn (Xi) -

n

2
fmn (Xi ) .

i =1

where diag(Y) is the diagonal matrix with entries on the diagonal given by (Y1, . . . , Yn). The next result establishes the asymptotic distribution of the estimator n evaluated at some point x in the support of X.

Theorem 3.3. Let Assumptions 1 ­ 5 and 7 be satisfied. If

n

Fmn  - 

2 X

=

o(1)

and

n

T(Ekn g -

g)

2 X

=

o(1)

then we have

n/Vn(x) n(x) - (x) d N(0, 1).

(3.7)

Moreover, if Y is bounded, and mnkn = o(k4nn) then

n Vn(x) n(x) - (x) d N(0, 1).

In the setting of Corollary 3.2, condition (3.7) is satisfied if max nm-n2/dx, nknkn-2 = o(1). Thereby, in both estimation steps, undersmoothed estimators have to be consid-
ered. This ensures that also the sieve approximation bias for estimating the function g
becomes asymptotically negligible. Moreover, the second result of the theorem requires the additional rate mnkn = o(4knn). In the setting of Corollary 3.2, this is equivalent to mn = o kn-(8t+1)n in the mildly ill-posed case and to mn = o n exp(-4kn2t) in the severely ill-posed case.

4. Sample Selection with Endogenous Covariates
In many economic applications, it is necessary to correct for both sample selection of the dependent variable and endogeneity of (some) covariates. In this section, we show that under the assumptions of Section 2 identification of the corresponding reduced form equation can be achieved. Under further conditions, which are common in the nonparametric instrumental variable literature, identification of the structural function is also obtained. An estimator of the nonparametric structural function is proposed and we establish its rate of convergence as well as its asymptotic distribution.

4.1. Model and Identification

In this section, we consider the instrumental variable model under selectively observed outcomes given by

Y = (Z) + U where E[U|X] = 0 and Y = Y

(4.1)

11

where Z is a dz-vector of possibly endogenous regressors in the sense that E[U|Z] 0 and hence (Z) need not to coincide with E[Y|Z]. Here, X is a vector of instruments used to identify the structural function . The instrument X is also assumed to satisfy Assumption 1; that is,  X|Y. An example is the estimation of Engel curves, where Y denotes budget share allocated to alcohol which is often not reported (see for instance the British FES) and Z is total expenditure. Expenditure is commonly thought of as endogenous and typically instrumented for with labor income X. In this case, the instrument certainly influences Y through Z but is unlikely to directly influence survey nonresponse. The reduced form equation of the structural model (4.1) is given by
E[Y|X] = E[(Z)|X]

where the left hand side is not identified. By making use of equation (2.2), we obtain the reduced form

E[Yg(Y)|X] = E[(Z)|X]

(4.2)

where the left hand side is identified under Assumptions 1­3. Thereby, completes of Z with respect to X ensures identification of the structural function . In the following example, we see that Assumptions 1 and 2 are satisfied in a triangular model.

Example 4.1. Let us rewrite model (4.1) in reduced form and additionally specify a selection equation. Then the assumption  X | Y is satisfied in the triangular model
Y = E[(Z)|X] +  where E[|X] = 0  = (Y, )
and  (X, ). As in D'Haultfoeuille [2011] it can be argued that, under some assumptions on the distribution of (X, ), Y is complete for X.

4.2. The Estimator and its Rate of Convergence

In this section, we propose an estimator for the structural function  and derive its rate of convergence. For any   LZ2 we introduce the function (·, g, ) = E[Yg(Y) - (Z)|X = ·]. The least squares estimator of (·, g, ) is given by

n

n(·, g, ) = fmn (·)t (Xmt n Xmn )-

Yi g(Yi) - (Zi) fmn(Xi).

i=1

Let us now propose a plug-in minimum distance estimator of  which involves the estimator gn given in (3.3) of the inverse selection probability g. That is, we estimate  by

n

n  arg min

n2(Xi, gn, ).

n i=1

(4.3)

Here, we consider the linear sieve space n = (·) =

kn j=1

 j p j (·)

:





Rkn

of dimension

kn <  for some basis functions {pj}j 1 in L2Z. In particular, we have the least squares

solution

n(·)  pkn (·)t kn and k = ZtkXm(Xmt Xm)- XtmZk -ZtkXm(XtmXm)- Xmt Gn

(4.4)

where Gn = Y1 gn(Y1), . . . , Yn gn(Yn) t and Zk = (pk(Z1), . . . , pk(Zn))t.

12

Assumption 8. (i) We observe a sample ((1, Y1, Z1, X1), . . . , (n, Yn, Zn, Xn)) of iid. copies of (, Y, Z, X) where Y = Y. (ii) There exists a constant C 1 and a sequence of positive integers (kn)n 1 satisfying supzZ pkn(z) 2 Ckn such that kn2/n = o(1). (iii) The smallest eigenvalue of E[pk(Z)pk(Z)t] is bounded away from zero uniformly in k. (iv) For every   LZ2 there exists kn  n such that kn -  Z = O(kn-/dz) for some constant  > 0.
Let us introduce the linear conditional expectation operator K : LZ2  LX2 with K = E[(Z)|X] for all   LZ2 . We introduce the following assumption which ensures identification of the  in the model (4.1).

Assumption 9. (i) For some  > 0 and for every   kn there exists FmnK  Fmn such that

FmnK-K  = O(mn-/dx). (ii) For every There exists a sequence (j)j 1 such that K

function

2 X

C

j=1Lj2Z, E, p[j (2ZZa)n|Xd]

=0 K

implies

2 X

c

(Z) = 0. (iii)

 j=1



j

, pj

2 Z

for all   LZ2 and some constants c, C > 0.

The next result establishes the rate of convergence of the estimator n.

Theorem 4.1. Let Assumptions 1­5, 8 and 9 hold true. Then we have

n - 

2 Z

=

Op

max

kn-2/dz ,

kn nkn

,

k-n1

T(Ekn g -

g)

2 X

.

(4.5)

In contrast to Theorem 3.1, the additional bias due to sample selection is also effected

by the potential ill-posed coming from endogeneity of covarites Z. Under the conditions

of Corollary 3.2, the bias -kn1

T(Ekn g

-

g)

2 X

can

be

bounded

by

-kn1kn kn-2.

Thereby, the

usual rate in nonparametric instrumental regression (see Chen and Reiß [2011]) can be only

obtained if knkn2/dz const. knkn2 for all n sufficiently large.

Remark 4.1. To conclude this section it is worth to mention that with our estimation method we can deal with another type of endogeneity, different from the one just considered. Suppose that the random vector X that satisfies Assumption 1 is endogenous, in the sense that the relationship of interest is the structural function  satisfying

Y = (X) + U where E[U|X] 0 and Y = Y.

(4.6)

This situation can be easily dealt with by assuming that there exists another vector W of instruments such that E[U|W] = 0 and  X | (Y, W). The latter assumption replaces Assumption 1 and corresponds to the one in Remark 2.1. For simplicity, we assume that W is observed for all the individuals so that the conditional distribution of X|W is identified from the data. Moreover, we have to assume that P( = 1|Y, W) > 0 a.s. and that Assumption 2 holds with (Y) replaced by (Y, W). Then  is identified through (4.6) and
E[Y|W] = E[YP( = 1|Y, W)g(Y, W)|W] = E[Yg(Y, W)|W] = E[Yg(Y, W)|W].

Consequently, we obtain the identified reduced form equation

E[Yg(Y, W)|W] = E[(X)|W]

and identification follows as above.

13

4.3. Pointwise Asymptotic Distribution of the Estimator
This subsection is about inference on the structural function  evaluated at some point of the support of Z. To establish the asymptotic distribution of our estimator we require the following additional assumptions. Let us introduce the matrix Kn = E[ fmn(X)pkn(Z)t] and recall Qn = E[ fmn(X) fmn(X)t], Tn = E[ fmn(X)ekn(Y)t], and TnY = E[Y fmn(X)ekn(Y)t].
Assumption 10. (i) Let E[|Yg(Y) - (Z)|4|X] C and Var(Yg(Y) - (Z)|X) c for some constants c, C > 0. (ii) The matrix TtnTn has full rank. (iii) There exist some constants c, C > 0 such that c n/kn C where n is the minimum eigenvalue of KtnQ-n1Kn. (iv) It holds Ekn g - g  = O(kn-+1/2) for  > 1/2 and kn  -   = O(kn-/dz+1/2) for  > dz/2. Since E[|Y|2|X] = E[|Y|2g(Y)|X] E[|Yg(Y)|2|X] it holds

Var(Yg(Y) - (Z)|X) E[|Yg(Y)|2|X] - 2 E[|Yg(Y)|2|X] E[2(Z)|X] + E[2(Z)|X]
= E[|Yg(Y)|2|X] - E[2(Z)|X] 2
E[|Y|2|X]/2 - E[2(Z)|X] Var(U|X)/4 - 3 E[2(Z)|X]/2
where we used twice the basic inequality (a - b)2 a2/2 - b2. Thus, Var(Yg(Y) - (Z)|X) is bounded from below if Var(U|X) is sufficiently large, more precisely, if Var(U|X) 2(c + 3 E[2(Z)|X]). On the other hand, assuming a bounded conditional fourth moment of the structural disturbance U (E[U4|X] const.) implies E[|Yg(Y) - (Z)|4|X] const. due to the finite support of Y. This condition can be similarly motivated as in Example 3.1. Assumption 10 (ii) further ensures that the asymptotic distribution is not degenerate. Let us introduce

Wn(z) = Var pkn (z)t KtnQ-n1Kn -1KtnQn-1 fmn (X)(Yg(Y) - (Z)) + Var pkn (z)t Knt Qn-1Kn -1Knt TYn Tnt Qn-1Tn -1TtnQ-n1 fmn (X) - E[ fmn (X)]

.

If there is no endogenous selection (that is, g  1) then the first part of the asymptotic variance formula coincides to the one obtained by Chen and Pouzo [2013] in nonparametric instrumental regression. Under endogenous selection, however, an additional term in the asymptotic variance arises due to the first step estimation of the inverse selection probability g. Let An = ZtknXmn(XtmnXmn)- Xmt nZkn/n . In the next result we replace the variance Wn(z) by the estimator

n
Wn(z) = n-1

pkn (z)tAn-(Ztkn Xmn )(Xmt n Xmn )- fmn (Xi) Yi gn(Yi) - n(Zi)

2
+

i=1

n

n-1

pkn (z)tAn-(Zktn Xmn /n)Xmt n diag(Y)Ykn

Ytkn Xmn (Xmt n Xmn )-Xmt n Ykn

-
×

i=1

Yktn Xmn (Xmt n Xmn )- fmn (Xi) -

n

2
fmn (Xi )

i =1

14

where diag(Y) is the diagonal matrix with entries on the diagonal given by (Y1, . . . , Yn). The next result establishes the asymptotic distribution of the estimator n evaluated at some point z in the support Z of Z.
Theorem 4.2. Let Assumptions 1 ­ 5, and 8 ­ 10 be satisfied. If

n

kn  - 

2 Z

=

o(1)

and

n

T(Ekn g -

g)

2 X

=

o(1)

(4.7)

then we have

n/Wn(z) n(z) - (z) d N(0, 1).

Moreover,

if

Y

is

bounded

and

mn

=

o(

 n

min(

 kn

2kn

,

 kn

2kn

))

then

n Wn(z) n(z) - (z) d N(0, 1).

5. A Model Specification Test

Our estimation procedure crucially relies on the conditional independence between selection and covariates given potential outcomes (see Assumption 1). Hence, it would be desirable to test the validity of this assumption before conducting the estimation procedure. An attractive feature of Assumption 1 is that it is indeed testable (cf. Theorem 2.4 in D'Haultfoeuille [2010]). In this section we construct a test for this assumption. As seen in Section 2, given Assumptions 2 and 3, Assumption 1 is equivalent to the operator equation Tg = 1. Let us consider a reasonable class of functions for g namely F =   LY2 : (·) 1 and  - Ekn Y Ckn- for any n 1 for some  > 0. The null hypothesis under consideration is
H0 : there exists a function g  F such that Tg = 1.

The test statistic. Our testing procedure is based on the criterion in (3.3). We verify

whether

n i=1

n2 (Xi ,

gn)

does

not

become

too

large,

which

is

the

case

if

the

true

inverse

conditional probability function g does not satisfy the minimal smoothness conditions

imposed by H0. By reformulating the quantity

n i=1

n2 (Xi

,

gn)

we

obtain

our

test

statistic

Sn =

n (gn(Yi)i - 1) fmn (Xi) t (Xtmn Xmn )-

n
(gn(Yi)i - 1) fmn (Xi)

i=1 i=1

(5.1)

where the dimension mn coincides with the first step dimension used for the estimator n. Our testing procedure builds on Breunig [2015]. But as we consider a constraint estimation procedure we cannot apply the method of Breunig [2015] directly. A constraint sieve testing procedure was proposed by Breunig [2013] but for the specific situation of quantile versions of instrumental variable models. In addition, note that in these two papers the basis functions used to construct the test statistics are assumed to be orthonormal, which is not required in the following.

15

Asymptotic distribution of the statistic. Our test statistic Sn is asymptotically normally distributed if it is standardized by appropriate mean and variance, which are introduced in the next definition.
Definition 5.1. Let us introduce the matrix

mn = E g(Y) - 1 2Q-n1/2 fmn (X) fmn (X)tQ-n1/2

Then the trace and the Frobenius norm of mn are respectively denoted by µmn and mn.
Assumption 11. There exist constants c, C > 0 such that Var(g(Y)|X) c and E[(g(Y) - 1)4|X] C.
 Due to Assumption 11 (iii) it holds mn C mn for some constant C > 0. The next result establishes asymptotic normality of Sn after standardization.
Theorem 5.1. Let Assumptions 1­6, and 11 be satisfied. If

m3n = o(n),

knm2n = O(nkn ),

and

max

kn, nkn kn-2

 = o( mn)

(5.2)

then it holds under H0

 ( 2mn )-1 nSn - µmn

d N(0, 1).

Estimation of Critical Values. For the estimation of critical values of Theorem 5.1,
t
let us define Un = 1gn(Y1) - 1, . . . , ngn(Yn) - 1 . We estimate the matrix mn by mn  (Xmt n Xmn )-1/2Xmt n diag(Un)2 Xmn (Xtmn Xmn )-1/2. The asymptotic result of Theorem 5.1 continues to hold if we replace mn by the Frobenius norm of mn, denoted by mn, and µmn by the trace of mn, denoted by µmn. Theorem 5.2. Let the assumptions of Theorem 5.1 be satisfied. Then it holds under H0
 ( 2mn )-1 nSn - µmn d N (0, 1).

Limiting behavior under local alternatives. In the following, we study the power of the test, that is, the probability to reject a false hypothesis against a sequence of linear local alternatives that tends to zero as the sample size tends to infinity. We consider alternative models defined through a sequence of functions gn that satisfies

Tgn - 1 - m1n/4n-1/2 X = o(m1n/4n-1/2)

(5.3)

for some function   L4X. Due to (5.3), for any n 1 the function gn does not satisfy Tgn = 1. The next result is a direct consequence of Proposition 2.4 of Breunig [2013] and thus, its proof is omitted.
Proposition 5.3. Given the conditions of Theorem 5.1 it holds under (5.3)

 ( 2mn )-1 n Sn - µmn d N 2-1/2

E[(X) fj(X)] 2, 1 .

j=1

16

 0.0 0.5 1.0

6. Monte Carlo simulation
In this section, we study the finite-sample performance of our test by presenting the results of a Monte Carlo simulation. There are 1000 Monte Carlo replications in each experiment. Let X = () where   N(0, 1). Further, generate Y from the model Y = (X) + cVV where (x) =  8(x - 0.5) with standard normal distribution function , cV = 0.4, and V  N(0, 1).
Estimation of conditional expectation. We consider estimation of the conditional expectation of Y given X. We generate realizations of the selection variable  from  
 estimator in (3.7) MAR estimator 0.0 0.2 0.4 0.6 0.8 1.0 x
Figure 1: The regression function , the median of n with 90% confidence intervals and an estimator under MAR assumption
Binomial(1, h(Y)) where h(y) = 0.4  1{y 0.4} + 1{y > 0.4}. We estimate the function  by
using the constraint estimator n given in (3.5). As basis functions we use B-splines of order 3 with 2 knots (hence kn = 6) and for the criterion function we use orthogonal B-splines of order 3 with 6 knots (hence mn = 10). Figure 1 depicts the median of the estimator n together with its 90% pointwise confidence bands and an estimator under the missing at random (MAR) assumption. The MAR estimator becomes more biased for small values of x and lies even outside of the pointwise confidence bands of n.
Nonparametric Specification Test. Results are presented for the nominal level 0.05. Let us now study the finite sample behavior of our nonparametric specification test.
We construct the observations of  via the function h(y) = 1/(10y + 2) + 0.5 for y  [0, 1]. If H0 holds true we generate   Binomial(1, h(Y)).
In the experiments where H0 fails, X is not a valid instrument in the sense that it influences the endogenous selection. In this case, we generate realizations of  from
  Binomial(1, (1 - )h(Y) + (X))
17

for some constant  > 0 and where (x) = 1 - (2x - 1)2. Clearly, if  = 0 then the null hypothesis H0 is true. We estimate the regression function  as in the previous paragraph.

Model  H0 true 1
2
3


0.2 0.3 0.5 0.2 0.3 0.5 0.2 0.3 0.5

Empirical Rejection probability ( 2mn )-1(nSn - µmn ) 0.046 0.221 0.389 0.562 0.374 0.677 0.814 0.659 0.909 0.942

Table 1: Empirical Rejection probabilities for Nonparametric Specification Test
In Table 1, we depict the empirical rejection probabilities of our test statistic with critical value 0.05. The critical values of these statistics are estimated as described in Theorem 5.2.
7. Empirical Illustration
In this section, we apply our estimation procedure to study the way in which the level of expenditure of an individual affects his/her expected "propensity to work".
We use data from the German Internet Panel (GIP)3. This data set contains data about individual attitudes and preferences which are relevant for political and economic decision-making processes. The survey represents the German speaking population aged 16 to 75 in Germany.
In our application we measure the "propensity to work" by the "number of desired hours" which is present in our data set. The latter variable is the number of weekly hours a person would like to work by taking into account that the income would change according to the hours of work. Let Y denote this variable and X denote the variable "expenditure". The latter measures the total average expenditure in one month of a person.
The object of interest in our study is the regression function of Y given X, that is, the expected number of desired hours given a level of monthly total expenditure.
In our data set we have 1118 observations, a small number of missing in the variable "expenditure" (56 observations) and a large number of missing in the variable "number of desired hours" (392 observations). As the number of missing values in "expenditure" is small we eliminate these observations from our data set (since the bias is going to be negligible) so that the sample size we work with becomes n = 1056 and the missing values in the variable "number of desired hours" are now 365. This shows that only less than half
3The GIP is a survey part of the Collaborative Research Centre "Political Economy of Reforms" (SFB 884) based at the University of Mannheim which studies the determinants and perceptions of political reforms and their consequences for the economy.

18

prop 0.2 0.4 0.6 0.8 1.0

0 10 20 30 40 50 60 y
Figure 2: Estimator of P( = 1|Y = y) with 90% percent confidence intervals.

of the missing in "expenditure" corresponds to missing in "number of desired hours". The fact that Y is not observed is likely to be endogenous since one could think that in "extreme" situations, where Y is either excessively low or excessively high, a person
would be more likely to not provide this information. While the variable "expenditure" is statistically related to Y, it is reasonable to assume that it is independent of the selection mechanism once Y is accounted for. In particular, we have implemented our specification
test proposed in Section 5. We have computed the test statistics for a grid of values for
mn and the maximum absolute value of the (standardized) test statistics is obtained for mn = 42 and is 0.1757. Therefore, our test fails to reject H0 at the level 5%.
Figure 2 depicts our estimator for the conditional probability P( = 1|Y) and we
observe that this estimated probability of reporting increases with potential desired hours
of working. The pointwise confidence intervals, however, are too wide to make significant
statements. A reason for this is that the conditional probability function is a solution to an
inverse problem and hence estimators can be imprecise in finite samples.

Mean Median Min Max Std.

n missing

Y = Y 22.05 X 1439

30 1225

0 60 18.2392 1056 373 0 6000 899.2948 1056 0

Table 2: Descriptive Statistics for Y = Y and X.
Figure 3 shows the estimated regression function of "number of desired hours" on "expenditure" together with the 90% percent confidence intervals. The estimator is based on the nonparametric methodology described in Section 3 where we use B-Splines where the first step was penalized by a smoothness matrix as described in Blundell et al. [2007] page 1638. The additional penalization avoids to stablize the estimates even if the number

19

desired working hours 20 25 30 35 40 45 50

0

1000

2000

3000

4000

5000

6000

expenditure

Figure 3: Regression curve of "number of desired hours" on "expenditure" with 90% percent confidence intervals.

of B-spline basis functions might be chosen too large. The B-splines used here are order 4 with 3 knots, hence kn = 8. We also report the estimated regression function of "number of desired hours" on "expenditure" estimated under the MAR-assumption in Figure 4. As we see from the figures the MAR estimator is only significantly different from our estimator when expenditure is small.

20

desired working hours 20 25 30 35 40 45 50

0

1000

2000

3000

4000

5000

6000

expenditure

Figure 4: Regression under MAR of "number of desired hours" on "expenditure" with 90% percent confidence intervals.

A. Appendix

Throughout the proofs, we will use C > 0 to denote a generic finite constant that may be

different in different uses. Imn denotes the mn × mn identity matrix. Further, for ease of

notation we write

i for

n i=1

and

i <i for

i

ii-=11. For a matrix A, we denote by A its

operator norm.

In the following, we denote Qn = n-1 i fmn(Xi) fmn(Xi)t. By Assumption 4, the eigenvalues of E[ fmn(X) fmn(X)t] are bounded away from zero and hence, it may be assumed that E[ fmn(X) fmn(X)t] = Imn (cf. Newey [1997], p. 161).

A.1. Proofs of Section 3.
Proof of Theorem 3.1. The proof is based on the following decomposition

n - 

2 X

2 Q-n1(Imn - Qn) Xmt n Gn/n - Qn E[(X) fmn (X)] 2

+4

Xtmn Gn/n - Qn E[(X) fmn (X)]

2+4

Fmn  - 

2 X

= 2In + 4IIn + 4IIIn (say). (A.1)

First observe that

In Q-n1 2 Qn - Imn 2 Xtmn Gn/n - Qn E[(X) fmn (X)] 2.

21

We have Qn - Imn 2 = m2n/n and Qn-1 2 = 1 + op(1) as Newey [1997] page 162. Further, we observe

mn
IIn 4 n-1

Yi gn(Yi) - (Ekn g)(Yi) fj(Xi) - TMid(gn - Ekn g), fj X 2

j=1 i

mn
+ 8 n-1

Yi (Ekn g)(Yi) fj(Xi) - E[Y(Ekn g)(Y) fj(X)] 2

j=1 i

mn
+ 4 n-1

mn
fj(Xi) fl(Xi) - E[ fj(X) fl(X)] E[Y fl(X)] 2

j=1 i l=1

mn mn

+2

TMid(gn

- Ekn g),

fj

2 X

+8

TMid(Ekn g -

g),

fj

2 X

j=1 j=1

= 4Bn1 + 8Bn2 + 4Bn3 + 2Bn4 + 8Bn5 (say).

Consider Bn1. The Cauchy Schwarz inequality implies

mn

Bn1 n - kn 2

n-1 Yiekn (Yi) fj(Xi) - E[Yekn (Y) fj(X)] 2

j=1 i

= n - kn 2 Op(knmn/n) = Op(mn/n)

where we used that kn n - kn 2 = Op(1). Consider Bn2. Since Var(YEkn g(Y)) is bounded we have

E Bn2

mn
n-1 E Y Ekn g(Y) fj(X) 2
j=1

n-1 sup fmn (x) 2 E Y Ekn g(Y) 2
xX

2n-1 sup

fmn (x) 2

Var(Y Ekn g(Y)) +

TMidEkn g

2 G

xX

= O(mn/n).

Consider Bn3. It holds
mn
E Bn3 = n-1 E | fj(X)(Fmn )(X)|2
j=1

2n-1 sup

fmn (x) 2

Fmn  - 

2 X

+



2 X

= O(mn/n).

xX

Consider Bn4. We observe

Bn4 =

Fmn TMid(gn

- Ekn g)

2 X

TMid(gn

- Ekn g)

2 X

sup
{Gkn :(·)

  1} 

TMid( - Ekn g)

2 X 

T( - Ekn g)

2 X



T(gn

- Ekn g)

2 X

22

Due to Assumption 5 (iv) we have

sup
{Gkn :(·)

  1} 

TMid( T( -

- Ekn g)

Ekn g)

2 X

2 X

  

<

.

From Blundell et al. [2007] page 1659 we deduce

T(gn

- Ekn g)

2 X

=

Op

m-n 2/dx

+ mn/n +

T(Ekn g -

g)

2 X

.

Consequently, we have

Bn4 = Op m-n2/dx + mn/n +

T(Ekn g -

g)

2 X

.

Further,

Bn5

Fmn T(Ekn g -

g)

2 X

=

O

T(Ekn g -

g)

2 X

,

which proves the result.

Proof of Corollary 3.2. It is sufficient to check that

sup
Gkn

TMid X T X

<

.

(A.2)

Let T denote the adjoint operator of T which is given by T = E[(W)|Y]. Since the multiplication operator Mid is a selfadjoint operator we obtain

TMid

2 X

=

TMid, TMid X =

T, (T)-1MidTTMid X

T X (T)-1MidTTMid X

From the link condition

T

2 X

c

 j=1



j

, ej

2 Y

we

infer

by

a

duality

argument

(T)-1

2 X

c-1

 j=1

-j 1

, ej

2Y.

Let

L

be

a selfadjoint operator

acting on

L2Y

with

eigenvalue

decompo-

sition {1j/2, ej}j 1. Then we conclude

TMid

2 X

c-1 T X L-1MidTTMid X

c-1 T X TMidL-1 X TMid X

which gives

TMid X T X

c-1 TMidL-1 X.

Consequently, (A.2) follows since TMidL-1 X is bounded.

Proof of Theorem 3.3. To bound the asymptotic variance Vn(x) from below we observe that assumption Var(Yg(Y)|X) C yields

Vn(x) fmn(x)t E fmn(X) Var(Yg(Y)|X) fmn(X)t fmn(x)/2 C fmn(x) 2.

In the following, we also make use of

Var(Yg(Y) - (X)|X) = E(|Yg(Y) - (X)|2|X) = E(|Yg(Y)|2|X) - 2(X) = Var(Yg(Y)|X).

23

The proof is based on the decomposition

n(x) - (x) = fmn(x)t(nQn)-1 fmn(Xi) Yi g(Yi) - (Xi)
i
+ fmn (x)t(nQn)-1 fmn (Xi)Yi gn(Yi) - Ekn g(Yi)
i
+ fmn (x)t(nQn)-1 fmn (Xi)Yi Ekn g(Yi) - g(Yi)
i
+ fmn (x)t(nQn)-1 fmn (Xi)(Xi) - (x)
i
= In + IIn + IIIn + IVn (say).

Let us denote kn = TtnTn -1Ttn E[ fmn(X)]. As in the proof of Theorem 3.1 and since E[ fmn(X)] = E[g(Y) fmn(X)] it is easily seen

IIn

=

 n

 fmn (x)t TnY(kn -kn )+ n

fmn (x)t TYn

TtnTn

-1Tnt E

fmn(X) g(Y)-Ekn g(Y)

+ op (1).

From Assumption 5 (iv) we infer

TYn Tnt Tn -1Tnt

sup
G

TMid X Fmn T X

Tn Tnt Tn -1Ttn

C Tn TtnTn -1Tnt = C

where we used the fact that projection matrices are bounded in the operator norm. The

Cauchy

Schwarz

inequality

together

with

n

T(g

-

Ekn g)

2 X

=

o(1)

implies

 IIn = n fmn (x)tTYn (kn - kn ) + op(1).

Consequently, we obtain

n/Vn(x) In + IIn =
i

nVn(x) -1/2 fmn (x)t fmn (Xi)(Yi g(Yi) - (Xi)) - TYn Tnt Tn -1Tnt fmn (Xi) - E[ fmn (X)]

+ op(1)

= sin + op(1)
i
Moreover, sin, 1 i n satisfy the Lindeberg condition which can be seen as follows. It holds E[sin] = 0 and n E[s2in] = 1. Following Newey [1997] (p. 164), for all  > 0 due to E |Yg(Y) - (X)|4 C we observe

1E[si2n {|sin|>}] n2 E |sin/|4 Cn-1-2mn2 = o(1).
i

Again

from

Assumption

5

(iv)

together

with

n

T(g

-

Ekn g)

2 X

=

o(1)

yields

nIIIn

=

op(1).

Finally,

nIVn

=

op(1)

follows

from

n

Fmn 

-



2 X

=

o(1),

which

completes

the

proof

of

the

first statement in the theorem.

By Lemma A.1 we have n Vn(x) n(x) - (x) = n Vn(x) n(x) - (x) (Vn(x)/Vn(x))1/2 d N(0, 1)
which proves the second result of the theorem.

24

Lemma A.1. Let Assumptions 1 ­ 5 and 7 be satisfied. Moreover, assume that Y is bounded. Then, Vn(x)-1Vn(x) = 1 + op(1)

where Vn(x) and Vn(x) are as defined in Theorem 3.3

Proof. Let us introduce some notation. We denote n = E fmn(X) Var(Yg(Y)|X) fmn(X)t ,

n, f = Imn - E[ fmn (X)] E[ fmn (X)t], n =

n i=1

fmn (Xi) fmn (Xi)t

Yi

gn(Yi)

-

n(Xi)

2
/n,

n, f

=

Qn - (Xmt 1n/n)(1nt Xm/n) and n =

i

fmn (Xi) fmn (Xi)t

Yi

g(Yi)

-

(Xi)

2
/n.

Moreover, let

h1t = (Vn(x))-1/2 fmn (x)tQn-1, h1t = (Vn(x))-1/2 fmn (x)t, Tn = Xmt n Ykn /n

ht2 = (Vn(x))-1/2 fmn (x)tQ-n1 Xmt n diag(Y)Ykn /n TtnQ-n1Tn -Tnt Qn-1

and h2t = (Vn(x))-1/2 fmn (x)tTYn Tnt Tn -1Tnt . Hence,

Vn(x) ht1nh1 + ht2n, f h2 = Vn(x)

and by noticing that h1t nh1 + h2t n, f h2 = 1, the triangle inequality gives

(Vn(x))-1/2Vn(x)(Vn(x))-1/2 - 1 h1t (n - n)h1 + h1t (n - n)h1 + h1t nh1 - h1t nh1

+ ht2(n, f - n, f )h2 + ht2n, f h2 - h2t n, f h2 . (A.3)

Remark that h1 - h1 = op(1) and h1 = Op(1). Consider h2. We have

(Tnt Q-n1Tn)-1-(Tnt Tn)-1

(TtnQn-1Tn)-1

= Op(-kn2n-1/2 mnkn),

as we see in the following. It holds

(Tnt Tn) - (Tnt Q-n1Tn)

(TtnTn)-1

(A.4)

Tnt QnTn - Tnt Tn

Tnt (Qn - Imn )Tn + Tnt (Tn - Tn) + Ttn(Tn - Tn)

= Ttn(Qn - Imn )Tn + (Tn - Tn)t(Tn - Tn) + 2Ttn(Tn - Tn)

Tnt (Qn - Imn )Tn 2 + Tn - Tn 2 1 + Qn - Imn + 2||Tn - Tn ||Tn|| + Qn - Imn

= Tnt (Qn - Imn )Tn 2 + op(1).

Moreover,

Tnt (Qn - Imn )Tn 2

kn
n-1 E E[el(Y) fmn (X)]t fmn (X) fmn (X)t E[ej(Y) fmn (X)] 2
j,l=1

 kn  = n-1 E j,l=1 (Fmn Tel)(X)(Fmn Tej)(X) 2

2
n-1 sup (Tekn )(x) 2
xX
 2
n-1 syuYp ekn(y) 2

= O(n-1kn2)

25

since (Tnt Q-n1Tn)-1

(Tnt Tn)-1 + op(1) and (Tnt Tn)-1

Ck-n1 by assumption 7 (iv),

Qn-1 - I 2 = Op(m2n/n), Xmt n diag(Y)Ykn /n - TnY 2 = Op(mnkn/n), Tn - Tn 2 = Op(mnkn/n).

Then, h2 - h2 2 = op(1) by Assumptions 4 (ii) and 5 (ii), (iv), and so h2 = Op(1).

Under Assumptions 4 (ii)-(iii) and 7 (i), we can show similarly as in Newey [1997] page 165

­ 166 that:

ht1nh1 - h1t nh1 = op(1)

and

ht1(n - n)h1 = op(1).

(A.5)

Moreover, denote Sn = n-1 i fmn(Xi) fmn(Xi)t|Yi g(Yi)-(Xi)|, Sn = E[ fmn(X) fmn(X)t|Y g(Y)-
(X)|], Dgn(·) = gn(·) - g(·) and Dn(·) = n(·) - (·) and remark that E Sn - Sn 2 Cm2n/n under Assumption 7 (i). Hence,

ht1(n - n)h1 = n-1

ht1 fmn (Xi) 2

22
Yi gn(Yi) - n(Xi) - Yi g(Yi) - (Xi)

i

n-1 ht1 fmn (Xi) 2 (YiDgn (Yi) - Dn (Xi))2
i

+ 2 n-1

h1t fmn (Xi) 2 Yi g(Yi) - (Xi) (YiDgn (Yi) - Dn (Xi))
i
op(1)h1t [Qn - Imn ]h1 + 2op(1)h1t [Sn - Sn]h1 + op(1) h1 2 + op(1)h1t Snh1 (A.6)

since supyY |y|2(supyY |Dgn (y)|)2 = Op max kn2/(nkn ), kn-2+1 and

2
sup |Dn(x)| = Op
xX

max

m-n 2/dx+1,

mn2 n

,

mn

T(Ekn g -

g)

2 X

by the Cauchy Schwartz inequality and both converge to zero under the assumptions of the theorem. Next, by the Cauchy-Schwarz inequality,

|h2t n, f h2 - ht2n, f h2|

|(h2 - h2)tn, f (h2 - h2) + 2h2t n, f h2 - 2h2t n, f h2|

|(h2 - h2)tn, f (h2 - h2)| + 2|(h2 - h2)tn, f h2|

= op(1).

(A.7)

Furthermore, by E n, f - n, f 2 Cm2n/n,

|h2t (n, f - n, f )h2|

n, f - n, f h2 2 = op(1).

(A.8)

Then, by (A.3), the triangle inequality and (A.5)­(A.8), the result of the lemma follows.

A.2. Proofs of Section 4.
Proof of Theorem 4.1. The proof is based on the inequality n -  Z kn  -  Z + n - kn  Z.

26

By assumption, we have kn -  Z = O(kn-/dz) and thus, it is sufficient to bound n -

kn  Z.

By Lemma B.2 of Chen and Pouzo [2012] it holds

n

-

kn 

2 Z

Ck-n1 K(n -

kn )

2 X

.

From

the

proof

of

Theorem

3.1

we

have

mn
n-1

Yi gn(Yi) fj(Xi) - E[Yg(Y) fj(X)] 2 = Op(rn)

j=1 i

(A.9)

where we denote rn = max

m-n 2/dx , n-1mn,

T(g - Ekn g)

2 X

. Consequently, we observe

n-1
i

Yi gn(Yi) - (kn )(Zi) pmn (Xi) 2

2 E Y g(Y) - (kn)(Z) pmn(X) 2 + Op(rn)

2

K(kn 

-

)

2 X

+

Op

rn

+

Fmn K(kn  - )

2 X

.

Further, using the elementary inequality (a - b)2 a2/2 - b2 and again applying relation (A.9) gives uniformly in 

n-1
i

Yi gn(Yi) - (Zi) fmn (Xi) 2 E Y - (Z) fmn (X) 2/2

kn
- max n-1
j=1 Bkn

i

Yi gn(Yi) - (Zi) fj(Xi) - E Y - (Z) fj(X) 2

C

K(kn 

-

)

2 X

-

Op

rn

+

Fmn K(kn  - )

2 X

.

For some  > 0 let us denote n = {  n :

K(-)

2 X

 rn} where rn = rn + Fmn K(kn -

)

2 X

.

Therefore,

following

the

proof

of

Lemma

B.1

of

Chen

and

Pouzo

[2012]

we

obtain

P

K(n

- )

2 X

P min
n i

n

Yi gn(Yi) - (Zi) fmn (Xi) 2

Yi gn(Yi) - (kn )(Zi) fmn (Xi) 2
i

P

min

K( - )

2 X

n

K(kn - )

2 X

+ Op(rn)

which goes to zero for all n 1 as   . This shows K(n - kn) X = Op(rn) and thus proves the result.
Proof of Theorem 4.2. Observe that the asymptotic variance Wn(z) is bounded from below. Indeed, from condition Var(Yg(Y) - (Z)|X) C we infer

Wn(z)

pkn (z)t KtnKn -1Knt E

fmn(X) Var(Yg(Y)

- (Z)|X) fmn(X)t

Kn

KtnKn

-1
pkn (z)

C pkn (z)t

KtnKn

-1
pkn (z)

27

which we use in the following. Let An-1 = Zktn Xmn (Xmt n Xmn )-1 Xtmn Zkn /n -1Zktn Xmn (Xtmn Xmn )-1 and A-n1 = (Knt Kn)-1Knt . Then, we make use of the decomposition

n(z) - (z) = pkn (z)tn-1A-n1 fmn (Xi) Yi g(Yi) - (Zi)
i
+ pkn (z)tn-1An-1 fmn (Xi)Yi gn(Yi) - Ekn g(Yi)
i
+ pkn (z)tn-1An-1 fmn (Xi)Yi Ekn g(Yi) - g(Yi)
i
+ pkn (z)tn-1A-n1 fmn (Xi)(Zi) - (z)
i
= In + IIn + IIIn + IVn (say).

Let us denote kn = Knt Kn -1Knt E[Yg(Y) fmn(X)]. As in the proof of Theorem 3.1 and since E[ fmn(X)] = E[g(Y) fmn(X)] it is easily seen

IIn =

 n pkn (z)t

Knt Kn

-1KtnTnY(kn - kn )

 + n pmn (z)t

KtnKn -1KtnTnY TtnTn -1Ttn E

fmn(X) g(Y) - Ekn g(Y)

+ op(1).

Since the largest eigenvalue of TnY Tnt Tn -1Ttn is bounded and Assumption 5 (iv) together

with

n

T(g - Ekn g)

2 X

=

o(1)

the

Cauchy

Schwarz

inequality

implies

IIn

=

 n pkn (z)t

Knt Kn

-1KtnTYn (kn

- kn) + op(1).

Consequently, we obtain

n/Wn(z) In + IIn =
i

nWn(z) -1/2pkn (z)tAn-1 fmn (Xi)(Yi g(Yi) - (Zi))
- KtnKn -1KtnTYn TtnTn -1Tnt fmn (Xi) - E[ fmn (X)] + op(1) = sin + op(1)
i

Moreover, sin, 1 i n satisfy the Lindeberg condition which can be seen as follows. It holds E[sin] = 0 and n E[s2in] = 1. From Assumption 5 (iv) we infer TYn TtnTn -1Tnt C Tn TtnTn -1Tnt = C and due to E |Yg(Y) - (X)|4 C we observe

1E[si2n {|sin|>}] n2 E |sin/|4 Cn-1-2m2n = o(1).
i

Again

from

Assumption

5

(iv)

together

with

n

T(g

-

Ekn g)

2 X

=

o(1)

yields

nIIIn

=

op(1).

Finally,

nIVn

=

op(1)

follows

from

n

kn 

-



2 Z

=

o(1),

which

completes

the

proof

of

the

first statement in the theorem.

28

To prove the second statement of the theorem, remark that by Lemma A.2, Wn(z)-1Wn(z) = 1 + op(1). Therefore, we obtain

n Wn(z) n(z) - (z) = n Wn(z) n(z) - (z) (Wn(z)/Wn(z))1/2 d N(0, 1)

which proves the second result of the theorem.

Lemma and mn

=A.o2(.LnetmAinss(umpkntio2knn s,

1 ­k5n , k2ann)d).

8 ­ 10 Then,

be

satisfied.

Moreover,

assume

that

Y

is

bounded

Wn(z)-1Wn(z) = 1 + op(1)

where Wn(z) and Wn(z) are as defined in Theorem 3.3.

Proof. Let n, f and n, f as defined in the proof of Lemma A.1. We denote n = E fmn(X) Var(Yg(Y)-

(Z)|X) fmn (X)t , n = n-1 i fmn (Xi) fmn (Xi)t Yi gn(Yi)-n(Zi) 2 and n = n-1 i fmn (Xi) fmn (Xi)t Yi g(Yi)-

(Xi)

2
.

Moreover,

let

An

=

(Ztkn Xmn /nQ-n1Xmt n Zkn /n),

An

=

KtnKn,

h3t = (Wn(z))-1/2pkn (z)tAn-1(Zktn Xmn /n)Q-n1,

ht3 = (Wn(z))-1/2pkn (z)tA-n1Ktn,

h4t = (Wn(z))-1/2pkn (z)tAn-1(Ztkn Xmn /n)Xmt n diag(Y)Ykn TtnQn-1Tn -Tnt Qn-1 and ht4 = (Wn(z))-1/2pkn (z)tA-n1 KtnTnY TtnTn -1Tnt . Hence, Wn(z) ht3n h3 + ht4n, f h4 Wn(x) and by noticing that h3t nh3 + h4t n, f h4 = 1 and by the triangle inequality

=

(Wn(z))-1/2Wn(z)(Wn(z))-1/2 - 1 h3t (n - n)h3 + h3t (n - n )h3
+ h3t nh3 - ht3n h3 + ht4(n, f - n, f )h4 + ht4n, f h4 - h4t n, f h4 . (A.10)

Remark that An-1 - A-n 1

An-1

An - An



A-n1 = Op(max(

knmn , n

mn n

)k-n2

)

since An-1 2 2 A-n1 2 + op(1), An-1 2 = -n1 Ck-n2, Xmt n Zkn /n - Kn 2 = Op(knmn/n) and Q-n1 - I 2 = Op(mn2/n). Then, h3 - h3 = op(1) and h3 = Op(1). By these results, (A.4), Xtmndiag(Y)Ykn/n - TnY 2 = Op(mnkn/n), Tn - Tn 2 = Op(mnkn/n), Assumptions 4 (ii) and 5 (ii), (iv) and the assumptions of the lemma we conclude that h4 - h4 2 = op(1) and so
h4 = Op(1). Under Assumptions 4 (ii) ­ (iii) and 10 (i), we can show similarly as in Newey [1997] page
165 ­ 166 that:

ht3nh3 - h3t n h3 = op(1)

and

ht3(n - n )h3 = op(1).

(A.11)

29

Moreover, denote i = Yi gn(Yi)-n(Xi) , i = Yi g(Yi)-(Xi) , Sn = n-1 i fmn (Xi) fmn (Xi)t|i |, Sn = E[ fmn(X) fmn(X)t||], Dgn(Yi) = gn(Yi) - g(Yi) and Dn(Xi) = n(Xi) - (Xi) and remark that E Sn - Sn 2 Cm2n/n under Assumption 10 (i). Hence,

n
ht3(n - n )h3 = n-1

h3t fmn (Xi) 2 ((i )2 - (i )2)

i=1

n
n-1

ht3 fmn (Xi) 2 (YiDgn (Yi) - Dn (Xi))2 + 2 n-1 n

h3t fmn (Xi) 2 i(YiDgn (Yi) - Dn (Xi))

i=1 i=1

op(1)h3t [Qn - Imn ]h3 + 2op(1)ht3[Sn - Sn]h3 + op(1) h3 2 + op(1)h3t Snh1 (A.12)

since supyY |y|2(supyY |Dgn (y)|)2 = Op max kn2/(nkn ), kn-2+1 and

 2 szuZp |Dn(x)| = Op

max

kn-2/dz+1,

kn2 nkn

,

knk-n1

T(Ekn g -

g)

2 X

and both converge to zero under the assumptions of the lemma. Next, by the CauchySchwarz inequality and similarly as in the proof of Lemma A.1 we get

|ht4n, f h4 - ht4n, f h4|

h4 - h4 2 + 2 (h4 - h4)tn, f (h4 - h4) 1/2 h4t n, f h4 1/2

C h4 - h4 2

= op(1).

(A.13)

Furthermore, by E n, f - n, f 2 Cm2n/n,

|ht4(n, f - n, f )h4|

n, f - n, f h4 2 = op(1).

(A.14)

Then, by (A.10) and (A.11)-(A.14), the result of the lemma follows.

A.3. Proofs of Section 5.

Proof of Theorem 5.1.



( 2mn )-1

mn j=1

|n-1/2

Since we have Qn - Imn 2 = op(mn2/n) it is sufficient to prove that i(i gn(Yi) - 1) fj(Xi)|2 - µmn d N(0, 1). The proof of this statement

is based on the decomposition

mn
|n-1

mn
(i gn(Yi) - 1) fj(Xi)|2 = n-1

i g(Yi) - 1 fj(Xi) 2

j=1 i

j=1 i

- 2 mn n2 j=1

i g(Yi) - 1 fj(Xi)
i

i gn(Yi) - g(Yi) fj(Xi)
i

mn
+ n-1

i gn(Yi) - g(Yi) fj(Xi) 2 = In - 2IIn + IIIn.

j=1 i

(A.15)

30

Consider In. We calculate

-m1n nIn - µmn

=

1 mn n

i

mn j=1

i g(Yi) - 1 fj(Xi) 2 - E g(Y) - 1 2 fj2(X)

+

1 mn n

i

i

mn j=1

i g(Yi) - 1

i g(Yi ) - 1 fj(Xi) fj(Xi )

where the first summand tends in probability to zero as n  . Indeed,we have

E

1 mn n

i

mn j=1

i g(Yi) - 1 fj(Xi) 2 - E g(Y) - 1 2 fj2(X) 2

1 2mn n E

mn j=1

g(Y) - 1 fj(X) 2 - E

g(Y) - 1 2 fj2(X) 2

1 m2 n

n

sup
xX

fmn(x) 4 E g(Y) - 1 4

Cm2n m2 n n

=

o(1).

Therefore,

to

establish

(

 2mn )-1(nIn

-

µmn )

d

N

(0,

1)

it

is

sufficient

to

show

 2
mn n i i

mn j=1

i g(Yi) - 1

i g(Yi ) - 1 fj(Xi) fj(Xi ) d N (0, 1).

This follows from Lemma A.2 of Breunig [2015]. Consider IIn. We observe

mn
nIIn =
j=1

i g(Yi) - 1 fj(Xi) n-1 i gn(Yi) - g(Yi) fj(Xi)

ii

mn
=
j=1

i g(Yi) - 1 fj(Xi) n-1 i gn(Yi) - Ekn g(Yi) fj(Xi)
ii

mn
+
j=1

i g(Yi) - 1 fj(Xi) n-1 i Ekn gn(Yi) - g(Yi) fj(Xi)
ii

= Cn1 + Cn2.

Consider Cn1. We have
Cn1 = Diag(1, . . . , kn )1/2(kn - kn )
mn
× i g(Yi) - 1 fj(Xi) Diag(1, . . . , kn)-1/2 E[ekn(Y) fj(X)] + op(1).
j=1 i

31

Using

mn
E
j=1

i g(Yi) - 1 fj(Xi) Diag(1, . . . , kn)-1/2 E[ekn(Y) fj(X)] 2
i

mn
nE

2
(g(Y) - 1) fj(X)

Diag(1, . . . , kn )-1/2 E[ekn (Y) fj(X)] 2

j=1

Cn Diag(1, . . . , kn )-1/2 E[ekn (Y) fmn (X)t] 2.

Since Diag(1, . . . , kn)-1/2 E[ekn(Y) fmn(X)t] 2 = O(kn) it holds

Cn1 = Diag(1, . . . , kn )1/2(kn - kn ) Op(

 knn) = Op(kn) = op( mn).

Further, we have

mn

E |Cn2|

E |(Ekn g - g)(Y) fj(X)|2 E (i g(Y) - 1) fj(X) 2

j=1

mn

+ Cn1/2 E

T(Ekn g - g), fj X fj(X)

j=1



C mn Ekn g - g  + n T(Ekn g - g) X

=

O(mnkn1/2-

+

 nkn

kn-)

=

 o( mn)

where we used that mnkn1-2 = o(1). Consider IIIn. It holds true that

IIIn Cn (kn - kn )t E[ekn (Y) fmn (X)t] 2

+ Cn

T(Ekn g -

g)

2 X

= Op kn + nkn kn-2 
= op( mn)

which completes the proof of the first result in the theorem.

 (Pro2omf no)f-1Thµemonr-emµm5n.2.mmRnn e. mThareksttahtaetm(en2tomfnt)h-e1

 nSn - µmn = ( 2mn )-1 nSn - µmn theorem follows from the results of

+mn
mn
Theo-

rem 5.1, Lemma A.3 and Lemma A.4.

Lemma A.3. Let Assumptions 1 ­ 6 be satisfied. Then,

m-1n mn = 1 + op(1) where mn and mn are as defined in Theorem 5.1.

32

Proof. Let · F denote the Frobenius norm of a matrix. Then mn = mn F and mn = mn F. Let us denote mn = n-1 i fmn(Xi) fmn(Xi)t(Ekn g(Yi)i - 1)2. Observe that Observe

mn - mn

2 F

= n-1 fmn (Xi) fmn (Xi)t |(gn - Ekn g)(Yi)|2i + 2(Ekn g(Yi)i - 1)i(gn - Ekn g)(Yi)
i

2 n-1

fmn (Xi) fmn (Xi)t|(gn - Ekn g)(Yi)|2i

2 F

i

+ 2 n-1

fmn (Xi) fmn (Xi)t(Ekn g(Yi)i - 1)i(gn - Ekn g)(Yi)

2 F

i

= 2In + 2IIn.

2 F

We further calculate

In

1 n

i(kn

- kn )tekn (Yi) fmn (Xi) fmn (Xi)tekn (Yi)t|(kn

- kn )

2 F

i

(kn

- kn )t E[ekn (Y) fmn (X) fmn (X)tekn (Y)t](kn

- kn )

2
+ op(1)
F

mn
kn - kn 4 E[ ekn (Y) 2| fj(X) fl(X)|]2 + op(1)

j,l=1

Cm2n kn - kn 4 E ekn (Y) 2 2 + op(1)

= Op m2nkn4/(kn n)2 = op(1)

 by using kn = o( mn). Similarly, we conclude

IIn

1 n

(Ekn g(Yi)i - 1) fmn (Xi) fmn (Xi)tekn (Yi)t|(kn

- kn )

2 F

i

E[(Ekn g(Y) - 1) fmn (X) fmn (X)tekn (Y)t](kn

- kn )

2
F + op(1)

mn kn
kn - kn 2

2
E[(Ekn g(Y) - 1)el (Y) fj(X) fl(X)] + op(1)

j,l=1 l =1

mn kn

= kn - kn 2

Fmn T((Ekn g - 1) · el ) ·

fj

2 X

+

op(1)

j=1 l =1

= Op mnkn2/(kn n) = op(1)

33

 by using kn = o( mn). Next,

E

mn - mn

2 F

=

mn
E
j,l=1





1 n

i

2 fj(Xi) fl(Xi)(Ekn g(Yi)i - 1)2 - E fj(X) fl(X)(g(Y) - 1)2 

1 n

mn
E

f

2 j

(X)

fl2(X)(Ekn

g(Y)

-

1)4

mn
+

E fj(X) fl(X)(Ekn g(Y) - g(Y))2

j,l=1 j,l=1

C

m2n n

+

mn

Fmn T(Ekn g - g) ·

fj

2 X

= O(m2nn-1 + mnkn1-2) = o(1)

j=1

2

Finally, by these results and the reverse triangle inequality we conclude that

m-1n mn - 1 = -m1n mn F - mn F -m1n mn - mn F + m-1n mn - mn F = op(1) which proves the result. Lemma A.4. Let Assumptions 1­6 be satisfied. Then,

µmn = µmn + op(mn )

where µmn and µmn are as defined in Theorem 5.1.

Proof.

The proof of Lemma A.3 establishes

mn - mn

F

=

op(1).

In

particular, 

convergence

of the trace of mn to the trace of mn follows by using |µmn - µmn|

mn mn - mn F =

op(mn ).

References
H. Ahn and J. L. Powell. Semiparametric estimation of censored selection models with a nonparametric selection mechanism. Journal of Econometrics, 58(1):3­29, 1993.
C. Ai and X. Chen. Efficient estimation of models with conditional moment restrictions containing unknown functions. Econometrica, 71:1795­1843, 2003.
A. Belloni, V. Chernozhukov, D. Chetverikov, and K. Kato. Some new asymptotic theory for least squares series: Pointwise and uniform results. Journal of Econometrics, 186(2): 345 ­ 366, 2015.
R. Blundell, X. Chen, and D. Kristensen. Semi-nonparametric iv estimation of shapeinvariant engel curves. 75(6):1613­1669, 2007.
C. Breunig. Specification testing in nonparametric instrumental quantile regression (submitted). Technical report, 2013.
C. Breunig. Goodness-of-fit tests based on series estimators in nonparametric instrumental regression. Journal of Econometrics, 184(2):328­346, 2015.
C. Breunig and J. Johannes. Adaptive estimation of functionals in nonparametric instrumental regression. Econometric Theory, pages 1­43, 2011.

34

G. Chamberlain. Asymptotic efficiency in semiparametric models with censoring. Journal of Econometrics, 32:189­218, 1986.
K. Chen. Parametric models for response-biased sampling. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(4):775­789, 2001.
X. Chen and D. Pouzo. Estimation of nonparametric conditional moment models with possibly nonsmooth moments. Econometrica, 80(1):277­322, 2012.
X. Chen and D. Pouzo. Sieve quasi likelihood ratio inference on semi/nonparametric conditional moment models. Technical report, Cowles Foundation for Research in Economics, Yale University, 2013.
X. Chen and M. Reiß. On rate optimality for ill-posed inverse problems in econometrics. Econometric Theory, 27:497­521, 2011.
X. Chen, H. Hong, and D. Nekipelov. Nonlinear models of measurement errors. Journal of Economic Literature, 49(4):901­937, 2011.
V. Chernozhukov, S. Lee, and A. M. Rosen. Intersection bounds: Estimation and inference. Econometrica, 81:667­737, 2013.
S. Darolles, Y. Fan, J.-P. Florens, and E. Renault. Nonparametric instrumental regression. Econometrica, 79(5):1541­1565, 2011.
M. Das, W. K. Newey, and F. Vella. Nonparametric estimation of sample selection models. The Review of Economic Studies, 70(1):33­58, 2003.
L. Davezies and X. D'Haultfoeuille. Partial identification with missing data. mimeo, 2013.
C. De Boor. A practical guide to splines. Mathematics of Computation, 1978.
X. D'Haultfoeuille. A new instrumental method for dealing with endogenous selection. Journal of Econometrics, 154(1):1­15, 2010.
X. D'Haultfoeuille. On the completeness condition in nonparametric instrumental problems. Econometric Theory, 27(3):460, 2011.
X. D'Haultfoeuille and A. Maurel. Another look at identification at infinity of sample selection models. Econometric Theory, 29, 2013.
J. Heckman. Shadow prices, market wages, and labor supply. Econometrica, 70:679­694, 1974.
J. Heckman. Sample selection bias as a specification error. Econometrica, 47:153­161, 1979.
M. Henry, Y. Kitamura, and B. Salanie´. Partial identification of finite mixtures in econometric models. Quantitative Economics, 5:123­144, 2014.
S. Hoderlein, L. Nesheim, and A. Simoni. Semiparametric estimation of random coefficients in structural economic models. CEMMAP Woking paper, CWP09/12, 2012.
Y. Hu and S. Schennach. instrumental variable treatment of nonclassical measurement error models. Econometrica, 76:195­216, 2008a.
35

Y. Hu and S. Schennach. Instrumental variable treatment of nonclassical measurement error models. Econometrica, 76(1):195­216, 2008b.
A. Lewbel. Endogenous selection or treatment model estimation. Journal of Econometrics, 141:777­806, 2007.
C. F. Manski. Anatomy of the selection problem. The Journal of Human Resources, 24:343­360, 1989.
C. F. Manski. The selection problem. In C. E. Sims, editor, Advances in Econometrics, Sixth World Congress. Cambridge University Press., 1994.
W. K. Newey. Convergence rates and asymptotic normality for series estimators. Journal of Econometrics, 79(1):147 ­ 168, 1997.
W. K. Newey and J. L. Powell. Instrumental variable estimation of nonparametric models. Econometrica, 71:1565­1578, 2003.
E. A. Ramalho and R. J. Smith. Discrete choice non-response. The Review of Economic Studies, 80(1):343­364, 2013.
D. B. Rubin. Inference and missing data. Biometrika, 63:581­592, 1976. E. Tamer. Partial identification in econometrics. Annual Review of Economics, 2:167­195,
2010. G. Tang, R. J. Little, and T. E. Raghunathan. Analysis of multivariate missing data with
nonignorable nonresponse. Biometrika, 90(4):747­764, 2003.
36

SFB 649 Discussion Paper Series 2015

For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

001 002
003
004 005
006 007 008 009 010 011 012 013 014 015 016 017
018 019 020 021

"Pricing Kernel Modeling" by Denis Belomestny, Shujie Ma and Wolfgang Karl Härdle, January 2015. "Estimating the Value of Urban Green Space: A hedonic Pricing Analysis of the Housing Market in Cologne, Germany" by Jens Kolbe and Henry Wüstemann, January 2015. "Identifying Berlin's land value map using Adaptive Weights Smoothing" by Jens Kolbe, Rainer Schulz, Martin Wersing and Axel Werwatz, January 2015. "Efficiency of Wind Power Production and its Determinants" by Simone Pieralli, Matthias Ritter and Martin Odening, January 2015. "Distillation of News Flow into Analysis of Stock Reactions" by Junni L. Zhang, Wolfgang K. Härdle, Cathy Y. Chen and Elisabeth Bommes, January 2015. "Cognitive Bubbles" by Ciril Bosch-Rosay, Thomas Meissnerz and Antoni Bosch-Domènech, February 2015. "Stochastic Population Analysis: A Functional Data Approach" by Lei Fang and Wolfgang K. Härdle, February 2015. "Nonparametric change-point analysis of volatility" by Markus Bibinger, Moritz Jirak and Mathias Vetter, February 2015. "From Galloping Inflation to Price Stability in Steps: Israel 1985­2013" by Rafi Melnick and till Strohsal, February 2015. "Estimation of NAIRU with Inflation Expectation Data" by Wei Cui, Wolfgang K. Härdle and Weining Wang, February 2015. "Competitors In Merger Control: Shall They Be Merely Heard Or Also Listened To?" by Thomas Giebe and Miyu Lee, February 2015. "The Impact of Credit Default Swap Trading on Loan Syndication" by Daniel Streitz, March 2015. "Pitfalls and Perils of Financial Innovation: The Use of CDS by Corporate Bond Funds" by Tim Adam and Andre Guettler, March 2015. "Generalized Exogenous Processes in DSGE: A Bayesian Approach" by Alexander Meyer-Gohde and Daniel Neuhoff, March 2015. "Structural Vector Autoregressions with Heteroskedasticy" by Helmut Lütkepohl and Aleksei Netsunajev, March 2015. "Testing Missing at Random using Instrumental Variables" by Christoph Breunig, March 2015. "Loss Potential and Disclosures Related to Credit Derivatives ­ A CrossCountry Comparison of Corporate Bond Funds under U.S. and German Regulation" by Dominika Paula Galkiewicz, March 2015. "Manager Characteristics and Credit Derivative Use by U.S. Corporate Bond Funds" by Dominika Paula Galkiewicz, March 2015. "Measuring Connectedness of Euro Area Sovereign Risk" by Rebekka Gätjen Melanie Schienle, April 2015. "Is There an Asymmetric Impact of Housing on Output?" by Tsung-Hsien Michael Lee and Wenjuan Chen, April 2015. "Characterizing the Financial Cycle: Evidence from a Frequency Domain Analysis" by Till Strohsal, Christian R. Proaño and Jürgen Wolters, April 2015.

SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2015

For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

022 023
024 025 026 027 028 029
030 031 032 033
034 035 036 037
038 039 040 041 042 043

"Risk Related Brain Regions Detected with 3D Image FPCA" by Ying Chen, Wolfgang K. Härdle, He Qiang and Piotr Majer, April 2015. "An Adaptive Approach to Forecasting Three Key Macroeconomic Variables for Transitional China" by Linlin Niu, Xiu Xu and Ying Chen, April 2015. "How Do Financial Cycles Interact? Evidence from the US and the UK" by Till Strohsal, Christian R. Proaño, Jürgen Wolters, April 2015. "Employment Polarization and Immigrant Employment Opportunities" by Hanna Wielandt, April 2015. "Forecasting volatility of wind power production" by Zhiwei Shen and Matthias Ritter, May 2015. "The Information Content of Monetary Statistics for the Great Recession: Evidence from Germany" by Wenjuan Chen and Dieter Nautz, May 2015. "The Time-Varying Degree of Inflation Expectations Anchoring" by Till Strohsal, Rafi Melnick and Dieter Nautz, May 2015. "Change point and trend analyses of annual expectile curves of tropical storms" by P.Burdejova, W.K.Härdle, P.Kokoszka and Q.Xiong, May 2015. "Testing for Identification in SVAR-GARCH Models" by Helmut Luetkepohl and George Milunovich, June 2015. "Simultaneous likelihood-based bootstrap confidence sets for a large number of models" by Mayya Zhilova, June 2015. "Government Bond Liquidity and Sovereign-Bank Interlinkages" by Sören Radde, Cristina Checherita-Westphal and Wei Cui, July 2015. "Not Working at Work: Loafing, Unemployment and Labor Productivity" by Michael C. Burda, Katie Genadek and Daniel S. Hamermesh, July 2015. "Factorisable Sparse Tail Event Curves" by Shih-Kang Chao, Wolfgang K. Härdle and Ming Yuan, July 2015. "Price discovery in the markets for credit risk: A Markov switching approach" by Thomas Dimpfl and Franziska J. Peter, July 2015. "Crowdfunding, demand uncertainty, and moral hazard - a mechanism design approach" by Roland Strausz, July 2015. ""Buy-It-Now" or "Sell-It-Now" auctions : Effects of changing bargaining power in sequential trading mechanism" by Tim Grebe, Radosveta Ivanova-Stenzel and Sabine Kröger, August 2015. "Conditional Systemic Risk with Penalized Copula" by Ostap Okhrin, Alexander Ristig, Jeffrey Sheen and Stefan Trück, August 2015. "Dynamics of Real Per Capita GDP" by Daniel Neuhoff, August 2015. "The Role of Shadow Banking in the Monetary Transmission Mechanism and the Business Cycle" by Falk Mazelis, August 2015. "Forecasting the oil price using house prices" by Rainer Schulz and Martin Wersing, August 2015. "Copula-Based Factor Model for Credit Risk Analysis" by Meng-Jou Lu, Cathy Yi-Hsuan Chen and Karl Wolfgang Härdle, August 2015. "On the Long-run Neutrality of Demand Shocks" by Wenjuan Chen and Aleksei Netsunajev, August 2015.

SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2015

For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

044 045 046 047 048 049 050

"The (De-)Anchoring of Inflation Expectations: New Evidence from the Euro Area" by Laura Pagenhardt, Dieter Nautz and Till Strohsal, September 2015. "Tail Event Driven ASset allocation: evidence from equity and mutual funds' markets" by Wolfgang Karl Härdle, David Lee Kuo Chuen, Sergey Nasekin, Xinwen Ni and Alla Petukhina, September 2015. "Site assessment, turbine selection, and local feed-in tariffs through the wind energy index" by Matthias Ritter and Lars Deckert, September 2015. "TERES - Tail Event Risk Expectile based Shortfall" by Philipp Gschöpf, Wolfgang Karl Härdle and Andrija Mihoci, September 2015. "CRIX or evaluating Blockchain based currencies" by Wolfgang Karl Härdle and Simon Trimborn, October 2015. "Inflation Co-movement across Countries in Multi-maturity Term Structure: An Arbitrage-Free Approach" by Shi Chen, Wolfgang Karl Härdle, Weining Wang, November 2015. "Nonparametric Estimation in case of Endogenous Selection" by Christoph Breunig, Enno Mammen and Anna Simoni, November 2015.

SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

