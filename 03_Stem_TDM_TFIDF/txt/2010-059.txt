BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2010-059
Nonparametric Regression with Nonparametrically Generated Covariates
Enno Mammen* Christoph Rothe** Melanie Schienle***
* Universität Mannheim, Germany ** Toulouse School of Economics, France *** Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Nonparametric Regression with Nonparametrically Generated Covariates
Enno Mammen, Christoph Rothe, and Melanie Schienle
University of Mannheim, Toulouse School of Economics, and Humboldt University Berlin
This Version: September 2010
Abstract We analyze the properties of non- and semiparametric estimation procedures involving nonparametric regression with generated covariates. Such estimators appear in numerous econometric applications, including nonparametric estimation of simultaneous equation models, sample selection models, treatment effect models, and censored regression models, but so far there seems to be no unified theory to establish their statistical properties. Our paper provides such results, allowing to establish asymptotic properties like rates of consistency or asymptotic normality for a wide range of semi- and nonparametric estimators. We also show how to account for the presence of nonparametrically generated regressors when computing standard errors. JEL Classification: C14, C31 Keywords: Empirical Process, Propensity Score, Control Variable Methods, Semiparametric Estimation
Enno Mammen, Department of Economics, University of Mannheim, D-68131 Mannheim, Germany. E-mail: emammen@rumms.uni-mannheim.de. Christoph Rothe, Toulouse School of Economics, 21 All´ee de Brienne, F31000 Toulouse, France. E-mail: rothe@cict.fr. Melanie Schienle, School of Business and Economics, Humboldt University Berlin, Spandauer Str. 1, D-10178 Berlin, Germany. E-mail: melanie.schienle@wiwi.hu-berlin.de. The third author acknowledges support of Deutsche Forschungsgemeinschaft (DFG) via the Collaborative Research Center 649 "Economic Risk".
1

1 Introduction
A wide range of econometric applications requires nonparametric estimation of a regression function when some of the covariates are not directly observed, but have themselves only been estimated nonparametrically in a preliminary step. A prominent example is the estimation of structural functions in triangular simultaneous equation models (e.g. Newey, Powell, and Vella, 1999; Blundell and Powell, 2004; Imbens and Newey, 2009), which requires conditioning on an estimate of a control variable to account for endogeneity. Other applications involving "nonparametrically generated regressors" include sample selection models (Das, Newey, and Vella, 2003), treatment effect models (Heckman, Ichimura, and Todd, 1998), and censored regression models (Linton and Lewbel, 2002), amongst many others. In contrast to parametric regression problems with generated regressors, where general results are nowadays included in most graduate textbooks (e.g. Wooldridge, 2002, Chapter 6.1), the statistical properties of their nonparametric counterparts are not well understood, with results typically only being derived in the specific context determined by the respective application.
This paper provides a unified theory to analyze a wide class of estimators in models involving nonparametric regression with nonparametrically generated covariates. Our main result is that the presence of pre-estimated covariates affects the first-order asymptotic properties of the estimated regression function only through a smoothed version of the first-stage estimation error, reducing the "curse of dimensionality" to a secondary concern in this context. Based on this new insight, we derive simple and explicit stochastic expansions that can not only be used to establish asymptotic normality or the rate of consistency of the estimated regression function itself, but also to study the properties of more complex estimators, in which estimation of a regression function merely constitutes an intermediate step. Examples for the latter case include structured nonparametric models imposing e.g. additive separability (Stone, 1985), and semiparametric M-estimators involving infinite dimensional nuissance parameters (e.g. Andrews, 1994; Newey, 1994b; Chen, Linton, and Van Keilegom, 2003). Our results thus cover a wide range of econometric models, and should therefore be of general interest.
Our paper considers nonparametric estimation of a regression function m0(x) = E(Y |r0(S) = x) when the function r0 is unknown, but can be estimated from the data. In particular, we study the properties of the estimator mLL obtained through local linear regression (Fan and Gijbels, 1996) of the dependent variable Y on the generated covariates R^ = r^(S), where r^ is some nonparametric estimate of r0 from a first stage. Using results from empirical process theory,
2

we show that the presence of generated covariates affects the first-order asymptotic properties of mLL only through a smoothed version of the estimation error r^(s) - r0(s). This additional smoothing typically improves the rate of convergence. In order to achieve a certain rate of convergence of the estimator mLL it is thus not necessary that the estimator r^ converges with the same rate or a faster one. This result, which apparently has not been noted before in the literature, constitutes the main contribution of this paper. It has the important implication that the curse of dimensionality is only a secondary concern when working with nonparametrically generated covariates.
Our main result can e.g. directly be used to establish asymptotic normality or uniform rates of consistency of the estimate of m0. Furthermore, we can derive a formula for the asymptotic variance that accounts for the presence of generated covariates. This is demonstrated in the present paper for the important special case that r0 is the conditional mean function in an auxiliary nonparametric regression. Extensions to other settings are immediate. Our result also provides a convenient way to analyze the properties of more complex estimation procedures, in which estimation of m0 constitutes an intermediate step. In this paper, we consider three substantial econometric applications exhibiting such a structure in greater detail: nonparametric estimation of a simultaneous equation model, nonparametric estimation of a censored regression model, and estimation of average treatment effects via regression on the nonparametrically estimated propensity score. The types of technical difficulties encountered in these examples are representative for those in a wide range of econometric applications.
It should be stressed that our main result does neither require the generated regressors to emerge from a specific type of model, nor do we require a specific procedure to estimate them. In particular, our main result holds irrespective of whether the function r0 is a regression function or a density, or whether it is estimated by kernel methods, orthogonal series or sieves. Moreover, our results also applies in settings where r0 is estimated using parametric or semiparametric restrictions. Our analysis only requires two fairly weak general conditions ensuring uniform consistency of the estimator r^, and that the function r0 is not too complex. Both are straightforward to verify in practice. Our main result, however, is specific to using a local linear smoother for obtaining the final estimate of m0. In particular, our proofs make use of certain technical properties of this estimator that are not shared by other common methods. While it might be possible to derive a result similar to our main finding for other methods such as orthogonal series or sieves by e.g. extending results in Song (2008), we conjecture that this
3

would require a substantially more involved technical argument.1 As noted above, parametric estimation of models with generated regressors has a long tra-
dition in econometrics. We refer to Pagan (1984) or Oxley and McAleer (1993) for extensive surveys of the literature. More recently, a number of papers have studied models with nonparametrically generated regressors. Imbens and Newey (2009) use nonparametric estimates of control variables to correct for endogeneity in triangular structural equation models with nonseparable disturbances. Similar techniques are used by Newey, Powell, and Vella (1999) for simultaneous equation models with additive disturbances, Blundell and Powell (2004) and Rothe (2009) for single-index binary choice models with endogenous regressors, and Ahn and Powell (1993) and Das, Newey, and Vella (2003) for the estimation of sample selection models with a nonparametrically specified selection mechanism. Linton and Lewbel (2002) face nonparametrically generated covariates when estimating a regression function under fixed censoring of the dependent variable. Lewbel and Linton (2007) consider estimation of a homothetically separable functions. Rilstone (1996) uses generated regressors to reduce the dimensionality of certain nonparametric regression problems. In the literature on program evaluation, Heckman, Ichimura, and Todd (1998) consider estimating the average treatment effect on the treated through regression on the estimated propensity score. Conditioning on an estimate of a propensity score is also required for computing the Marginal Treatment Effect discussed in Heckman and Vytlacil (2005, 2007) and Carneiro, Heckman, and Vytlacil (2009, 2010). Similar issues also appear for the estimation of a generalized Roy model in d'Haultfoeuille and Maurel (2009). There are also several applications in financial econometrics. Kanaya and Kristensen (2009) consider fitting a stochastic volatility model using the nonparametric estimate of the instantaneous volatility process in Kristensen (2009). Conrad and Mammen (2009) consider nonand semiparametric specifications of GARCH-in-Mean models where generated covariates are iteratively plugged into a nonparametric mean equation. They make use of empirical process
1Song (2008) considers series estimation of the functional g(x, r) = E(Y |r(X) = x) indexed by x  X  R and r  , where  is a function space with finite integral bracketing entropy, and derives a rate of consistency uniformly over (x, r)  X × . He thus considers a related but different estimand (he considers the functional (x, r)  g(x, r) whereas we consider the function x  g(x, r0) for some fixed function r0). For our setting, in a first step one needs a result on the rates for the difference between estimates of g(x, r) and g(x, r0) uniformly over x  X and r taking values in a shrinking neighborhood of a fixed function r0. The rates are different from the case where r only takes values in a fixed neighborhood. Furthermore, qualitatively different mathematical techniques are required, in particular if one needs stochastic expansions of the difference and not only rates. This will be demonstrated in this paper in the case of local linear smoothing for the estimation of g(x,r).
4

methods that are related to the approach of this paper. The aforementioned papers typically rely on restrictions implied by the respective application for their asymptotic analysis. Moreover, in some cases only limited results such as upper bounds on the rate of convergence are derived. In contrast, the results in our paper are not tied to a specific model, and are thus easy to use when developing new applications. They can also be used to derive new and improved results concerning the asymptotic properties of many existing estimators for which so far only a limited analysis has been available.
To the best of our knowledge, there are only few papers on nonparametric regression with generated covariates not tailored to a specific application. Andrews (1995) shows that it is easy
 to establish properties of kernel-based estimators in the presence of parametrically (i.e. nconsistent) generated covariates, but such arguments do not carry over to the nonparametric case. Sperlich (2009) provides some bias and variance calculations for kernel estimators using predicted variables. To derive his results, he assumes a particular stochastic expansion for the generated covariates. This expansion includes a remainder term satisfying certain moment and inequality conditions that are not fulfilled by standard smoothing estimators. His assumptions also lead to asymptotic results that are different from the ones obtained in the present paper. Finally, in a recent contribution Hahn and Ridder (2010) consider the asymptotic variance of semiparametric M-estimators based on nonparametrically generated covariates, generalizing classic results by Newey (1994b). Their approach is to derive the influence function of the estimator of the finite dimensional parameter vector heuristically, i.e. without giving explicit regularity conditions on the estimators involved. In contrast, our paper provides a complete asymptotic theory for nonparametric regression with generated covariates, that would be needed to implement the results in Hahn and Ridder (2010) for a specific estimator. Furthermore, whereas Hahn and Ridder (2010) focus on the estimation of finite dimensional parameters in certain semiparametric settings, our paper deals with the properties of nonparametric regression with generated covariates in general.
The outline of this paper is as follows. In the next section, we describe our setup in detail and give some motivating examples. Section 3 establishes the asymptotic theory and states the main results. Section 4 provides a number of useful extensions. In Section 5, we apply our results to the examples given in Section 2, thus illustrating their application in practice. Finally, Section 6 concludes. All proofs are collected in the Appendix.
5

2 Nonparametric Regression with Generated Covariates

2.1 Model and Estimation Procedure
The nonparametric regression model with generated regressors can be written as

Y = m0(r0(S)) +  with E(|S) = 0,

(2.1)

where Y is the dependent variable, S is a p-dimensional vector of covariates, m0 : Rd  R and r0 : Rp  Rd is an unknown function and  is an error term that has mean zero conditional

on the observed covariates. We assume that there is additional information available outside of

the basic model (2.1) such that the function r0 is identified. For example, r0 could be (some

known transformation of) the mean function in an auxiliary nonparametric regression, which

may involve another random vector T in addition to Y and S.

Our aim is to estimate the function m0(r) = E(Y |r0(S) = r). Since r0 is unobserved,

obtaining a direct estimator based on a nonparametric regression of Y on R = r0(S) is clearly

not feasible. We therefore consider the following two-stage procedure. In the first stage, an

estimate r^ of r0 is obtained. We do not prescribe a specific estimator for this step. Instead, we

only impose the high-level restrictions that the estimator r^ is uniformly consistent, converging at

a rate specified below, and takes on values in a function class that is not too complex. Depending

on the nature of the function r0, these kind of regularity conditions are typically satisfied by

various common nonparametric estimators, such as kernel-based procedures or series estimators,

under suitable smoothness restrictions. In the second step, we then obtain our estimate mLL of m0 through a nonparametric regression of Y on the generated covariates R^ = r^(S), using local

linear smoothing. That is, our estimator is given by mLL(x) = , where

n
(, ) = argmin (Yi -  - T (Ri - x))2Kh(Ri - x),
, i=1

with Kh(u) =

d j=1

K(uj /hj )/hj

a

d-dimensional

product

kernel

built

from

the

univariate

kernel

function K, and h = (h1, ..., hd) a vector of bandwidths that tend to zero as the sample size n

tends to infinity.

For the later asymptotic analysis, it will be useful to compare mLL to an infeasible estimator

mLL that uses the true function r0 instead of an estimate r^. Such an estimator can be obtained

by local linear smoothing of Y versus R = r0(S), i.e. it is given by mLL(x) = , where

n
(, ) = argmin (Yi -  - T (Ri - x))2Kh(Ri - x).
, i=1

6

In order to distinguish these two estimators, we refer to mLL in the following as the real estimator, and to mLL as the oracle estimator.
Our use of local linear estimators in this paper is based on the following considerations. First, in a classical setting with fully observed covariates, estimators based on local linear regression are known to have attractive properties with regard to boundary bias and design adaptivity (see Fan and Gijbels (1996) for an extensive discussion) and they allow a complete asymptotic description of their distributional properties. In the present setting with generated covariates, these properties simplify the asymptotic treatment. The design adaptivity leads to a discussion of bias terms that do not require regular densities for the randomly perturbed covariates, and the complete asymptotic theory allows a clear description how the final estimator is affected by the estimation of the covariates. On the other hand, our assumptions on the estimation of the covariates are rather general and can be verified for a broad class of smoothing methods including sieves and orthogonal series estimators.
2.2 Motivating Examples
There are many econometric applications which involve nonparametric estimation of a regression function using nonparametrically generated covariates. Here we focus on three motivating examples. In this section we state their setup and explain how they fit into our framework. In Section 5, we show how our general high-level results given in the following section can be used to study their asymptotic properties in detail.
2.2.1 Regression on the Propensity Score
Propensity score methods are widely used in the program evaluation literature (see e.g. Imbens (2004) for an extensive review). Their popularity is due to the famous result by Rosenbaum and Rubin (1983) that when all confounders are observable, biases due to nonrandom selection into the program can be removed by conditioning on the propensity score, which is defined as the probability of selection into the program given the confounders. To be specific, let Y1, Y0 be the potential outcomes with and without program participation, respectively, D  {0, 1} an indicator of program participation, Y = Y1D + Y0(1 - D) be the observed outcome, X a vector of confounders, i.e. exogenous covariates, and let (x) = Pr(D = 1|X = x) be the propensity score. A typical object of interest in this context is the average treatment effect (ATE), defined as
AT E = E(Y1 - Y0).
7

When the assignment of the participation status is unconfounded, i.e. Y1, Y0D|X, the ATE can be estimated by various procedures, which may or may not make use of the propensity score. Examples include matching estimators and propensity score reweighting estimators (see Imbens (2004) for references and further examples). A method that has so far not been analyzed in detail uses nonparametric regression on the estimated propensity score. As shown by Rosenbaum and Rubin (1983), unconfounded assignment implies that Y1, Y0D|(X), and thus we have that d() = E(Y |D = d, (X) = ). The ATE is therefore identified through the relationship

AT E = E(1((X)) - 0((X))).

(2.2)

Similar arguments can be made for other measures of program effectiveness (e.g. Heckman, Ichimura, and Todd, 1998). Estimating the ATE by a sample analogue of (2.2) requires nonparametric estimation of 1() and 0(). We can cover this in our framework (2.1) with (Y, S) = (Y, (D, X)) and r0(S) = (D, (X)).

2.2.2 Nonparametric Simultaneous Equation Models

Another field of application for our results is the analysis of nonparametric estimators that use control variable techniques to account for endogeneity. The key idea of this approach is to introduce additional conditioning variables which fully capture the dependence between covariates and the unobserved heterogeneity. Such control variables appear naturally in many settings, but are often not directly observable and have to be estimated from the data. Consider for example the estimation of nonparametric simultaneous equation models with additive disturbances discussed in Newey, Powell, and Vella (1999). These authors study a triangular system of equations of the form

Y = µ1(X1, Z1) + U X1 = µ2(Z1, Z2) + V,

(2.3) (2.4)

imposing the restrictions that E(V |Z1, Z2) = 0, E(U ) = 0 and E(U |Z1, Z2, V ) = E(U |V ). The last conditions follows e.g. if the instruments Z = (Z1, Z2) are jointly independent of the disturbances (U, V ) and if the disturbances have mean zero. Now let m(x1, z1, v) = E(Y |X1 = x1, Z1 = z1, V = v). An implication of this model is that

m(x1, z1, v) = µ1(x1, z1) + (v),

where (v) = E(U |V = v). Newey, Powell, and Vella (1999) proposed a series estimator of the structural function µ1 that exploits this additive separability. An alternative approach to

8

estimating µ1, which we pursue in this paper, is to use the method of marginal integration (Newey, 1994a; Linton and Nielsen, 1995). This method relies on the fact that

m(x1, z1, v)fV (v)dv = µ1(x1, z1),

(2.5)

where fV is the probability density function of V . An estimate of µ1 can thus be obtained from a sample version of (2.5). Since the residuals V are not directly observed but have themselves to be estimated by some nonparametric method, estimation of the function m fits into our framework with (Y, S) = (Y, (X1, Z1, Z2), X1) and r0(S) = (X1, Z1, X1 - µ2(Z1, Z2)).
Remark 1. Imbens and Newey (2009) consider a generalized version of the above simultaneous equation model where the disturbances may not enter the equations additively. This model fits into the framework of this paper but requires a careful analysis of additional boundary problems that go beyond the scope of this paper. We will therefore study this model in a separate paper.

Remark 2. An alternative to marginal integration would be an approach based on smooth backfitting (Mammen, Linton, and Nielsen, 1999). Smooth backfitting estimators avoid several problems encountered by marginal integration in case of covariates with moderate or high dimension, but involves a more involved statistical analysis which is beyond the scope of the present paper. Results on smooth backfitting with nonparametrically generated covariates will be presented in a separate paper.

2.2.3 Nonparametric Censored Regression
As a final example, consider the nonparametric estimator of a regression function in the presence of fixed censoring proposed by Linton and Lewbel (2002). Consider the model

Y = max(0, µ0(X) - U ),

(2.6)

where U is an unobserved mean zero error term that is assumed to be independent of the

covariates X. Fixed censoring is a common phenomenon in many economic applications, e.g.

the analysis of wage data. Note that the censoring threshold could be different from zero, as

long as it is known. Linton and Lewbel (2002) establish identification of the function µ0 under

the tail condition limu- uFU (u) = 0 on the distribution function FU of U . In particular,

they show that the function µ0 can be written as

0 1

µ0(x) = 0 -

dr, r0(x) q0(r)

(2.7)

9

where r0(x) = E(Y |X = x), q0(r) = E(I{Y > 0}|r0(X) = r), and 0 is some suitably chosen constant. An estimate of the function µ0 can then be obtained from a sample analogue of (2.7), i.e. through numerical integration of a nonparametric estimate of the function q0(r)-1. Nonparametric estimation of q0 involves nonparametrically generated regressors, and thus fits into our framework with (Y, S) = (I{Y > 0}, X) and r0(S) = r0(X).

3 Asymptotic Properties

It is straightforward to show that mLL consistently estimates the function m0 under standard

conditions. Obtaining refined asymptotic properties, however, requires more involved argu-

ments. Our main result, derived in this section, is a stochastic expansion of the difference

between the real and the oracle estimator, in which the leading term turns out to be a kernel-

weighted average of the first stage estimation error. This important finding can e.g. be used to

obtain uniform rates of consistency for the real estimator, or to prove its asymptotic normality.

This is demonstrated explicitly for the case that r^ results from a local polynomial conditional

mean regression.

Throughout this section, we use the notation that for any vector a  Rd the value amin =

min1jd aj denotes the smallest of its elements, a+ =

d j=1

aj

denotes

the

sum

of

its

elements,

a-k = (a1, . . . , ak-1, ak+1, . . . , ad) denotes the d - 1-dimensional subvector of a with the kth

element removed, and ab = (a1b1, . . . , abdd) for any vector b  Rd.

3.1 Assumptions
In order to analyse the asymptotic properties of the local linear estimator with nonparametrically generated regressors, we make the following assumptions.2
Assumption 1 (Regularity Conditions). We assume the following properties for the data distribution, the bandwidth, and kernel function K.
(i) The sample observations (Yi, Si), i = 1, . . . , n are independent and identically distributed.
(ii) The random vector R = r0(S) is continuously distributed with compact support IR = IR,1 × ... × IR,d. Its density function fR is twice continuously differentiable and bounded away from zero on IR.
2At various points in this section, we will impose assumptions on the rates at which certain quantities tend to zero. We prefer to formulate these assumption without including (various powers of) logarithmic terms. This simplifies the notation for the theorems and proofs at the cost of only a minor loss in generality.

10

(iii) The function m0 is twice continuously differentiable on IR.
(iv) E[exp(||)|S]  C almost surely for a constant C > 0 and  > 0 small enough.
(v) The kernel function K is a twice continuously differentiable, symmetric density function with compact support, say [-1, 1].
(vi) The bandwidths h = (h1, . . . , hd) satisfies hj  n-j for j = 1, . . . , d and + < 1.
Assumption 1 contains mostly standard conditions from the literature on kernel-based nonparametric regression, with the exception of Assumption 1 (iv). This assumption restricts the distribution of the error term  to have subexponential tails conditional on S. This is a technical condition that will be needed to apply certain results from empirical process theory in our proofs.
Assumption 2 (Accuracy). The components rj and r0,j of r and r0, respectively, satisfy
sup |rj(s) - r0,j(s)| = oP (n-j )
s
for some j > j and all j = 1, . . . , d.
Assumption 2 is a "high-level" restriction on the accuracy of the estimator r^. It requires each component of the estimate of the function r0 to be uniformly consistent, converging at rate at least as fast as the corresponding bandwidth in the second stage of the estimation procedure. Such results are widely available for all common nonparametric estimators. See e.g. Masry (1996) for results on the Nadaraya-Watson, local linear and local polynomial estimators, or Newey (1997) for series estimators.
Assumption 3 (Complexity). There exist sequences of sets Mn,j such that
(i) Pr(rj  Mn,j)  1 as n   for all j = 1, . . . , d. (ii) For a constant CM > 0 and a function rn,j with rn,j - r0,j  = o(n-j ), the set Mn,j =
Mn,j  {rj : rj - rn,j   n-j } can be covered by at most CM exp(-j nj ) balls with · -radius  for all   n-j , where 0 < j  2, j  R and ·  denotes the supremum
norm.
Assumption 3 requires the first-stage estimator r^ to take values in a function space Mn,j that is not too complex, with probability approaching 1. Here the complexity of the function space is measured by the cardinality of the covering sets. This is a typical requirement for many
11

results from empirical process theory. See Van der Vaart and Wellner (1996) for details. The

second part of Assumption 3 is typically fulfilled under suitable smoothness restrictions. For

example, suppose that Mn,j is the set of functions defined on some compact set IS  Rp whose partial derivatives up to order k exist and are uniformly bounded by some multiple of nj for some j  0. Then Assumption 3(ii) holds with j = p/k and j = jj (Van der Vaart and Wellner, 1996, Corollary 2.7.2). For kernel-based estimators of r0, one can then verify part (i)

of Assumption 3 by explicitly calculating the derivatives. Consider e.g. the one-dimensional

Nadaraya-Watson estimator rn,j with bandwidth of order n-1/5. Choose rn,j equal to r0,j plus

asymptotic bias term. Then one can check that the second derivative of rn,j - rn,j is absolutely

bounded

by

OP

 ( log

n)

=

oP

(nj )

for

all

j

>

0.

For

sieve

and

orthogonal

series

estimators,

Assumption 3(i) immediately holds when the set Mn,j is chosen as the sieve set or as a subset

of the linear span of an increasing number of basis functions, respectively.

3.2 The Key Stochastic Expansion

With the assumptions described in the previous section, we are now ready to state our main
result, a stochastic expansion of our real estimator mLL(x) around the oracle estimator mLL(x). The results explicitly characterizes the influence of the presence of nonparametrically generated
regressors on the final estimator of the regression function m0. To state the theorem, let ^ (x) = , where
n
(, ) = argmin ((r(Si) - r0(Si)) -  - T (r0(Si) - x))2Kh(r0(Si) - x),
, i=1
and define the set IR-,n = {x  IR : the support of Kh(· - x) is a subset of IR}.
Theorem 1. Suppose Assumptions 1­3 hold. Then

sup mLL(x) - mLL(x) + m0(x)T ^ (x) = OP (n-)
xIR
where  = min{1, ..., 3} with

1 <

1 (1
2

-

+)

+

(

-

)min

-

1 2

max (jj
1jd

+

j ),

2 < 2min + ( - )min,

3 < min + ( - )min.

Uniformly over x  IR-,n we have that

1
^ (x) = n

n i=1

Kh(r0(Si) - x)(r(Si) -

1 n

n i=1

Kh(r0(Si

)

-

x)

r0(Si))

+

OP

(n-).

(3.1)

12

The leading term in the above expansion of the real estimator mLL(x) around the oracle estimator mLL(x) is given by the product of the derivative of m0 and a smoothed version of the first-stage estimation error r(s) - r0(s). In order to achieve a certain rate of convergence for the real estimator it is thus not necessary to have an estimator of r0 that converges with the same rate or a faster one, since the asymptotic properties of the estimator using nonparametrically generated regressors only depend on a smoothed version of the first-stage estimation error. While smoothing does not affect the order of the deterministic bias part, it typically reduces the variance and thus allows for less precise first-stage estimators. Another implication of the theorem is that using generated regressors has asymptotically negligible consequences in regions where the regression function is flat, since m0(x) = 0 in this case.
Remark 3. In Theorem 1 no assumptions are made about the process generating the data for estimation of r0. In particular, nothing is assumed about dependencies between the errors in the pilot estimation and the regression errors i. We conjecture that better rates than n- can be proven under such additional assumptions, but the results would only be specific to the respective full model under consideration. One way to extend our approach to such a setting would be to use our empirical process methods to bound the remainder term of higher order differences between m and m, and to treat the leading terms of the resulting higher order expansion by other more direct methods.
Remark 4. One could also derive an explicit representation of the term ^ (x) for values of x near the boundary of the support of R. This would be similar to the one given in (3.1), but involve weighting by more complicated kernel functions.
3.3 Two-Stage Nonparametric Regression
Theorem 1 can be used to derive asymptotic properties of the real estimator mLL, such as uniform rates of consistency or pointwise asymptotic normality in various econometric models. In this subsection, we demonstrate how explicit forms of the results in Theorem 1 can be obtained in the specific case that r0 is the conditional expectation function in an auxiliary nonparametric regression. Then we show how these can be employed to derive desired asymptotic properties. The chosen setting is arguably the most common way nonparametrically generated covariates appear in practice, and all the applications we consider in detail in this paper are either of this or a very closely related form.
13

We consider a "two-stage" nonparametric regression model given by

Y = m0(r0(S)) + , T = r0(S) + ,

where  is an unobserved error term that satisfies E[|S] = E[|S] = 0. For simplicity, we

focus on the case that R = r0(S) is a one-dimensional covariate, but generalizations to multiple

generated covariates or the presence of additional observed covariates are immediate.

Our strategy for deriving asymptotic properties of mLL in this framework is as follows: We first derive an explicit representation for the adjustment term ^ (x) from Theorem 1, which can

then be combined with standard results about the oracle estimator mLL. In order to obtain such

a result, it is convenient to use a kernel-based smoother in the first stage to estimate r0. Since the bias of ^ (x) is of the same order as of this first-stage estimator, we propose to estimate the

function r0 via q-th order local polynomial smoothing, which includes the local linear estimator

as the special case q = 1. Formally, the estimator is given by r^(s) = ^, where


n

2

(^, ^) = argmin Ti -  -

rT (Si - s)u Lg(Si - s)

, i=1

1u+q

(3.2)

and Lg(s) =

p j=1

L(sj /g)/g

is

a

p-dimensional

product

kernel

built

from

the

univariate

kernel

L, g is a bandwidth, which for simplicity is assumed to be the same for all components, and

1u+q denotes the summation over all u = (u1, . . . , up) with 1  u+  q. When r0 is sufficiently smooth, the asymptotic bias of local polynomial estimators of order q is well-known

to be of order O(gq+1) uniformly over x  IR (if q is uneven), and can thus be controlled.

A further technical advantage of using local polynomials is that the corresponding estimator

admits a certain stochastic expansion under general conditions, which is useful for our proofs.

We make the following assumption, which is essentially analogous to Assumption 1 except for

Assumption 4(iii). This additional assumption requires higher order smoothness of the kernel,

necessary to bound the k-th derivative of the estimator r. This allows to verify the Complexity

Assumption 3 for r.

Assumption 4. We assume the following properties for the data distribution, the bandwidth, and kernel function L.

(i) The observations (Si, Yi, Ti) are i.i.d. and the random vector S is continuously distributed with compact support IS = IS,1 × ... × IS,p. Its density function fS is bounded and bounded away from zero on IS. It is also differentiable with a bounded derivative.

14

(ii) The function r0 is q + 1 times continuously differentiable on IS.

(iii) The kernel function L is a k-times continuously differentiable, symmetric density function with compact support, say [-1, 1], for some natural number k  max{2, p/2}.

(iv) The bandwidth satisfies g  n- for some 0 <  < 1/p.

To simplify the presentation, we also assume that the function r0(s) is strictly monotone in at least one of its arguments, which can be taken to be the last one without loss of generality. This Assumption could be easily removed at the cost of a substantially more involved notation in the following results.

Assumption 5. The function r0(s-p, u) is strictly monotone in u, and r0(s-p, (s-p, x)) = x for some twice continuously differentiable function .

The following Lemma shows that in the present context, the function ^ (x) can be written

as the sum of a smoothed version of the first stage estimator's bias function, a kernel-weighted

average of the first-stage residuals 1, . . . , n, and some higher order remainder terms. For a concise presentation of the result we introduce some particular kernel functions. Let L denote

the p-dimensional equivalent kernel of the local polynomial regression estimator, given in (A.22)

in the Appendix, and define the one-dimensional kernel functions

Hg(x, s) =

1 L g

u1,

(s-p, x) g

-

sp

-

1(s-p, x)u1

du1,

(3.3)

Mh(x, s) = Kh r0(s) - x - r0(s)uh L(u)du.

(3.4)

Then, with this notation, we obtain the following Lemma.

Lemma 1. Suppose that Assumptions 1, 4 and 5 hold. Then we have that, uniformly over

x  IR,

^ (x) = ^ A(x) + ^ B(x) + Op

log(n) ngp

+ Op

log(n) (ngp)1/2(nh)1/2

,

where ^ B(x) = Op(gq+1) and ^ A(x) = Op((log(n)/(n max{g, h}))1/2). Moreover, uniformly over x  IR-,n, it is ^ B(x) = gq+1E[b(S)|r0(S) = x]+op(gq+1) with a bounded function b(s) given in (A.21) in the Appendix, and the term ^ A(x) allows for the following expansions uniformly over x  IR-,n, depending on the limit of g/h:

a) If g/h  0 then

^ A(x)

=

1 nfR(x)

n i=1

Kh(r0(Si)

-

x)i

+

Op

g 2 log(n) 1/2 .
h nh

15

b) If h = g then

^ A(x)

=

1 nfR(x)

n i=1

Mh(x, Si)i

+

op(n-1/2).

c) If g/h   then

^ A(x)

=

1 nfR(x)

n i=1

Hg(x, Si)x(S-p,i, x)i

+ Op

h 2 log(n) 1/2 .
g ng

It should be emphasized that in all three cases of the Lemma the leading term in the expression for ^ A(x) is equal to an average of the error terms i weighted by a one-dimensional kernel function, irrespective of p = dim(S). The dimension of the covariates thus affects the properties of ^ (x) only through higher-order terms. Furthermore, it should be noted that one can also derive expressions of ^ (x) similar to the ones above for values of x close to the boundary of the support. Likewise these take the form of a one-dimensional kernel weighted average of the error terms i plus a higher-order term. The corresponding kernel function, however, has a more complicated closed form varying with the point of evaluation.

Remark 5. The previous lemma can easily be modified in two directions. First, if the secondorder kernel function K is replaced with a kernel function of order k, the order of the remainder term in the representation of ^ A(x) can be strengthened to Op((g/h)k(nh/ log(n))-1/2) in case a) of the Lemma, and to Op((h/g)k(nh/ log(n))-1/2) for case c), under appropriate smoothness conditions. The expansions in Lemma 1 also continue to hold if the local polynomial estimator of r0 is replaced by a Nadaraya-Watson estimator with a higher order kernel function whose moments up to order q equal zero.

Combining Theorem 1 and Lemma 1 with well-known results about the oracle estimator mLL, various asymptotic properties of the real estimator mLL can be derived. In the following theorems we present results in the most relevant scenarios, addressing uniform rates of consistency, stochastic expansions of order oP (n-2/5) for proving pointwise asymptotic normality, and a more refined expansion of order oP (n-1/2) that is useful when m0 is estimated as an intermediate step in a semiparametric problem.
Starting with considering uniform rate of consistency, it is well-known (Masry, 1996) that under Assumption 1 the oracle estimator satisfies
sup |mLL(x) - m(x)| = Op((log(n)/nh)1/2 + h2) .
xIR
This implies the following result.

16

Theorem 2. Suppose that Assumptions 1, 4 and 5 hold. Then

sup |mLL(x) - m(x)| = Op
xIR

log(n)1/2 (nh)1/2

+ h2

+

log(n) ngp

+

log(n) (ngp)1/2(nh)1/2

+ gq+1

+ n-

.

Straightforward calculations show that the term of order OP (n-) is dominated by the other

remainder terms if  < max{(1/2 - )/p, (1 - 7/2)/p, (1 - 3/2)/(p + q + 1)}. Similarly, under

appropriate smoothness restrictions, all of the last four terms on the right-hand side of the last

equation can be made strictly smaller than the first two ones given an appropriate choice of

 and . One can thus recover the oracle rate for the real estimator, even if the first-stage

estimator converges at a strictly slower rate. Next, we derive stochastic expansions of mLL of order oP (n-2/5) for the case that  =

1/5. Such expansions immediately imply results on pointwise asymptotic normality of the real

estimator. It turns out that applying Theorem 1 requires p < 3/10 in this case. Therefore,

in order to use expansions a) and b) of Lemma 1, only p = 1 is admissible, i.e. S must be

one-dimensional in order for choices of  with    to be feasible. We will consider this case

in the next theorem. The case of oversmoothed pilot estimation with  <  will be discussed in

Theorem 4.

Theorem 3. Suppose that Assumptions 1, 4 and 5 hold with  = 1/5 and p = q = 1 Then the following expansions hold uniformly over x  IR-,n:

a) If 1/5 <  < 3/10 then

mLL(x)

-

m0(x)

=

1 nfR(x)

n i=1

Kh(r0(Si)

-

x)(i

-

m0(x)i)

+ 1 h2 2

u2K(u)du m0(x) + op n-2/5 .

In particular, we have

(nh)1/2(mLL(x)

-

m0(x)

-

1 h2 2

u2K(u)du m0(x)) d N (0, m2 (x))

where m2 (x) = Var( - m0(R)|R = x) K(t)2dt/fR(x) is the asymptotic variance.

b) If  = 1/5 then

mLL(x)

-

m0(x)

=

1 nfR(x)

n i=1

Kh(r0(Si)

-

x)i

-

Khx(r0(Si)

-

x)i

+

1 (x)h2 2

+

op

n-2/5

,

where Kx(v) = K(v - r (r-1(x))u)L(u)du is a kernel that depends on x and the bias is

given by (x) = u2K(u)du m0(x) - u2L(u)du r0 (r0-1(x))m0(x). In particular, we have

(nh)1/2(mLL(x) - m0(x) - (x)h2) d N (0, m2 (x))

17

where now m2 (x) = [Var(|R = x) K(t)2dt-2m0(x)E(|R = x) K(t)Kx(t)dt+m0(x)2× Var(|R = x) Kx(t)2dt]/fR(x) is the asymptotic variance.

We can see that under the conditions of the theorem the limiting distribution of mLL(x) is affected by the pilot estimation step. In particular, if  >  the estimator mLL(x) has the same limiting distribution as the local linear estimator in the hypothetical regression model

Y = m0(r0(S)) + ,

where  =  - m0(r0(S)). Depending on the curvature of m0 and the covariance of  and , the asymptotic variance of the estimator using generated regressors can be bigger or smaller than that of the oracle estimator mLL.
The next theorem discusses the case when  < . For such a choice of bandwidth, the limit distribution of mLL is the same as for the oracle estimator mLL. The effect exerted by the presence of nonparametrically generated regressors is thus asymptotically negligible for conducting inference on m0 in this case.

Theorem 4. Suppose that Assumptions 1, 4 and 5 hold with  <  = 1/5. Then the following

expansion

holds

uniformly

over

x



IR-,n

if

2 5

(q

+

1)-1

<

<

3 10

p-1:

mLL(x) = mLL(x) + op n-2/5

=

m0(x)

+

1 nfR(x)

n i=1

Kh(r0(Si)

-

x)i

+

1 h2 2

In particular, we have

u2K(u)du m0(x) + op n-2/5 .

(nh)1/2(mLL(x)

-

m0(x)

-

1 h2 2

u2K(u)du m0(x)) d N (0, m2 (x))

where m2 (x) = Var(|R = x) K(t)2dt/fR(x) is the asymptotic variance.

When the bandwidth parameters are chosen such that  < , i.e we have that g/h  , we can also derive stochastic expansions of mLL of order oP (n-1/2) for choices of  > 1/4. This type of expansion is often needed for the analysis of semiparametric problems in which m0 plays the role of an infinite dimensional nuisance parameter. Examples include estimation of weighted averages or weighted average derivatives of m0, or more generally the class of semiparametric M-estimators (e.g. Newey (1994b), Andrews (1994) or Chen, Linton, and Van Keilegom (2003)).
Compared to the expansion of order oP (n-2/5) in the previous Theorem, expansions of order oP (n-1/2) contain an additional higher order term that accounts for estimation errors in the pilot estimation step.

18

Theorem 5. Suppose that Assumptions 1, 4 and 5 hold with  > . Under these conditions, the

following

expansions

hold

uniformly

over

x



IR-,n

if



>

1/4

and

1 2

(q

+

1)-1

<



<

1 2

(1

-

3)p-1:

mLL(x)

-

m0(x)

=

1 nfR(x)

n i=1

Kh(r0(Si)

-

x)i

-

m0(x)

1 nfR(x)

n i=1

Hg(x, Si)x(S-p,i, x)i

+ op

n-1/2

.

Note that the conditions of the last two theorems impose restrictions on the smoothness

of the function r0. To obtain the expansion of order oP (n-2/5) in Theorem 4 we need that

q+1 >

10 3

2 5

p

=

4 3

p.

For the expansion of order oP (n-1/2) in Theorem 5 it is necessary that

q + 1 > (1 - 3)-1p > 4p. Thus, in both cases the required number of derivatives q has to

increase linearly with the dimension of the respective covariates p. In Section 4.3, we discuss a

modified version of the real estimators that requires weaker smoothness conditions.

4 Extensions

4.1 Estimation of Derivatives

In certain applications, it is necessary to estimate the derivatives of the regression function

m0, instead of the function itself. One example from the literature on program evaluation is the estimation of the Marginal Treatment Effects (MTE), which is defined as the derivative of

the conditional expectation of an outcome variable given the (usually unobserved) propensity

score. See e.g. Heckman and Vytlacil (2005, 2007) or Carneiro, Heckman, and Vytlacil (2009,

2010) for details. In this section, we discuss extensions of the results in the last section to the

estimation of derivatives of m0. We consider an estimator based on local quadratic fits. The theory of the last section could also be extended to higher order derivatives (by using higher

order local polynomials), but we restrict our analysis to first order derivatives because of their importance in econometrics. We define the real estimator of the derivative as mLQ(x) = , where with Ri = r(Si)

n
(, , ) = argmin

Yi -  - T (Ri - x) - (Ri - x)T (Ri - x)

2
Kh(Ri - x).

,, i=1

Furthermore, the oracle estimator is defined as mL Q(x) =  with

n

(, , ) = argmin

Yi -  - T (Ri - x) - (Ri - x)T (Ri - x) 2 Kh(Ri - x),

,, i=1

19

where Ri = r0(Si). We also define (x) =  by
n
(, , ) = argmin (-m0(Ri)T (Ri - Ri) -  - T (Ri - x) - (Ri - x)T (Ri - x))2Kh(Ri - x).
,, i=1
With this notation, we can state a result analogous to Theorem 1.

Theorem 6. Suppose Assumptions 1­3 hold and assume additionally that the function m0 is three-times continuously differentiable on IR. Then it holds for 1  j  d with ,j = min(1,j , 2,j , 3,j ), 1,j = j + 1, 2,j = j - min + 2, 3,j < j - 2min and 1, 2 as in Theorem 1 that

sup mL Q,j (x) + mLQ,j (x) - (x) = OP (n-,j ).
xIR

(4.1)

Furthermore, uniformly over x  IR-,n we have with ,j = min(,j, j + 3) and 3 as in Theorem 1, that

(x) =

1 n

n

Kh(r0(Si) - x)(r0(Si) - x)(r0(Si) - x)T

-1

i=1

1 n

n

Kh(r0(Si) - x)(r0(Si) - x)m0(x)T (r(Si) - r0(Si)) + OP (n-,j ).

i=1

(4.2)

For the important special case that r0 is a conditional expectation function estimated by local polynomials, one can derive results analogous to those obtained in Section 3.3 by using

the same type of arguments. These are omitted here for the sake of brevity.

4.2 Design Densities with Unbounded Support
One of the assumptions used to derive the stochastic expansion in Theorem 1 is that the covariates R = r0(S) have bounded support. In this subsection, we relax this condition, allowing R to be supported on an arbitrary subset of Rp. This result might be helpful in settings involving unbounded covariates, or more generally covariates whose density tends zero in certain areas. We make the following assumption.
Assumption 6. The variable R = r0(S) is continuously distributed with support IR  Rq. Its density has a bounded continuous derivative.
Generalizing Theorem 1, we bobtain a stochastic expansion that holds uniformly over an increasing sequence of subsets of the support IR where the density fR is sufficiently large. Note that when the support is unbounded the density can not be strictly positive everywhere.

20

Theorem 7. Suppose Assumptions 1(i),(iii)-(vi),2,3 and 6 hold. Then for CS > 0 large enough it holds that

sup n(x)-1
xIR ,n

mLL(x) - mLL(x) + m0(x)^ (x)

= OP (n-)

(4.3)

where  is defined as in Theorem 1 and n(x) = (infuSh(x) fR(u))1/2(supuSh(x) fR(u))-1, where

Sh(x) is the support of Kh(x - ·) and where

1
^ (x) = n

n i=1

Kh(r0(Si) - x)(r(Si) -

1 n

n i=1

Kh(r0(Si

)

-

x)

r0(Si))

+

OP

(n-).

(4.4)

The supremum in (4.3) runs over the set IR ,n = {x  IR : infuSh(x) fR(u) > CS(nh)-1 log n}

for a constant CS that is large enough.

4.3 Avoiding Entropy Conditions via Crossvalidation

In this subsection, we consider a slightly modified version of our estimator of m0, obtained

through L-fold crossvalidation. We show that using such an estimator can improve the result

of Theorem 1 in two directions. First, an analogous result can be established without imposing

an entropy condition such as Assumption 3, and second, one can obtain a faster rate for the

remainder term. The improvements are asymptotic. For finite samples, cross validation may be

affected by using smaller subsamples in the estimation steps. This may cause instabilities that

are not reflected in a first order asymptotic analysis.

Our following theoretical treatment contains crossvalidation as a leading example, but the

framework is slightly more general. Nevertheless, we call the resulting estimator crossvalidation

estimator and denote it by mCLLV . The estimator works as follows. Let Nl, l = 1, . . . , L be

a partition of N = {1, ..., n}, and denote the number of elements in the l-th set by #Nl.

Assume that for every l  {1, . . . , L} there exists an estimator r[l] of r0 that is independent of

(Yi, Si) : i  Nl. In the two-stage regression model discussed in Section 3.3, a possible approach

would be to compute r[l] in the same way as r before, but only using the data points (Yi, Si, Ti) with i  Nl. For each l  {1, . . . , L} we then define the estimators m[Ll]L where mL[l]L(x) = [l],

and

([l], [l]) = argmin (Yi -  - T (r[l](Si) - x))2Kh(r[l](Si) - x).
, iNl

Finally, we define the crossvalidation estimator mCLLV of the function m0 as a weighted average

of the mL[l]L, with weights given by the proportion of data points used in the second stage. That

is, we put mLCLV (x) =

L l=1

lm[Ll]L

(x)

with

l

=

#Nl/n.

For

this

estimator,

a

result

similar

to

Theorem 1 can be established under the following assumption.

21

Assumption 7. We impose the following restriction about the accuracy of the first stage estimators and the number of partitions L.

(i) For 1  l  L there exist estimators rl of the functions rl, that are independent of {(Si, Yi) : i  Nl}. The components rj[l] and r0,j of r[l] and r0, respectively, satisfy

sup
s

max
1lL

|rj[l](s)

-

r0,j

(s)|

=

OP

(n-j

)

for some j > j and all j = 1, . . . , p.

(ii) It holds that cn  #Nl  Cn, for some constants 0 < c < C and 0 <   1.

This first part of this assumption is a slight modification of Assumption 2, requiring a certain uniform rate of consistency for the first-stage estimators calculated from the different subsamples. Again, such results are straightforward to verify for many common nonparametric estimation procedures. The second part imposes a restriction on the size of the crossvalidation sets.

Theorem 8. Suppose that Assumptions 1 and 7 hold. Then

sup mCLLV (x) - mLL(x) + m (x)^ CV (x) = OP (n-CV ).
xIR
Here CV = min{CV,1, ..., CV,3} with
1 CV,1 < 2 ( - +) + ( - )min, CV,2 < 2min + ( - )min, CV,3 < min + ( - )min.

Furthermore, with ^ [Cl]V (x)L[l]L(x) = [l], where

L
^ CV (x) = l^ [Cl]V (x)
l=1

([l], [l]) = argmin ((r[l](Si) - r0(Si)) -  - T (r[l](Si) - x))2Kh(r[l](Si) - x).
, iNl
For x  IR-,n we have that

^ CV (x) =

L

n-1 l

l=1

iNl Kh(r0(Si) - x)(r[l](Si) - n-1 iNl Kh(r0(Si) - x)

r0(Si))

+

OP

(n-CV

).

22

The result in Theorem 8 provides an improvement over Theorem 1 because it holds without imposing a restriction on the complexity of the function r0, such as the entropy condition in Assumption 3. Of course, some kind of smoothness restrictions are still usually needed to verify Assumption 7 for a specific estimator. A further refinement compared to Theorem 1 is that the stochastic expansion is typically more precise, in the sense that the rate at which the remainder term converges to zero is weakly faster, i.e. we have that CV   because CV,1 > 1.

4.3.1 Crossvalidation for Estimating Averages of the Regression Function

We now discuss a cross validation approach for the estimation of a weighted average  =

m0(x)w(x)dx of the regression function m0. The advantage of this method is that it re-

quires somewhat weaker regularity conditions than direct approaches based on Theorem 1. The

framework is as above. Again we divide the sample into L subsets N1, ..., NL  {1, ..., n},

L l=1

Nl

= {1, ..., n}

but

now

we

assume

that

L

is

fixed.

We

rewrite



as

=

m(x)w(x)dx

with m(x) = m(x)fR(x) and w(x) = w(x)/fR(x). Now, we assume that there exist estimators

wl and rl of the functions w and rl, that are independent of {(Si, Yi) : i  Nl} and we consider

the following estimator of :

^ = L nl n
l=1

ml(x)wl(x)dx,

where

ml (x)

=

1 nl

iNl

Kh(rl(Si)

-

x)Yi.

Our next theorem states that this estimator is n1/2-consistent. For the theorem we make the

following assumptions.

Assumption 8. (i) The observations (Si, Yi), i = 1, ..., n are i.i.d. and it holds that Yi = m(r(Si)) + i with E[i|Si] = 0 and E[2i |Si] < C, almost surely, for a constant C < .
(ii) The function m0 is bounded. It holds w(r(s))2fS(s)ds <  and c  nl/n  C for some constants 0 < c < C.

(iii) For 1  l  L there exist estimators wl and rl of the functions w and rl, that are independent of {(Si, Yi) : i  Nl} with the properties:

[wl,h(rl(s)) - w(r(s))]2fS(s)ds = oP (1), wl,h(rl(s))m(r(s))fS(s)ds - w(r(s))m(r(s))fS(s)ds = OP (n-1/2),

(4.5)

where wl,h(u) = Kh(u - x)wl(x)dx.

23

Theorem 9. Suppose that Assumption 8 holds. Then we have that ^ =  + OP (n-1/2).
For a derivation of the asymptotic distribution of ^ -  one would need more information about the construction of the estimators wl and rl. In particular one would need a linear expansion of the left hand side of (4.5).

5 Applications

5.1 Regression on the Propensity Score
As our first application, consider estimation of the Average Treatment Effect (ATE) via regression on the (estimated) propensity score. Recall that the parameter of interest is given by

AT E = E(Y1) - E(Y0) = E(1((X))) - E(0((X))),

(5.1)

where (x) = E(D|X = x) is the propensity score and d() = E(Y |D = d, (X) = ) for d = 0, 1. A natural estimate of the ATE is thus the following sample version of (5.1):

1 ^ =
n

n
(^1(^ (Xi)) - ^0(^ (Xi))),

i=1

where ^ (x) is the q-th order local polynomial estimator of (x), and ^d() is the local linear

estimator of d(), computed using the first-stage estimates of the propensity score. Here the

binary covariate D is accommodated via the usual frequency method, i.e. the estimate ^d is

computed by local linear regression of Yi on ^ (Xi) using the nd =

n i=1

I{Di

=

d}

observations

with D = d only. To the best of our knowledge the asymptotic properties of ^ have not been

derived before in the literature.3

Proposition 1. Assume that Assumption 1 holds with (Y, S, T ) = (Y, (D, X), D), r0(S) = (D, (X)), m0(d, ) = d(), and the obvious modifications to accommodate the binary covariate D, and that Assumption 4 holds with r0(S) = (X). Also suppose that   (1/4, 1/3) and (1/2)(q + 1)-1 <  < (1 - 3)p-1. Under these conditions, we have that

 n(^

-

AT

E

)

d

N

(0,

E((Y

,

D,

X

)2

))

3Heckman, Ichimura, and Todd (1998) consider estimating a closely related parameter, average treatment effect on the treated, by conditioning on the estimated propensity score.

24

where

(Y, D,

X)

=

µ1 (X )

-

µ0 (X )

+

D(Y - µ1(X)) (X )

-

(1

-

D)(Y - µ0(X)) 1 - (X)

-

AT E ,

is the influence function, and µd(x) = E(Y |D = d, X = x) for d = 0, 1.

It turns out that under the conditions of the proposition the asymptotic variance of ^ equals the corresponding semiparametric efficiency as bound obtained by Hahn (1998). The estimator obtained via regression on the estimated propensity score thus has the same limit properties as other popular efficient estimators of the ATE under unconfoundedness, such as e.g. the propensity score reweighting estimator of Hirano, Imbens, and Ridder (2003). Note that in order to prove this result, we use that our assumption on the regression residuals in (2.1) implies that µd(x) = d((x)), which is certainly restrictive in the present context. In a recent paper, Hahn and Ridder (2010) argue that this restriction should not be necessary to obtain the conclusion of Proposition 1, using the approach in Newey (1994b) to compute the asymptotic variance of semiparametric estimators. To implement their result, one would have to derive an stochastic expansion similar to that in Theorem 1 for a more general version of the model (2.1) with E(|r0(S)) = 0. Such an extension is not trivial, and is currently under investigation.
We also remark that the conditions of the proposition imply that both p^ and ^d are uniformly consistent for their respective population counterparts at a rate faster than the well-known minimal convergence rate of n-1/4 given by Newey (1994b) for semiparametric two-stage procedures.

5.2 Nonparametric Simultaneous Equation Models
We now consider nonparametric estimation of the structural function µ1 in the triangular simultaneous equation model (2.3)­(2.4) using the method of marginal integration. In order to keep the notation simple, we restrict our attention to the arguably most relevant case with a single endogenous regressor, but allow for an arbitrary number of exogenous regressors and instruments. Let µ^2(z) be the qth order local polynomial estimator of µ2(z) = E(X1|Z = z), and let m^ (x1, z1, v) be the local linear estimator of m(x1, z1, v) = E(Y |X1 = x1, Z1 = z1, V = v). The latter is computed using the generated covariates V^i = X1i - µ^2(Zi) instead of the true residuals Vi from equation (2.4). For simplicity, we use the same bandwidth for all components of m, i.e we put j   for all j = 1, . . . , (2 + d1). The marginal integration estimator of µ1(x1, z1) is

25

then given by the following sample version of (2.5):

1 µ^1(x1, z1) = n

n

m^ (x1, z1, V^i).

i=1

(5.2)

Using similar arguments as in the proof of Theorem 3, the following proposition establishes the

estimator's asymptotic normality.

Proposition 2. Suppose that Assumptions 1 holds with (Y, S, T ) = (Y, (X1, Z1, Z2), X1) and R = r0(S) = (X1, Z1, X1 - µ2(Z1, Z2)), and that Assumption 4 holds with r0(S) = µ2(Z1, Z2). Furthermore, suppose that   (max{1/(5 + d1), 1/(2p + 3)}, 1/(1 + d1)), and that   (, ¯), where  and ¯ are constants depending on , q and dj = dim(Zj) as follows:
¯ = 1 - 3 and  = 1 - (d1 + 1) , 2p 2(q + 1)

where p = d1 + d2. Under these conditions, we have that

 nh1+d1 (µ^1(x1,

z1)

-

µ1(x1,

z1))

d

N

0, E

2(x1, z1, V ) fXZ|V (x1, z1, V )

K~ (t)2dt

where K~ (t) =

1+d1 i=1

K(ti)

is

a

(1 + d1)-dimensional

product

kernel,

and

2(x1,

z1,

v)

=

Var(Y

-

m(R)|R = (x1, z1, v)).

Under the conditions of the proposition, the asymptotic variance of µ^1(x1, z1) is not influenced by the presence of generated regressors: If m^ was replaced in (5.2) with an oracle estimator m using the actual disturbances Vi instead of the reconstructed ones, the result would not change. The intuition for this result is analogous for the one given after Proposition 1.

5.3 Nonparametric Censored Regression

We now consider estimation of the censored regression model given in (2.6). Let r^(x) be the

qth order local polynomial estimator of the conditional mean r0(x) = E(Y |X = x), and let q^(r)

be the local linear estimator of q0(r) using the generated covariates r^(Xi). Then the estimate

µ0 is given by

1

µ(x) =  +

du,

r^(x) q^(u)

(5.3)

where the constant  is chosen large enough to satisfy  > maxi=1,...,n r^(Xi) with probability

tending to one. Generalizing Linton and Lewbel (2002), we consider the use of higher-order local

polynomials for the first stage estimator, and allow the bandwidth used for the computation

of r^ and q^ to be different. For presenting the asymptotic properties of µ, let s0(x) = E(I{Y > 0}|X = x) be the proportion of uncensored observations conditional on X = x, and assume

26

that this function is continuously differentiable and bounded away from zero on the support of X. We then obtain the following proposition.

Proposition 3. Suppose that Assumptions 1 and 4 hold with (Y, S, T ) = (I{Y > 0}, X, Y ) and R = r0(S) = r0(X). Furthermore, suppose that   (, ¯) where  and ¯ are constants depending on ,q and p as follows:

¯ = 1 - 3

1 - 4

and  = max

,

1

.

p p 2(q + 1) + p

Under these conditions, we have that

 ngp(µ^(x)

-

µ0(x))

d

N

where r2(x) = Var(Y |X = x).

0,

fS

r2(x) (x)s02(x)

L(t)2dt ,

The proposition is analogous to Theorem 5 in Linton and Lewbel (2002). However, using our results substantially simplifies the proof and provides insights on admissible choices of bandwidths. Note that the lower bound  is chosen such that both the bias of r^ and q^ tends to zero at a rate faster than (ngp)-1/2. Due to this undersmoothing the limiting distribution of µ^ - µ is centered at zero. In contrast to the other examples, here the final estimator converges at the same rate as the generated regressors. This is due to the fact that the function r^ is not only used to compute q^, but also determines the limits of integration in (5.3). The direct influence of the generated regressors in the estimation of q is again asymptotically negligible.

6 Conclusions
In this paper, we analyze the properties of nonparametric estimators of a regression function, when some the covariates are not directly observable, but have been estimated by a nonparametric first-stage procedure. We derive a stochastic expansion showing that the presence of generated regressors affects the limit behavior of the estimator only through a smoothed version of the first-stage estimation error. We apply our results to a number of practically relevant econometric applications.

A Mathematical Appendix
Throughout the Appendix, C and c denote generic constants chosen sufficiently large or sufficiently small, respectively, which may have different values at each appearance. Furthermore, define M¯ n = M¯ n,1 × . . . × M¯ n,d.
27

A.1 Proof of Theorem 1

In order to prove the statement of the theorem, we have to introduce some notation. First, it follows from standard calculations that the real estimator mLL can be written as

mLL(x) = m0(x) + mLL,A(x) + mLL,B(x) + mLL,C (x) + mLL,D(x),

where mLL,j(x) = j for j  {A, B, C, D}, and
n
(A, A) = argmin (i -  - T (r(Si) - x))2Kh(r(Si) - x),
, i=1 n
(B, B) = argmin (m0(r0(Si)) - m0(x) - m0(x)T (r0(Si) - x) -  - T (r(Si) - x))2Kh(r(Si) - x),
, i=1 n
(C , C ) = argmin (-m0(x)T (r^(Si) - r0(Si)) -  - T (r(Si) - x))2Kh(r(Si) - x),
, i=1 n
(D, D) = argmin (m0(x)T (r(Si) - x) -  - T (r(Si) - x))2Kh(r(Si) - x).
, i=1
Similarly, the oracle estimator mLL can be represented as

mLL(x) = m0(x) + mLL,A(x) + mLL,B(x) + mLL,D(x),

where mLL,j(x) = j for j  {A, B, D}, and
n
(A, A) = argmin (i -  - T (r0(Si) - x))2Kh(r0(Si) - x),
, i=1 n
(B, B) = argmin (m0(r0(Si)) - m0(x) - m0(x)T (r0(Si) - x)
, i=1
-  - T (r0(Si) - x))2Kh(r0(Si) - x),
n
(D, D) = argmin (m0(x)T (r0(Si) - x) -  - T (r0(Si) - x))2Kh(r0(Si) - x).
, i=1
Finally, we set mLL,C (x) = m0(x)^ (x). Note that by construction

mLL,D(x)  mLL,D(x)  0.

(A.1)

We now argue that

sup |mLL,A(x) - mLL,A(x)| = Op(n-1 )
xIR

(A.2)

For a proof of (A.2) note that mLL,A(x) and mLL,A(x) are given by the first elements of the vectors

M (x)-1n-1

n i=1

Kh(r(Si)

-

x)iwi(x)

or

M (x)-1n-1

n i=1

Kh(r0(Si)

-

x)iwi(x),

respectively,

where

wi(x) and wi(x) are the vectors with elements 1, (r1(Si) - x1)/h1, ..., (rd(Si) - xd)/hd or 1, ..., (r0,d(Si) -

xd)/hd, respectively. Furthermore, we have put M (x) = n-1

n i=1

wi(x)wi

(x)T

Kh(r(Si) - x)

and

M (x) = n-1

n i=1

wi(x)wi

(x)T

Kh(r0(Si)

-

x).

Using

these

representations

of

mLL,A(x)

and

mLL,A(x)

one sees that (A.2) follows from Lemma 2 and 3 below.

28

From Lemmas 3 and 4 we get that
sup |mLL,B(x) - mLL,B(x)| = Op(n-2 ),
xIR
sup |mLL,C (x) - mLL,C (x)| = Op(n-3 ).
xIR
Taken together, the results in (A.1)­(A.4) imply the statement of the theorem.

(A.3) (A.4)

Lemma 2. Suppose that the conditions of Theorem 1 hold. Then

1 sup | nxIR,r1,r2M¯ n

n i=1

Kh(r1(Si)

-

x)i

-

1 n

n i=1

Kh(r2(Si)

-

x)i|

=

Op(n-1 )

sup | 1 nxIR,r1,r2M¯ n

n i=1

Kh(r1(Si)

-

x) r1,j (Si) hj

-

xj

i

-

1 n

n i=1

Kh(r2(Si)

-

x)

r2,j

(Si) hj

-

xj i|

=

Op(n-1 ).

Proof. We only prove the first statement of the lemma. The second claim can be shown using essentially

the same arguments. Without loss of generality, we also assume that

1 > ( - )min.

(A.5)

If 1  ( - )min the statement of the lemma follows from a direct bound. For C1, C2 > 0 large enough (see below) we choose C such that

Pr(max |i| > C log(n))  n-C1 ,
i
|EiI{||  C log(n)}|  n-C2 .

(A.6) (A.7)

With this choice of C we define

i(r1, r2) = (Kh(r1(Si) - x) - Kh(r2(Si) - x))i

with

i = iI{|i|  Ci log(n)} - E(iI{|i|  C log(n)}).

Now for s  0, let M¯ s,n,j be a set of functions chosen such that for each r  M¯ n,j there exists r  M¯ s,n,j such that r - r   2-sn-j . That is, the functions in M¯ s,n,j are the midpoints of a (2-sn-j )-covering of M¯ n,j. By Assumption 3, the set M¯ s,n,j can be chosen such that its cardinality #M¯ s,n,j is at most C exp((2-sn-j )-j nj ). Furthermore, define M¯ s,n = M¯ s,n,1 × . . . × M¯ s,n,d.
For r1, r2  M¯ n we now choose r1s, r2s  M¯ s,n such that r1s,j -r1,j   2-sn-j and r2s,j -r2,j   C2-sn-j , for all j. We then consider the chain

Gn Gn
i(r1, r2) = i(r10, r20) - i(r1s-1, r1s) + i(r2s-1, r2s) - i(r1Gn , r1) + i(r2Gn , r2)

s=1

s=1

where Gn is the smallest integer that satisfies Gn > (1 + cG)(1 - ( - )min) log(n)/ log(2) for a constant

cG > 0. With this choice of Gn, we obtain that for l = 1, 2

T1

=

|

1 n

n

i(rlGn , rl)|  C log(n)2-Gn n-(-)min  Cn-1 .

i=1

(A.8)

29

Now for any a > cG define the constant ca = (

 s=1

2-as)-1.

It

then

follows

that

Pr( sup | 1 r1M¯ n n

n i=1

Gn
i(r1s-1, r1s)|
s=1

>

n-1 )



Gn

Pr( sup

s=1

r1M¯ n

|1 n

n i=1

i(r1s-1, r1s)|

>

ca2-asn-1 )



Gn

#M¯ s-1,n

#M¯ s,n

Pr(

1 n

n

i(r1,s, r1,s) > ca2-asn-1 )

s=1

i=1

+

Gn

#M¯ s-1,n#M¯ s,n

Pr(

1 n

n

i(r~1,s, r~1,s) < ca2-asn-1 )

s=1

i=1

= T2 + T3

where the functions r1,s, r~1,s  M¯ s-1,n and r1,s, r~1,s  M¯ s,n are chosen such that

1 Pr(
n

n i=1

i(r1,s, r1,s)

>

ca2-asn-1 )

=

max
r1s-1 ,r1s

1 Pr(
n

n i=1

i(r1s-1, r1s)

>

ca2-asn-1 ),

1 Pr(
n

n i=1

i(r~1,s, r~1,s)

<

ca2-asn-1 )

=

max
r1s-1 ,r1s

1 Pr(
n

n i=1

i(r1s-1, r1s)

>

ca2-asn-1 ).

We now show that both T2 and T3 tend to zero at an exponential rate:

T2  exp(-cnc), T3  exp(-cnc).

(A.9) (A.10)

We only show (A.9), as the statement (A.10) follows by essentially the same arguments. Using Assump-

tion 3, we obtain by application of the Markov inequality that

Gn
T2  C

exp((2-s

n-j

)-j

nj

)E(exp(n,s

1 n

n

i(r1,s, r1,s) - n,sca2-asn-1 ))

s=1 j

i=1

Gn
 C exp(

2sj nj j +j - n,sca2-asn-1 )

n

E(exp(n,s

1 n

i(r1,

r1)))

s=1 j

i=1

(A.11)

where n,s = c 2(2-a)sn-1+1-++2(-)min with a constant c > 0, small enough. Now the last term on the right hand side of (A.11) can be bounded as follows:

E(exp(n,s

1 n

i(r1,

r1)))



1

+

CE(n2,sn-22i (r1,

r1))

 exp(Cn2,sn-2n+-2(-)min 2-2s),

(A.12)

where we have used that

|n,s

1 n

i(r1,

r1)|



C n,s

1 n

log(n)n+ n-(-)min 2-s

 C log(n)n(-)min-1 2-as+s

 C log(n)n(cG-a)(1-(-)min)

C

30

for n large enough because of (A.5). Inserting (A.12) into (A.11), we obtain, if a and c were chosen

sufficiently small, that

Gn
T2  C exp(

s=1

j

2sj nj j +j - c22(1-a)sn1-21-++2(-)min )

Gn
 C exp(-csnc)

s=1

 exp(-cnc).

Finally, it follows from a simple argument that

T4

=

Pr( sup | 1 r1,r2M¯ n n

n i=1

i(r10, r20)|

>

n-1 )



exp(-cnc)

because the set M¯ 0,n can always be chosen such that it contains only a single element.

(A.13)

From (A.8), (A.9), (A.10) and (A.13), we thus obtain that

sup
xIR

Pr( sup
r1,r2M¯ n

|1 n

n i=1

Kh(r1(Si)

-

x)i

-

1 n

n i=1

Kh(r2(Si)

-

x)i|

>

Cn-1 )



exp(-cnc)

(A.14)

Now for CI > 0 choose a grid IR,n of IR with O(nCI ) points, such that for each x  IR there exists a

grid point x = x(x)  IR,n such that x - x  n-cCI . If CI is chosen large enough, this implies that

sup
xIR

1 sup | rM¯ n n

n i=1

Kh(r(Si)

-

x)i

-

1 n

n i=1

Kh(r(Si)

-

x)i|



n-1

(A.15)

for large enough n, with probability tending to one. Furthermore, it follows from (A.14) that

sup
xIR,n

sup
r1,r2M¯ n

|1 n

n i=1

Kh(r1(Si)

-

x)i

-

1 n

n i=1

Kh(r2(Si)

-

x)i|



n-1 .

(A.16)

The statement of the lemma then follows from (A.6)­(A.7) and (A.15) ­ (A.16), if the constants C1 and

C2 were chosen large enough.

Lemma 3. Suppose that the conditions of Theorem 1 hold. Then

sup
xIR,r1,r2M¯ n

|1 n

n i=1

Kh(r1(Si)

-

x)( r1,j (Si) hj

-

xj

)a( r1,l(Si) hl

-

xl )b

-1 n

n i=1

Kh

(r2(Si)

-

x)(

r2,j

(Si) hj

-

xj

)a(

r2,l

(Si) hl

-

xl )b|

=

Op(n-(-)min )

for j, l = 1, . . . , q j = l and 0  a + b  2, 0  a, b.

Proof. The lemma follows from

sup |Kh(r1(s) - x) - Kh(r2(s) - x)|  Cn-(-)min++
x,s

for r1, r2  M¯ n and the fact that

1 sup xIR,rM¯ n

n
Kh(r(Si) - x)  Cn-1++ sup #{i : |r0,j (Si) - xj |  Cn-j
i=1 xIR

for

j

= 1, ..., d}

= Op(1).

31

Define Ii(x) = I{ (r^(Si) - x)/h 1  1} as an indicator function that equals one if r^(Si) - x lies in the support of the kernel function Kh and zero otherwise, and let BK = diag(1, u2K(u)du, . . . , u2K(u)du) be a (d + 1) × (d + 1) diagonal matrix.

Lemma 4. Suppose that the assumptions of Theorem 1 hold. For a random variable Rn = Op(1) that neither depends on x nor i it holds that

sup |[m0(r0(Si)) - m0(x) - m0(x)T (r0(Si) - x)]Ii(x)|  Rnn-2min ,
xIR ,1in

sup
xIR

1 n

n

Kh(r^(Si)

-

x)wi(x)wi(x)T

-

1 n

n

Kh(r0(Si) - x)wi(x)wi(x)T

i=1 i=1

 Rnn-(-)min ,

sup
xIR

1 n

n

Kh(r0(Si) - x)wi(x)wi(x)T - fR(x)BK

i=1

 Rn(n-min + n-(1-+)/2

log n).

(A.17) (A.18) (A.19)

Proof. Claim (A.17) follows by a simple calculation. Claim (A.18) is a direct consequence of Lemma 3.

And (A.19) follows from standard arguments from kernel smoothing theory. For the stochastic part one

makes use of Lemma 5.

A.2 Proof of Lemma 1

In order to prove Lemma 1, we use the fact that the local polynomial estimator satisfies a certain uniform

stochastic expansion if Assumption 4 holds. In order to present this result, we first have to introduce a

substantial amount of further notation. For simplicity we assume g1 = ... = gp and we write g for this

joint value and for the vector g = (g, ..., g).

Let Ni =

i+q-1 q-1

be the number of distinct q-tuples u with u+ = i. Arrange these q-tuples as a

sequence in a lexicographical order (with the highest priority given to the last position so that (0, . . . , 0, i)

is the first element in the sequence and (i, 0, . . . , 0) the last element). Let i denote this one-to-one

mapping, i.e. i(1) = (0, . . . , 0, i), . . . , i(Ni) = (i, 0 . . . , 0). For each i = 1, . . . , q, define a Ni × 1 vector

µi(x) with its kth element given by xi(k), and write µ(x) = (1, µ1(x)T , . . . , µq(x)T )T , which is a column

vector of length N =

q i=1

Ni.

Let i =

L(u)uidu and define ni(x) =

L(u)uifS(x + gu)du. For

0  j, k  q, let Mj,k and Mn,j,k(x) be two Nj × Nk matrices with their (l, m) elements respectively

given by

[Mj,k]l,m = j (l)+k(m) and [Mnj,k(x)]l,m = n,j (l)+k(m)(x)

Now define the N × N matricies Mq and Mn,q(x) by

 M0,0 M0,1 . . . M0,q

 Mn,0,0(x) Mn,0,1(x) . . . Mn,0,q(x)

 



Mq



=

 





M1,0 ...

M1,1 ...

... ...

M1,q ...



 

,







Mn,q (x)

=

 





Mn,1,0(x) ...

Mn,1,1(x) ...

... ...

Mn,1,q (x)

 

...

  

 



Mq,0 Mq,1 . . . Mq,q

Mn,q,0(x) Mn,q,1(x) . . . Mn,q,q(x)

Finally, denote the first unit q-vector by e1 = (1, 0, . . . , 0). With this notation, it can be shown along classical lines (e.g. Masry, 1996) that the local polynomial estimator r^ admits the following stochastic

32

expansion:

r^(s)

-

r0(s)

=

1 n

n

e1Mn-q1(s)µ((Si - s)/g)Lg(Si - s)i + gq+1Bn(s) + Rn(s),

i=1

where, Bn is a bias term that satisfies

Bn(s)

=

(q

1 +

1)! e1Mq-1Aqr0(q+1)(s)

+

op(1)



b(s)

+

op(1),

with Aq = [M0,q+1, M1,q+1, . . . , Mq,q+1]T , and Rn is a remainder term which satisfies

(A.20) (A.21)

sup |Rn(s)| = Op (log(n)/ngp) .
sIS

The value of the expansion (A.20) is that this remainder term be made to be as small as op(n-1/2) by using an appropriate bandwidth g. When the function r0 is sufficiently smooth, and a local polynomial of appropriate order is used, the corresponding bias term is of smaller order than the remainder, and thus

asymptotically negligible. We remark that Kong, Linton, and Xia (2009) have recently shown the validity

of expansions analogous to the one presented in (A.20) for more general local polynomial M-regressions

and certain time series frameworks.

To prove the lemma, define the stochastic component and the bias term of the expansion (A.20)

as rA(s) = n-1

n i=1

e1Mn-q1(s)µ((Si

-

s)/g)Lg (Si

-

s)i

and

rB(s) = g2kBn(s), respectively.

Now

the

function ^ can be written as

1
^ (x) = n

n i=1

Kh

(r0

(Si

)

-

x)rA

(Si

)

1 n

n i=1

Kh(r0(Si)

-

x)

+

1 n

 ^ A(x) + ^ B(x) + Op

log(n) ngp

,

n i=1

Kh

(r0

(Si

)

-

x)rB

(Si)

1 n

n i=1

Kh(r0(Si)

-

x)

+

Op

log(n) ngp

uniformly over x  IR-,n. We first analyze the term ^ B(x). Through the usual arguments from kernel smoothing theory, one can show for x  IR-,n that

1
^ B(x) = g2k n

1 n

n i=1

Kh(r0(Si)

-

x)b(Si)

n i=1

Kh

(r0

(Si

)

-

x)

+

op (g 2k )

= g2kE(b(S)|r0(S) = x) + op(g2k)

because the function E(b(S)|r0(S) = x) is continuous with respect to x, see Assumption 4(ii). Next, consider the term ^ A(x). Using standard arguments from e.g. Masry (1996), we obtain that

^ A(x)

=

1 n2f^R(x)

n j=1

j

n i=1

Kh(r0(Si)

-

x)e1Mn-q1(Si)µ((Sj

-

Si )/g)Lg (Sj

-

Si)

1n

log(n)

= nfR(x) j=1 (x, Sj )j + Op (n2hgp)1/2

with (x, s) = E fS(S)-1Kh(r0(S) - x)e1Mq-1µ((Sj - S)/g)Lg(Sj - S)|Sj = s for s  IS-,n, where the set IS-,n contains all s  IS with the property that their k-th element sk does not lie in a gk-neighborhood
of the boundary of IS,k for k = 1, ..., p. This holds since Mn,q(s) converges to fS(s)Mq uniformly for s

33

in IS-,n. For s  IS-,n this can be written as (x, s) = Kh(r0(u) - x)Lg(u - s)du the modified kernel L is defined by

L(t) = e1Mq-1µ(t)L(t).

(A.22)

Note that L is the equivalent kernel of the local polynomial regression estimator (see Fan and Gijbels

(1996, Section 3.2.2)). For q = 0, 1 the equivalent kernel is in fact equal to the original one, whereas

L(t) is equal to L(t) times a polynomial in t of order q for q  2, with coefficients such that its moments up to the order q are equal to zero. For s  IS-,n we get that (x, s) = Kh(r0(u) - x)Lg(u, u - s)du with a kernel L(u, t) that has the same moment conditions in t but depends on u. We thus obtain that

^ A(x)

=

1 nfR(x)

n j=1

(x, Sj)j

+

Op(log(n)/(n2hgp)1/2).

(A.23)

We now derive explicit expressions for the leading term in equation (A.23) for the cases a)­c) of the

Lemma. Starting with case a), for which g/h  0, it follows by substitution and Taylor expansion arguments that for v  IS-,n with Kh(v) = h-1K (h-1v) and Kh (v) = h-1K (h-1v)

(x, v) = Kh(r0(s) - x)Lg(s - v)ds

= Kh(r0(v + tg) - x)L(t)dt

=

(Kh

(r0

(v)

-

x)

+

Kh(r0(v)

-

x)

r0

(v

+

tg) h

-

r0(v)

+

Kh

(1

-

x)

1 2

r0(v + tg) - r0(v)

2
)L(t)dt

h

= Kh(r0(v) - x) + Kh(r0(v) - x)

tg (r0(v) h

+

r0

(2

)

t2g2 2h

)L(t)dt

+

1 Kh (1 - x) 2

r0(3)tg

2
L(t)dt,

h

where 1,2 and 3 are intermediate values between r0(v) and r0(v + tg), v and v + tg, and v and v + tg, respectively. This gives an expansion for (x, v) of order (h/g)2. For v  IS-,n one gets an expansion of order h/g. Together with Lemma 5 in Appendix B, we thus obtain that

1 nfR(x)

n
(x, Sj)j
j=1

=

1 nfR(x)

n
Kh(r0(Sj) - x)j
j=1

+

Op

((

g h

)2

log(n) ),
nh

as claimed. To show statement b) of the Lemma, we rewrite the function  for v  IS-,n as follows:

(x, v) =

(Kh(r0(v)

-

x

+

r0(v)tg)

+

K

(

1 g

)r0

(2)

1 2

t2

)L

(t)dt

= Mh(x, v) + g

Kg

(1)r0

(2)

1 2

t2

L(t)dt

where 1 is an intermediate value between r0(v + gt) and r0(v) + r0(v)tg, and 2 is an intermediate value between v and v + gt. As in the proof of part a), it follows from Lemma 5 in Appendix B that

1n

1n

log(n)

nfR(x) j=1 (Sj )j = nfR(x) j=1 Mh(x, Sj )j + Op g ng ,

34

which implies the desired result. Now consider statement c) of the Lemma. In this case, where g/h  , we can rewrite the function (·) for v  IS-,n as follows:

(x, v) = Kh(r0(s) - x)Lg(s - v)ds

1 =
g

K(t)L (u, ((v1 + ug, x + th) - v2)/g) x(v1 + ug, x + th)dtdu

The statement of the Lemma then follows from tedious but conceptionally simple Taylor expansion arguments similar to the ones employed for case b), and Lemma 5.

A.3 Proofs of Theorems 2­5.

The statements of these theorems follow by direct application of Lemma 1 and Theorem 1. The statement

of Theorem 2 is immediate. For Theorem 3­5, we only have to check that the error bounds in Theorem 1

and Lemma 1 are of the desired order. We only discuss how the constants ,  and  can be chosen. Note

that all these constants have no subindex because we only consider the case d = 1. We apply Theorem

1 conditionally on the values of S1,...,Sn. Then the only randomness in the pilot estimation comes from

1, ..., n. We can decompose r into rA + rB, where rA is the local polynomial fit to (Si, i) and rB is

the local polynomial fit to (Si, r0(Si)). Conditionally given S1,...,Sn, the value of rB is fixed and for

checking Assumption 3 we only have to consider entropy conditions for sets of possible outcomes of rA. We will show that with  = p/k one can choose for  and  any value that is larger than (1 - p)/2 or -pk-1(1 - p)/2 + p, respectively. Note that then   2 because of Assumption 4(iii). It can be easily

checked that we get the desired expansions in Theorems 2 and 3 with this choices of  = p/k,  and 

(with  and  small enough). In particular note that we can make  +  as close to p as we like.

It is clear that Assumption 2 holds for this choice of . This follows by standard smoothing theory for

local polynomials. Compare also Lemma 5 and the proof of Lemma 1. It remains to check Assumption 3.

It suffices to check the entropy conditions for the tuple of functions (n-1

n i=1

Lh(Si

-

s)[(Si

-

s)/g] i

:

0  +  q, j  0 for j = 1, ..., p). This follows because we get rA by multiplying this tuple of functions

with a (stochastically) bounded vector. We now argue that all derivatives of order k of the functions

n-1

n i=1

Lh(Si

- s)[(Si - s)/g]i

can

be

bounded

by

a

variable

Bn

that

fulfills

Bn



bn

=

n )

with probability tending to one.

Here



is

a number with 

>

-

1 2

(1

-

p)

+

k.

This

bound

holds

uniformly in s and . Furthermore, the functions n-1

n i=1

Lh

(Si

-

s)[(Si

-

s)/g] i

can

be

bounded

by a variable An that fulfills An  an = n ) with probability tending to one. Here  is a number with



>

-

1 2

(1

-

p).

Again,

this

bound

holds

uniformly

in

s

and

.

We

now

consider

the

set

of

functions

on

IS that are absolutely bounded by an and that have all partial derivatives of order k absolutely bounded

by bn. We argue that this set can be covered by C exp(-p/kbnp/k) balls with · -radius  for   an.

Here the constant C does not depend on an and bn. This entropy bound shows that Assumption 3 holds

with these choices of ,  and . For the proof of the entropy bound one applies an entropy bound for

the set of functions on IS that are absolutely bounded by 1 and that have all partial derivatives of order

35

k absolutely bounded by 1. This set can be covered by C exp(-p/k) balls with · -radius  for   1. The desired entropy bound follows by rescaling of the functions. Note that we have that b-n 1an  0.

A.4 Proof of Theorem 6

For x  IR we can decompose Yi = Yi,A+Yi,B(x)+...+Yi,G(x), where Yi,A = i, Yi,B(x) = m0(x)T (r(Si)-

x),

Yi,C (x)

=

m0(x)

+

1 2

(r(Si)

-

x)T

m0 (x)(r(Si)

-

x),

Yi,D (x)

=

m0(r0(Si))

-

m0(x)

-

m0(x)T

(r0(Si)

-

x)

-

1 2

(r0(Si)

-

x)T

m0 (x)(r0(Si)

-

x),

Yi,E (x)

=

-m0(r0(Si))T

(r(Si)

-

r0(Si)),

Yi,F

(x)

=

(m0(r0(Si))

-

m0(x)

-

m0 (x)(r0(Si)

-

x))T (r(Si)

-

r0(Si)),

and

Yi,G(x)

=

-

1 2

(r(Si)

-

r0(Si))T

m0 (x)(r(Si)

-

r0(Si)).

The decomposition of Yi defines an additive decomposition of mLQ(x) into mL Q,A(x) + ... + mLQ,G(x).

Similarly, by decomposing Yi = Yi#,A +Yi#,B(x)+...+Yi#,D(x) we get mL Q(x) = mL Q,A(x)+...+mLQ,D(x).

In the latter decomposition we have chosen Yi#,A = Yi,A, Yi#,B = m0(x)T (r0(Si) - x), Yi#,C (x) = m0(x) +

1 2

(r0(Si)

-

x)T

m0 (x)(r0(Si)

-

x),

and

Yi#,D (x)

=

Yi,D (x).

Now, we compare these two additive decompositions. The difference mL Q,A(x) - mLQ,A(x) can be

treated as in the first part of the proof of Theorem 1 by application of empirical process methods. It is

helpful to multiply the j-th element of mL Q,A(x) and mL Q,A(x) by n-j . Then, for these new vectors the

whole analysis of the first part of Theorem 1 goes through without changing any exponential constants.

It remains to compare the other additive components. First, we have mLQ,B(x) = mL Q,B(x) = m0(x) and mL Q,C (x) = mL Q,C (x) = 0 by definition. Furthermore, one can easily check that Yi#,D(x) = Yi,D(x) is uniformly in x bounded by O(n-3min ). By some algebra this results in a uniform bound for

mj,LQ,D(x) - mj,LQ,D(x) of the order O(n-3min+j-(-)min ). The terms mL Q,F (x) and mLQ,G(x) can

be bounded by using uniform bounds on Yi,F (x) and Yi,G(x). Making use of all these results we get that

(4.1) follows from the fact that (x) = mL Q,E(x). Equation (4.2) follows with a classical smoothing

argument.

A.5 Proof of Theorem 7
The proof is analogous to Theorem 1 over increasing subsets. Direct calculations show that IR ,n is appropriately chosen.

A.6 Proof of Theorem 8
The proof is similar to the one of Theorem 1, but uses more direct arguments to show a result analogous to Lemma 2.

36

A.7 Proof of Theorem 9

We can write  = A + B with

A

=

L l=1

nl n

1 nl

iNl

wl(rl(Si))i,

B

=

L l=1

nl n

1 nl

iNl

wl(rl(Si))m(r(Si)).

We first show that A = OP (n-1/2). This claim immediately follows from
n
A = n-1 w(r(Si))i + oP (n-1/2).
i=1

(A.24)

For a proof of (A.24) we consider the conditional variance of

1 nl( nl

iNl

wl(rl(Si))i

-

1 nl

iNl

w(r(Si))i),

given the functions wl and rl and the values of Si for i  Nl. This conditional variance is bounded by Cn-l 1 iNl [wl,h(rl(Si)) - w(r(Si))]2. Because of Assumption 8(iii) this bound is of order oP (1). This shows (A.24).

It remains to show B -  = OP (n-1/2). This claim can be shown by calculating the conditional

variance

and

expectation

of

1 nl

iNl wl(rl(Si))m(r(Si)), given the functions wl and rl.

A.8 Proof of Proposition 1

Let f^ = (^1, ^0, ^ ) and f¯ = (1, 0, ), define the functional Sn(f ) as

1 Sn(f ) = n

n

f1(f3(Xi)) - f2(f3(Xi)) - AT E,

i=1

and let Sn(f )[h] = limt0(Sn(f + th) - Sn(f ))/t denote its directional derivative. One then obtains

through direct calculations that for any f = (f1,A + f1,B, f2,A + f2,B, f3) we have that

Sn(f ) - Sn(f¯) - Sn(f¯)[f - f¯] - ((f1,A - f¯1) + (f2,A - f¯2))(f3 - f¯3)  = O( f3 - f¯3 2 ( f1,A  + f2,A )) + O( f3 - f¯3 2 ) + O( f1,B  + f2,B ).
Now set f^1,A equal to the leading terms of a stochastic expansion of ^1 up to order op(n-1/2) (analogous to the one given in Theorem 5, but acommodating the presence of the indicator variable D), let f^1,B = f^1 - f^1,A = op(n-1/2) be the corresponding remainder term, and define f^2,A, f^2,B analogously. Since the conditions of the proposition imply that f^3 - f¯3  = op(n-1/4) and f^j,A  = Op(1) for j = 1, 2, we have that

^AT E - AT E = Sn(f^) = Sn(f¯) + T1,n + T2,n + T3,n + T4,n + op(n-1/2),

37

where

1 T1,n = n

n
(^1((Xi)) - 1((Xi))),

i=1

T2,n

=

-1 n

n
(^0((Xi)) - 0((Xi))),

i=1

1 T3,n = n

n
(^ (Xi) - (Xi))(1((Xi)) - 0((Xi))),

i=1

1 T4,n = n

n
((f^1,A((Xi)) - 1((Xi))) + (f^2,A((Xi)) - 0((Xi)))(^ (Xi) - (Xi))

i=1

To prove the asymptotic normality result, we show that

n(Sn(f¯) + T1,n + T2,n + T3,n + T4,n) d N (0, E((Y, D, X)2)).

First, note that the term Sn(f¯) is simply the sample average of i.i.d. mean zero random variables, and thus easy to handle. Now consider the term T1,n. Using the stochastic expansion in Theorem 1, a stochastic expansion for the estimated propensity score ^ (x) analogous to the one used in the proof of

Lemma 1, and projection arguments for (third order) U-Statistics (Ahn and Powell, 1993, Lemma A.3),

it follows that

1n T1,n = n
i=1

Di(Yi

- 1((Xi))) (Xi)

-

1((Xi))(Di

-

(Xi))

+ op(n-1/2).

(A.25)

Using the same line of reasoning, we also find that

1n

T2,n

=

- n

i=1

(1

-

Di)(Yi - 0((Xi)) 1 - (Xi)

-

0((Xi))(Di

-

(Xi))

+ op(n-1/2).

(A.26)

Now consider the term T3,n. Using again standard projection arguments for (now second order) UStatistics (Powell, Stock, and Stoker, 1989, Lemma 3.1) and a stochastic expansion for the estimated propensity score ^ (x) analogous to the one used in the proof of Lemma 1, one can show that

1 T3,n = n

n
(Di - (Xi))(1((Xi)) - 0((Xi))) + op(n-1/2).

i=1

(A.27)

Finally, by using again the stochastic expansions from Theorem 1 and the stochastic expansion of the estimated propensity score ^ (x) mentioned before, one can show that T4,n is equal to a third order U-statistic up to terms of order oP (n-1/2). This leading U-Statistic turns out to be degenerate, and we thus find that

T4,n = oP (n-1/2)

(A.28)

by applying Lemma A.3 in Ahn and Powell (1993). Finally, it follows from (??) that we can write Y = d((X)) + d with E(d|D = d, X) = 0 for d = 0, 1. This implies that µd(x) = E(Y |D = d, X = x) = d((x)) + E(d|D = d, X = x) = d((x)). The statement of the proposition then follows from this identity, equations (A.25)­(A.28), and an application of the central limit theorem.

38

A.9 Proof of Proposition 2
Let f^ = (m^ , µ^2) and f¯ = (m, µ2), define the functional Sn(f ) as 1n
Sn(f ) = n f1(x1, z1, X1i - f2(Zi)) - µ1(x1, z1),
i=1
and let Sn(f )[h] = limt0(Sn(f + th) - Sn(f ))/t denote its directional derivative. One then obtains through direct calculations that for any f = (f1,A + f1,B, f2) with bounded second derivatives we have that

Sn(f ) - Sn(f¯) - Sn(f¯)[f - f¯]   O( f2 - f¯2 2) + O( f2 - f¯2  f1(,vA) - f¯1(v) ) + O( f1,B )

where f1(,vA) (x1, z1, v) = df1,A(x1, z1, v)/dv. Using the same kind of arguments as in the proof of Lemma

1, under the conditions of the proposition one can derive the following uniform stochastic expansion of

m^ up to order op((nh1+d1 )-1/2):

m^ (x1,

z1,

v)

=

m(x1,

z1,

v)

+

1 nfR(x1, z1,

v)

n i=1

Kh((X1i,

Z1i,

Vi)

-

(x1, z1,

v))i

+

op((nh1+d1 )-1/2),

(A.29)

where i = Y - m(X1i, Z1i, Vi). Let f^1,A denote the two leading terms of this expansion, and denote the remainder term by f^1,B. Now it follows from e.g. Masry (1996) and the conditions on  and  that

f^2 - f¯2  = OP ((log(n)/(ngd1+d2 ))1/2) = op((nh1+d1 )-1/4),

and it follows from the same result together with Lemma 5 in Appendix B that

f^2 - f¯2  f^1(,vA) - f¯1(v)  = OP (log(n)/(n2h3+d1 gd1+d2 )1/2) = op((nh1+d1 )-1/2).

For any fixed values (x1, z1) we thus have that µ^1(x1, z1) - µ1(x1, z1) = Sn(f^) = Sn(f¯) + T1,n + T2,n + op((nh1+d1 )-1/2),

where

1

T1,n

=

- n

n

m(v)(x1, z1, Vi)(µ^2(Zi) - µ2(Zi)),

i=1

1n T2,n = n (m^ (x1, z1, Vi) - m(x1, z1, Vi)).
i=1

Being a simple sample average of i.i.d. mean zero random variables, one can directly see that Sn(f0) = Op(n-1/2) = op((nh1+d1 )-1/2). Using a stochastic expansion for µ^2 as in the proof of Lemma 1, and applying projection arguments for U-Statistics, one also finds that T1,n = Op(n-1/2) = op((nh1+d1 )-1/2).

Now consider the term T2,n. From the expansion in (A.29), it follows that for any fixed values (x1, z1)

we have that

T2,n

=

1 n

n j=1

1 nfR(x1, z1, Vj)

n i=1

Kh((X1i, Z1i, Vi) - (x1, z1, Vj ))i

+ op((nh1+d1 )-1/2).

(A.30)

39

This in turn implies that

 nh1+d1 T2,n

d

N

0, E

using projection arguments for U-Statistics.

2(x1, z1, V ) fXZ1|V (x1, z1, V )

K~ (t)2dt

A.10 Proof of Proposition 3

Our proof has the same structure as the one provided by Linton and Lewbel (2002), but making use

of Theorem 1 considerably simplifies some of their arguments. First, note that the restriction that  <  < ¯ implies that (ngp)1/2h2  0 and (ngp)1/2gq+1  0. From a second-order Taylor expansion,

we furthermore obtain that

µ^(x) - µ0(x)

=

1 (r^(x)
q0(r0(x))

- r0(x)) +

 r0 (x)

q^(s) - q0 q0(s)2

(s)

ds

-

q^ (r¯(x)) 2q^(r¯(x))2

(r^(x)

-

r(x))2

-

 r(x)

(q^(s) - q0(s))2 q^(s)q0(s)2

ds

+

(q^(r(x)) - q0(r(x)))2 q^(r(x))q0(r(x))

(r^(x)

-

r0(x))

 T1 + T2 + T3 + T4 + T5

where r^(x) and r(x) are intermediate values between r(x) and r^(x). Now it follows from standard

arguments for local linear estimators that

 ngpT1

d

N

0,

r2(x) fS (x)s02 (x)

L2(t)dt ,

since s0(x) = q0(r0(x)). To prove the proposition, it thus only remains to be shown that the remaining

four terms in the above expansion are of smaller order than T1. Under the conditions of the Proposition, it

is easy to show with straightforward rough arguments that inf q(s) > 0, sup q^ (s) = Op(1) and sup |q^(s) - q0(s)|2 = op((ngp)-1/2) where sup and inf are taken over s  (ro(x) - , 0 + ) for some > 0. This

directly implies that T3 + T4 + T5 = op((ngp)-1/2). Now consider the term T2. From Theorem 1, we

obtain that

T2 =

 r0 (x)

q~(s) - q0 q0(s)2

(s)

ds

-

 r0 (x)

q0(s)w(s) q0(s)2

ds

+

Op(n-),

where q~(x) is the oracle estimator of the function q obtained via local linear regression of I{Y > 0} on

r0(X), and w(s) =

n i=1

Kh(r0(Xi)

-

s)(r^(Xi)

-

r0

(Xi

))/

n i=1

Kh

(r0(Xi

)

-

s).

Using

similar

arguments

as in the proof of Lemma 1 and the other propositions, and the restriction that  <  < ¯, we obtain

that

 r(x)

q~(s) - q(s) q2(s) ds

=

1 n

n i=1

i fR(r0(Xi))

+

Op(h2)

=

Op(h2)

=

op((ngp)-1/2),

for i = I{Yi > 0} - q0(Xi), and that

 r(x)

q0(s)w(s) q0(s)2

ds

=

1 n

n i=1

i

q0

(r0

q0(r0(Xi)) (Xi))2fR(r0(Xi

))fX

(Xi)

+

Op

= op((ngp)-1/2),

log n ngp

+ Op(gq+1)

40

for i = Yi - r0(Xi). Thus T2 = op((ngp)-1/2). Finally, straightforward calculations show that  <  < ¯ also implies that Op(n-) = op((ngp)-1/2). This completes the proof.

B Additional Results

B.1 Uniform Rates for Generalized Kernels
The following lemma states uniform rates for averages of i.i.d. mean zero random variables weighted by "kernel-type" expressions. It is used in the proofs of several of our results. Modifications of the lemma are well known in the smoothing literature, see e.g. (Ha¨rdle, Jansen, and Serfling, 1988). The lemma can be proved by standard smoothing arguments. One can proceed by using a Markov inequality as in the proof of Lemma 2 but without making use of a chaining argument.

Lemma 5. Assume that D  Rdx is a compact set, and Wn,h is a kernel-type function that satisfies

Wn,h(u, z) = 0 for ||u - t(z)|| > bnh for some deterministic sequence 0 < b  |bn|  B < , and t :

RdS  Rdx a continuously differentiable function, for any u  D and z  RdS . Furthermore, assume that

|Wn,h(u,

z ) - Wn,h (v,

z)|



l

||u-t(z)|| h

h-dx

Wn

(v,

t(z))

with

supn

Wn

bounded,

and

that

E[exp

(||)|S]

<

C

a.s. for a constant C > 0 and  > 0 small enough. Then with a deterministic sequence an with |an|  A

we have that

1n

sup
xD

n anWn,h(x, Si)i
i=1

= Op

log(n) nhdx .

(B.1)

References

Ahn, H., and J. Powell (1993): "Semiparametric estimation of censored selection models with a nonparametric selection mechanism," Journal of Econometrics, 58(1-2), 3­29.
Andrews, D. (1994): "Asymptotics for semiparametric econometric models via stochastic equicontinuity," Econometrica, 62(1), 43­72.
(1995): "Nonparametric kernel estimation for semiparametric models," Econometric Theory, 11(03), 560­586.
Blundell, R., and J. Powell (2004): "Endogeneity in semiparametric binary response models," The Review of Economic Studies, 71(3), 655­679.
Carneiro, P., J. Heckman, and E. Vytlacil (2009): "Estimating marginal and average returns to education," Unpublished manuscript, University of Chicago.
Carneiro, P., J. Heckman, and E. Vytlacil (2010): "Evaluating Marginal Policy Changes and the Average Effect of Treatment for Individuals at the Margin," Econometrica, 78(1), 377 ­ 394.

41

Chen, X., O. Linton, and I. Van Keilegom (2003): "Estimation of semiparametric models when the criterion function is not smooth," Econometrica, 71(5), 1591­1608.
Conrad, C., and E. Mammen (2009): "Nonparametric regression on a generated covariate with an application to semiparametric GARCH-in-Mean models," Unpublished manuscript, University of Mannheim.
Das, M., W. K. Newey, and F. Vella (2003): "Nonparametric Estimation of Sample Selection Models," The Review of Economic Studies, 70(1), 33­58.
d'Haultfoeuille, X., and A. Maurel (2009): "Inference on a Generalized Roy Model, with an Application to Schooling Decisions in France," Unpublished manuscript, CREST-INSEE, Paris.
Fan, J., and I. Gijbels (1996): Local polynomial modelling and its applications. CRC Press.
Hahn, J. (1998): "On the role of the propensity score in efficient semiparametric estimation of average treatment effects," Econometrica, 66(2), 315­331.
Hahn, J., and G. Ridder (2010): "The Asymptotic Variance of Semiparametric Estimators with Generated Regressors," Unpublished manuscript.
Heckman, J., H. Ichimura, and P. Todd (1998): "Matching as an econometric evaluation estimator," Review of Economic Studies, 65(2), 261­294.
Heckman, J., and E. Vytlacil (2005): "Structural equations, treatment effects, and econometric policy evaluation," Econometrica, 73(3), 669­738.
Heckman, J. J., and E. J. Vytlacil (2007): "Econometric Evaluation of Social Programs, Part II: Using the Marginal Treatment Effect to Organize Alternative Econometric Estimators to Evaluate Social Programs, and to Forecast their Effects in New Environments," in Handbook of Econometrics, ed. by J. Heckman, and E. Leamer, vol. 6 of Handbook of Econometrics, chap. 71. Elsevier.
Hirano, K., G. Imbens, and G. Ridder (2003): "Efficient estimation of average treatment effects using the estimated propensity score," Econometrica, 71(4), 1161­1189.
Ha¨rdle, W., P. Jansen, and R. Serfling (1988): "Strong Uniform Consistency Rates for Estimators of Conditional Functionals," Annals of Statistics, 16, 1428­1449.
Imbens, G. (2004): "Nonparametric estimation of average treatment effects under exogeneity: A review," Review of Economics and Statistics, 86(1), 4­29.
Imbens, G., and W. Newey (2009): "Identification and Estimation of Triangular Simultaneous Equations Models Without Additivity," Econometrica, 77(5), 1481­1512.
42

Kanaya, S., and D. Kristensen (2009): "Estimation of Stochastic Volatility Models by Nonparametric Filtering," Unpublished manuscript.
Kong, E., O. Linton, and Y. Xia (2009): "Uniform bahadur representation for local polynomial estimates of M-regression and its application to the additive model," Econometric Theory.
Kristensen, D. (2009): "Nonparametric filtering of the realized spot volatility: a kernel-based approach," Econometric Theory, 26(01), 60­93.
Lewbel, A., and O. Linton (2007): "Nonparametric matching and efficient estimators of homothetically separable functions," Econometrica, 75(4), 1209­1227.
Linton, O., and A. Lewbel (2002): "Nonparametric censored and truncated regression," Econometrica, 70(2), 765­779.
Linton, O., and J. Nielsen (1995): "A kernel method of estimating structured nonparametric regression based on marginal integration," Biometrika, 82(1), 93­100.
Mammen, E., O. Linton, and J. Nielsen (1999): "The existence and asymptotic properties of a backfitting algorithm under weak conditions," Annals of Statistics, 27, 1443­1490.
Masry, E. (1996): "Multivariate local polynomial regression for time series: uniform strong consistency and rates," Journal of Time Series Analysis, 17(6), 571­599.
Newey, W. (1994a): "Kernel estimation of partial means and a general variance estimator," Econometric Theory, 10(2), 233­253.
Newey, W. (1994b): "The Asymptotic Variance of Semiparametric Estimators," Econometrica, 62, 1349­1382.
Newey, W. (1997): "Convergence rates and asymptotic normality for series estimators," Journal of Econometrics, 79(1), 147­168.
Newey, W., J. Powell, and F. Vella (1999): "Nonparametric estimation of triangular simultaneous equations models," Econometrica, 67(3), 565­603.
Oxley, L., and M. McAleer (1993): "Econometric issues in macroeconomic models with generated regressors," Journal of Economic Surveys, 7(1), 1­40.
Pagan, A. (1984): "Econometric issues in the analysis of regressions with generated regressors," International Economic Review, 25(1), 221­247.
Powell, J., J. Stock, and T. Stoker (1989): "Semiparametric estimation of index coefficients," Econometrica, 57(6), 1403­1430.
43

Rilstone, P. (1996): "Nonparametric estimation of models with generated regressors," International Economic Review, 37(2), 299­313.
Rosenbaum, P., and D. Rubin (1983): "The central role of the propensity score in observational studies for causal effects," Biometrika, 70(1), 41­55.
Rothe, C. (2009): "Semiparametric estimation of binary response models with endogenous regressors," Journal of Econometrics, 153(1), 51­64.
Song, K. (2008): "Uniform convergence of series estimators over function spaces," Econometric Theory, 24(6), 1463­1499.
Sperlich, S. (2009): "A note on non-parametric estimation with predicted variables," Econometrics Journal, 12(2), 382­395.
Stone, C. (1985): "Additive regression and other nonparametric models," Annals of Statistics, 13(2), 689­705.
Van der Vaart, A., and J. Wellner (1996): Weak convergence and empirical processes: with applications to statistics. Springer Verlag.
Wooldridge, J. (2002): Econometric analysis of cross section and panel data. The MIT press.
44

SFB 649 Discussion Paper Series 2010
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Volatility Investing with Variance Swaps" by Wolfgang Karl Härdle and Elena Silyakova, January 2010.
002 "Partial Linear Quantile Regression and Bootstrap Confidence Bands" by Wolfgang Karl Härdle, Ya'acov Ritov and Song Song, January 2010.
003 "Uniform confidence bands for pricing kernels" by Wolfgang Karl Härdle, Yarema Okhrin and Weining Wang, January 2010.
004 "Bayesian Inference in a Stochastic Volatility Nelson-Siegel Model" by Nikolaus Hautsch and Fuyu Yang, January 2010.
005 "The Impact of Macroeconomic News on Quote Adjustments, Noise, and Informational Volatility" by Nikolaus Hautsch, Dieter Hess and David Veredas, January 2010.
006 "Bayesian Estimation and Model Selection in the Generalised Stochastic Unit Root Model" by Fuyu Yang and Roberto Leon-Gonzalez, January 2010.
007 "Two-sided Certification: The market for Rating Agencies" by Erik R. Fasten and Dirk Hofmann, January 2010.
008 "Characterising Equilibrium Selection in Global Games with Strategic Complementarities" by Christian Basteck, Tijmen R. Daniels and Frank Heinemann, January 2010.
009 "Predicting extreme VaR: Nonparametric quantile regression with refinements from extreme value theory" by Julia Schaumburg, February 2010.
010 "On Securitization, Market Completion and Equilibrium Risk Transfer" by Ulrich Horst, Traian A. Pirvu and Gonçalo Dos Reis, February 2010.
011 "Illiquidity and Derivative Valuation" by Ulrich Horst and Felix Naujokat, February 2010.
012 "Dynamic Systems of Social Interactions" by Ulrich Horst, February 2010.
013 "The dynamics of hourly electricity prices" by Wolfgang Karl Härdle and Stefan Trück, February 2010.
014 "Crisis? What Crisis? Currency vs. Banking in the Financial Crisis of 1931" by Albrecht Ritschl and Samad Sarferaz, February 2010.
015 "Estimation of the characteristics of a Lévy process observed at arbitrary frequency" by Johanna Kappusl and Markus Reiß, February 2010.
016 "Honey, I'll Be Working Late Tonight. The Effect of Individual Work Routines on Leisure Time Synchronization of Couples" by Juliane Scheffel, February 2010.
017 "The Impact of ICT Investments on the Relative Demand for HighMedium-, and Low-Skilled Workers: Industry versus Country Analysis" by Dorothee Schneider, February 2010.
018 "Time varying Hierarchical Archimedean Copulae" by Wolfgang Karl Härdle, Ostap Okhrin and Yarema Okhrin, February 2010.
019 "Monetary Transmission Right from the Start: The (Dis)Connection Between the Money Market and the ECB's Main Refinancing Rates" by Puriya Abbassi and Dieter Nautz, March 2010.
020 "Aggregate Hazard Function in Price-Setting: A Bayesian Analysis Using Macro Data" by Fang Yao, March 2010.
021 "Nonparametric Estimation of Risk-Neutral Densities" by Maria Grith, Wolfgang Karl Härdle and Melanie Schienle, March 2010.

SFB 649 Discussion Paper Series 2010
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
022 "Fitting high-dimensional Copulae to Data" by Ostap Okhrin, April 2010. 023 "The (In)stability of Money Demand in the Euro Area: Lessons from a
Cross-Country Analysis" by Dieter Nautz and Ulrike Rondorf, April 2010. 024 "The optimal industry structure in a vertically related market" by
Raffaele Fiocco, April 2010. 025 "Herding of Institutional Traders" by Stephanie Kremer, April 2010. 026 "Non-Gaussian Component Analysis: New Ideas, New Proofs, New
Applications" by Vladimir Panov, May 2010. 027 "Liquidity and Capital Requirements and the Probability of Bank Failure"
by Philipp Johann König, May 2010. 028 "Social Relationships and Trust" by Christine Binzel and Dietmar Fehr,
May 2010. 029 "Adaptive Interest Rate Modelling" by Mengmeng Guo and Wolfgang Karl
Härdle, May 2010. 030 "Can the New Keynesian Phillips Curve Explain Inflation Gap
Persistence?" by Fang Yao, June 2010. 031 "Modeling Asset Prices" by James E. Gentle and Wolfgang Karl Härdle,
June 2010. 032 "Learning Machines Supporting Bankruptcy Prediction" by Wolfgang Karl
Härdle, Rouslan Moro and Linda Hoffmann, June 2010. 033 "Sensitivity of risk measures with respect to the normal approximation
of total claim distributions" by Volker Krätschmer and Henryk Zähle, June 2010. 034 "Sociodemographic, Economic, and Psychological Drivers of the Demand for Life Insurance: Evidence from the German Retirement Income Act" by Carolin Hecht and Katja Hanewald, July 2010. 035 "Efficiency and Equilibria in Games of Optimal Derivative Design" by Ulrich Horst and Santiago Moreno-Bromberg, July 2010. 036 "Why Do Financial Market Experts Misperceive Future Monetary Policy Decisions?" by Sandra Schmidt and Dieter Nautz, July 2010. 037 "Dynamical systems forced by shot noise as a new paradigm in the interest rate modeling" by Alexander L. Baranovski, July 2010. 038 "Pre-Averaging Based Estimation of Quadratic Variation in the Presence of Noise and Jumps: Theory, Implementation, and Empirical Evidence" by Nikolaus Hautsch and Mark Podolskij, July 2010. 039 "High Dimensional Nonstationary Time Series Modelling with Generalized Dynamic Semiparametric Factor Model" by Song Song, Wolfgang K. Härdle, and Ya'acov Ritov, July 2010. 040 "Stochastic Mortality, Subjective Survival Expectations, and Individual Saving Behavior" by Thomas Post and Katja Hanewald, July 2010. 041 "Prognose mit nichtparametrischen Verfahren" by Wolfgang Karl Härdle, Rainer Schulz, and Weining Wang, August 2010. 042 "Payroll Taxes, Social Insurance and Business Cycles" by Michael C. Burda and Mark Weder, August 2010. 043 "Meteorological forecasts and the pricing of weather derivatives" by Matthias Ritter, Oliver Mußhoff, and Martin Odening, September 2010. 044 "The High Sensitivity of Employment to Agency Costs: The Relevance of Wage Rigidity" by Atanas Hristov, September 2010. 045 "Parametric estimation of risk neutral density functions" by Maria Grith and Volker Krätschmer, September 2010.

SFB 649 Discussion Paper Series 2010
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
046 "Mandatory IFRS adoption and accounting comparability" by Stefano Cascino and Joachim Gassen, October 2010.
047 "FX Smile in the Heston Model" by Agnieszka Janek, Tino Kluge, Rafal Weron, and Uwe Wystup, October 2010.
048 "Building Loss Models" by Krzysztof Burnecki, Joanna Janczura, and Rafal Weron, October 2010.
049 "Models for Heavy-tailed Asset Returns" by Szymon Borak, Adam Misiorek, and Rafal Weron, October 2010.
050 "Estimation of the signal subspace without estimation of the inverse covariance matrix" by Vladimir Panov, October 2010.
051 "Executive Compensation Regulation and the Dynamics of the PayPerformance Sensitivity" by Ralf Sabiwalsky, October 2010.
052 "Central limit theorems for law-invariant coherent risk measures" by Denis Belomestny and Volker Krätschmer, October 2010.
053 "Systemic Weather Risk and Crop Insurance: The Case of China" by Wei Xu, Ostap Okhrin, Martin Odening, and Ji Cao, October 2010.
054 "Spatial Dependencies in German Matching Functions" by Franziska Schulze, November 2010.
055 "Capturing the Zero: A New Class of Zero-Augmented Distributions and Multiplicative Error Processes" by Nikolaus Hautsch, Peter Malec and Melanie Schienle, November 2010.
056 "Context Effects as Customer Reaction on Delisting of Brands" by Nicole Wiebach and Lutz Hildebrandt, November 2010.
057 "Consumption Growth and Volatility with Consumption Externalities" by Runli Xie, November 2010.
058 "Inflation, Price Dispersion and Market Integration through the Lens of a Monetary Search Model" by Sascha S. Becker and Dieter Nautz, November 2010.
059 "Nonparametric Regression with Nonparametrically Generated Covariates" by Enno Mammen, Christoph Rothe and Melanie Schienle, December 2010.

