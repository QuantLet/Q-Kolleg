BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2012-032
Copula Dynamics in CDOs
Barbara Choro-Tomczyk* Wolfgang Karl Härdle* Ludger Overbeck**
* Humboldt-Universität zu Berlin, Germany ** Justus-Liebig-Universität Gießen, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Copula Dynamics in CDOs. 
Barbara Choro´s-Tomczyk, Wolfgang Karl H¨ardle, Ludger Overbeck§
January 9, 2012
Abstract
Values of tranche spreads of collateralized debt obligations (CDOs) are driven by the joint default performance of the assets in the collateral pool. The dependence between the names in the portfolio mainly depends on current economic conditions. Therefore, a correlation implied from tranches can be seen as a measure of the general health of the credit market. We analyse the European market of standardized CDOs using tranches of iTraxx index in the periods before and during the global financial crisis. We investigate the evolution of the correlations using different copula models: the standard Gaussian, the NIG, the double-t, and the Gumbel copula model. After calibration of these models one obtains a time varying vector of parameters. We analyse the dynamic pattern of these coefficients. That enables us to forecast future parameters and consequently calculate Value-at-Risk measures for iTraxx Europe tranches.
Keywords: CDO, multivariate distributions, copula, implied correlations, Valueat-Risk.
JEL classification: C13, C22, C53, G32
1 Introduction
Financial institutions have been facing difficulties over the years for a wide variety of reasons, however, the last financial crisis has shown that one of the major source of problems was the credit risk management. The credit derivatives market was the most innovative and fastest growing derivative market during the past ten years. The rapid development
The financial support from the Deutsche Forschungsgemeinschaft Project HA2229/7-1 and via SFB 649 "O¨ konomisches Risiko", Humboldt-Universit¨at zu Berlin is gratefully acknowledged.
Corresponding author. Ladislaus von Bortkiewicz Chair of Statistics, C.A.S.E. - Center for Applied Statistics and Economics, Humboldt-Universit¨at zu Berlin, Unter den Linden 6, 10099 Berlin, Germany. Email: barbara.choros@wiwi.hu-berlin.de.
Ladislaus von Bortkiewicz Chair of Statistics, C.A.S.E. - Center for Applied Statistics and Economics, Humboldt-Universita¨t zu Berlin, Unter den Linden 6, 10099 Berlin, Germany
§Department of Mathematics, Giessen University, Arndtstrasse 2, 35392 Giessen, Germany.
1

was due to new possibilities that were offered by credit derivatives. Credit instruments are flexible financial products that enable the efficient repackaging and transfer of credit risk. Credit derivatives are attractive for yield seeking investors and banks that need to hedge their investments and fulfil the capital requirements. The most popular securities traded on open markets are credit default swaps (CDS), default baskets, and collateralized debt obligations (CDO). In this paper we consider tranches of the iTraxx Europe index because of the availability of market quotes. iTraxx tranches have a structure of a synthetic CDO. They are written on the portfolio of the 125 most liquid CDS on European companies.
The market standard tool for pricing CDO tranches is the one factor Gaussian copula model. The core assumption of this model is that one value of the correlation is sufficient to model the correlation of every pair of assets. The one factor model for the CDO valuation is an analogy to the Black-Scholes option pricing model where the implied correlation plays the role of the implied volatility parameter. The correlations implied from the different tranches of the same CDO are not equal and the observed phenomenon is called a correlation smile.
Modelling the risk of CDOs involves determining the loss distribution of the underlying portfolio. If we have a portfolio of several assets, we have to quantify the default risk of each obligor and also take into account the synergy of these risks. Because of the high dimensionality, the valuation of a CDO is usually achieved by applying a factor model. For a guide to credit risk models and credit derivatives we refer to Bluhm & Overbeck (2006) and Bluhm, Overbeck & Wagner (2010).
There has been a multitude of CDO pricing methods proposed. The most popular models are based on copula functions. The market standard is the Gaussian copula model proposed by Li (2000). Burtshell, Gregory & Laurent (2008) compare selected copula approaches. The alternative valuations methods are the random factor loading model (Andersen & Sidenius 2004), the intensity based models (Duffie & G^arleanu 2001), the multivariate asset value models (Zhou 2001, Overbeck & Schmidt 2005). Another stream of CDO pricing models come from a top-down framework. Representatives of this approach are Scho¨nbucher (2005), Bennani (2005), Sidenius, Piterbarg & Andersen (2008), Filipovic, Overbeck & Schmidt (2011).
Most of the above cited models are fully parametric and static. The focus of this research is on the dynamics of CDO parameters. We compare different copula models and investigate the dynamic evolution of the calibrated base parameters, which we explain in the next section. We study the dynamic pattern in data, check the stability of coefficients over time, and forecast the models' parameters. We calculate Value-at-Risk (VaR) measures for iTraxx Europe tranches with the aim to improve the understanding of the risk associated with trading credit derivatives.
An issue of the VaR estimation has appeared already in CDO studies. O'Kane & Schl¨ogl (2005) calculate VaR for credit portfolios using the Gaussian, Student-t, Clayton, and Gumbel copula models. Fender, Tarashev & Zhu (2008) compare VaR of corporate bonds and CDO tranches of the same ratings. However, both works do not use empirical data and do not carry any investigations over time. In consequence, they do not analyse the
2

dynamics in CDO models, neither conduct any backtesting for VaR which are the main contributions of this research.
The paper is structured as follows. Section 2 introduces the concept of CDOs. Section 3 presents the models. Section 4 shows results. Section 5 concludes.

2 CDOs
2.1 Market for credit risk transfer
The international standards for measuring and recognizing risk are provided in the Basel I and II Accords developed by the Basel Committee on Banking Supervision. The Basel capital requirements rules state that credit institutions must at all times maintain a minimum amount of financial capital, in order to cover the risks to which they are exposed. In relation to credit risk, Basel II permits two approaches. Banks can assess risk using the standardised approach, which involves external credit assessments, or they can use their own internal systems for rating credit risk. The latter possibility encouraged banks to develop more sophisticated risk management techniques. The strong capital requirements motivated banks to transfer risk from their balance sheets directly to investors leading to the development of new risk dispersal instruments like CDS, default baskets, CDOs. We judge the magnitude of losses incurred by the CDO tranches by calculating VaR. VaR is the most widely used risk measure, mostly because of Basel II requirements for financial services. The effectiveness of VaR models is commonly assessed by a backtesting procedure. For an overview of VaR we refer to Jorion (2006).

2.2 CDO Valuation

Assume the existence of a risk neutral pricing measure P, under which all discounted price processes are martingales. All expectations are taken with respect to this measure. Consider a CDO with a maturity T , J tranches and a pool of d entities at the valuation day t0. Every obligor i is represented by a default indicator

i(t) = 1(i  t), i = 1, . . . , d,

(1)

such that the obligor defaults at time t within a period [t0, T ] if the time of default variable i  t. The portfolio loss at time t is defined as

LGD d

L(t) = d

i(t),

t  [t0, T ],

i=1

(2)

where LGD is a common loss given default. Each tranche j = 1, . . . , J is defined by the detachment lj and attachment uj point which are the percentages of the portfolio losses and lj < uj. The loss Lj at time t is expressed as

Lj(t) = Lu(t, uj) - Lu(t, lj),

3

where Lu is specified from (2)

Lu(t, x) = min{L(t), x} for x  [0, 1].

The outstanding notional Fj(t) of the tranche j is written as

Fj(t) = F u(t, uj) - F u(t, lj),

where F u is

F u(t, x) = x - Lu(t, x) for x  [0, 1].

The fair spread of a CDO tranche is defined by the equivalence of the protection (called also default) and premium leg. The protection leg DLj is calculated as the expected value of the discounted stream of payments made upon defaults

T
DLj(t0) = (t0, t) E{Lj(t) - Lj(t - t)}, j = 1, . . . , J,
t=t1

(3)

where  is a discount factor and t is a time between t and the previous payment day.

The premium leg P Lj is expressed as the expectation of the present value of all premium

payments

T

P Lj(t0) = (t0, t)sj(t0)t E{Fj(t)}, j = 2, . . . , J,

(4)

t=t1

where sj denotes the spread of tranche j. For the equity tranche, the premium leg (4) turns into

T
P L1(t0) = (t0)(u1 - l1) + (t0, t) · 500 · t E{F1(t)},
t=t1

with un upfront payment  and a fixed spread of 500 bp. The tranche spread is found by solving P Lj(t0) = DLj(t0) for sj(t0)

sj(t0) =

T t=t1

(t0,
T t=t1

t) E{Lj(t) (t0, t)t

- Lj(t - E{Fj (t)}

t)}

,

for

j

= 2, . . . , J.

By denoting the denominator of (5) by P Lj (t0) we get

(5)

sj (t0 )

=

DLj P Lj

(t0) (t0)

,

for j = 2, . . . , J.

For the equity tranche the upfront payment is

100 T (t0) = u1 - l1 t=t0 ((t, t0) [E{L1(t) - L1(t - t)} - 0.05t E{F1(t)}])

=

100 u1 - l1

{DL1(t0)

-

0.05P

L1(t0)}.

For more details we refer to Chapter 3 in Bluhm & Overbeck (2006).

4

2.3 Types of Dependence Parameters
The standard CDO pricing methods are based on the Gaussian distribution. Therefore, it is common to equate the problem of modelling CDOs with modelling correlations. However, there are numerous approaches where the dependency is represented by a parameter (or parameters) that is not a linear correlation.
Compound correlation and compound parameter In a given copula model, a compound dependency parameter (lj, uj), j = 1, . . . , J, is a parameter that prices the tranche j so that it fits the market value. A present value P Vj of a tranche j is given by
T
P Vj(t0) = (t0, t) sj(t0)tE(lj,uj){Fj(t)} - E(lj,uj){Lj(t) - Lj(t - t)} , j = 2, . . . , J,
t=t1
where the expected value is calculated with respect to the distribution determined by the compound parameter (lj, uj).
In this work we investigate implied correlations and implied dependency parameters. An implied dependency parameter is a parameter calculated out of a market spread by inverting the pricing model. The standard Gaussian model uses only one correlation to specify the loss distribution and price all the tranches. However, the implied correlations are not the same across the tranches. The phenomenon observed is called a correlation smile and has been widely studied in the literature, see Amato & Gyntelberg (2005).
The main disadvantage of the compound parameters is that the mezzanine tranches are not monotonic in correlation. In consequence, there could be two parameters that yield the same market spread. Moreover, there is no guarantee that the implied parameter exists. These shortcomings motivate us to turn to base parameters.
Base correlation and base parameter The base correlations were introduced by McGinty & Ahluwalia (2004) from JP Morgan in the framework of the Homogeneous Large Pool Gaussian copula model, see Section 3. The main idea behind the concept of the base correlation is that every tranche can be decomposed into two tranches that have lower attachment point zero. Being long the mezzanine tranche with the attachment points lj and uj can be viewed as being simultaneously long the equity tranche with upper attachment point uj and short the equity tranche with upper attachment point lj. The base correlations are computed using a bootstrapping technique, i.e. we use the base correlation of the first tranche to calculate the second tranche, and so on. The expected losses of successive tranches are calculated recursively
E{L(3%,6%)} = E(0,6%){L(0,6%)} - E(0,3%){L(0,3%)}, E{L(6%,9%)} = E(0,9%){L(0,9%)} - E(0,6%){L(0,6%)}, . . .
5

A present value P Vj of a tranche j = 2, . . . , J in this approach is given by

T

P Vj(t0) =

(t0, t) sj(t0)t E(0,uj){Fju(t)} - E(0,lj){Fjl(t)} -

t=t1

(6)

E(0,uj){Luj (t, uj) - Lju(t - t, uj)} + E(0,lj){Ljl (t, lj) - Ljl (t - t, lj)} .

Although the base correlations overcome some limitations of the compound correlations, they also have drawbacks. The analysis of the Gaussian base correlations can be found in Willemann (2005). Willemann (2005) lists problems with the use of base correlations. He shows that even if the true default correlation increases, base correlations might decrease. Moreover, the expected losses for mezzanine tranches can be negative.
The concept of the base correlation can be applied to non-Gaussian copula. Then the expectations in the above formulae are taken with respect to a given distribution of the portfolio loss and the dependence parameters calculated using the above approach we call the base parameters.

3 CDO Dynamics
The most popular CDO pricing models are based on a factor approach combined with various copula functions. In this study we apply and compare the following one-factor models: the Gaussian copula model, the Normal Inverse Gaussian (NIG) copula model, the double-t copula model, the Gumbel Archimedean copula model. The copulae determine the dependency structure between entities in the pool. The industry standard methodology assumes that the values of assets are driven by one unobserved factor such that the individual defaults are conditionally independent given the realization of the factor. The factor reflects a state of economy and is common to all assets. We assume, according to the market practice, that the portfolio is homogeneous. For all models we use the factor representation and the large portfolio approximation technique.
3.1 Copula models
Let (1, . . . , d) be a vector of default times with a (risk-neutral) joint cumulative distribution function
F (t1, . . . , td) = P(1  t1, . . . , d  td) for all (t1, . . . , td)  Rd+.
We denote by F1, . . . , Fd the marginal distribution functions. From the Sklar theorem we know that there exists a copula C : [0, 1]d  [0, 1], such that
F (t1, . . . , td) = C{F1(t1), . . . , Fd(td)}.

6

For a survey over the mathematical foundations of copulae we refer to Nelsen (2006). A default time i of the asset i = 1, . . . , d is taken to be the first jump time of a Poisson process with an intensity i(t) and with exponentially distributed jumps

t
Fi(t) = P (i  t) = 1 - exp - i(u)du ,
t0

t  t0

An intensity function i(t) represents the instantaneous default probability of the obligor at time t. We assume that every individual name has a constant intensity function i(t) = i. Then the default probability is calculated as

pi(t) = Fi(t) = 1 - exp{-i(t - t0)}.

(7)

Large portfolio approximation

In the one-factor model, introduced by Vasicek (1987), default times are calculated from a

vector of latent variables (X1, . . . , Xd) and each variable Xi, i = 1, . . . , d is represented

as 

Xi = Y + 1 - Zi,

(8)

where Y is a systematic risk factor, {Zi}id=1 are idiosyncratic risk factors and all are independent.

The analytical tractability of the factor model is reached by assuming that the portfolio is homogeneous i.e. all the assets have the same exposure, share the same pairwise correlation , default probability p, and recovery rate R and all these values are constant for all time horizons. The number of obligors in the reference portfolio is large so that one may apply asymptotic techniques. A default occurs when the value of the variable Xi drops below the default threshold C = FX-1(p), where FX is a distribution function of Xi.

From the representation (8) we get that conditionally on the realization of the systematic

factor Y , the variables Xi, i = 1, . . . , d, are independent. Therefore, the individual

probability that Xi < C given that Y = y is derived from (8) and equals

C - y

p(y) = P(Xi < C|Y = y) = FZ

 1-

,

(9)

where Zi  FZ. As a single default event is a binary variable, the conditional distribution of the loss L of the portfolio of d assets follows a binomial distribution

P

k L=

=

k p(y)k{1 - p(y)}d-k.

dd

(10)

For portfolios of a sufficiently large size d the fraction of defaulted obligors for a given state of economy Y = y is approximately equal the conditional default probability (9). By the law of large numbers the percentage loss given Y tends in probability to p(Y )

P(L  x) = P{p(Y )  x}

= 1 - FY

C -FZ-1(x) 1-

.

(11)

7

The expected tranche loss in (3) and (4) is calculated as an integral with respect to the distribution (11) and with C(t) = FX-1{p(t)} for t0 < t  T .
Gaussian copula model In the Gaussian copula model the variables (8) are decomposed by the factors Y and {Zi}id=1 that are i.i.d. N(0, 1). In this framework the default times are given by i = Fi-1{(Xi)} with  denoting the cdf of a standard normal. The Gaussian copula was introduced in the CDO valuation by Li (1999). The large portfolio approximation for the Gaussian one-factor model, which is often referred to as Homogeneous Large Pool Gaussian Copula model, provides a simple and a fast solution, therefore, it quickly became an industry standard for pricing CDOs. However, the Gaussian copula has fundamental drawbacks. The main problem is that the Gaussian copula is not able to model properly the joint extreme events. The following copula models incorporate an effect of tail dependence.

Normal Inverse Gaussian (NIG) copula model
The default times are modelled from a latent vector (X1, . . . , Xd) as in (8), where now Y , {Zi}id=1 are independent NIG distributed variables

2 3

Y  NIG , , - 2 , 2 ,

 1- 1-

 1

-





2

 1

-





3

Zi  NIG  ,  , -  2 ,  2

(12) (13)

with 0  || <  and  = 2 - 2. For more detailed explanation of the NIG copula

model refer to Kalemanova, Schmid & Werner (2007). In order to simplify notation,

denote NIG

s,

s

,

-s

2 2

,

s

3 2

as NIG(s). Hence, (12) and (13) can be rewritten as

Y

 NIG(1) and Zi  NIG

 1-

.

The NIG distribution is stable under convolution



Xi  NIG

  1 2 1 3  ,  , -  2 ,  2

= NIG(1/),

however, the vector (X1, . . . , Xd) is not multivariate NIG distributed. The default times are computed as i = Fi-1{NIG(1/)(Xi)}.

Double-t model In this model default times are created from

 Xi = 

Y - 2 Y + Y

1-

Z - Z

2

Zi,

i = 1, . . . , d,

(14)

where Y and Zi are t distributed with Y and Z degrees of freedom respectively. The double-t one-factor model was introduced by Hull & White (2004). Since the Student t distribution is not stable under convolution, Xi are not t distributed and the copula is not a Student t copula. The default times are such that

i = Fi-1{FX (Xi)},

where the distribution FX of Xi needs to be approximated numerically.

8

Gumbel Archimedean copula model An Archimedean copula function C : [0, 1]d  [0, 1] is a copula that can be represented in the following form

C(u1, . . . , ud) = {-1(u1) + · · · + -1(ud)}, u1, . . . , ud  [0, 1],

(15)

where   { : [0; )  [0, 1] | (0) = 1, () = 0; (-1)j(j)  0; j = 1, . . . , } is

called a generator of the copula and usually incorporates a parameter . Each generator

is a Laplace transform of a cumulative distribution function FY of a positive random

variable Y , i.e. (t) =

 0

e-tw

dFY

(w).

The random variables of Archimedean copula possess a factor structure that allows us

to derive a large portfolio approximation similar to the one obtained in the classical

approach (11). The factor representation emerges from the sampling algorithm proposed

by Marshall & Olkin (1988). If we generate X1, . . . , Xd i.i.d. uniformly distributed on

[0, 1] and a variable Y that is independent of X1, . . . , Xd and whose Laplace transform

is ,

then the variables Ui

=



-

log Xi Y

,

i

=

1,

...,

d,

have the Archimedean copula

function (15) as the joint distribution function.

In this algorithm the dependence between the variables Ui, i = 1, . . . , d, is generated by the mixing variable Y . Therefore, conditional on the realisation of Y , the random variables Ui are independent.

Let Ui = p¯i(i)  U[0, 1], i = 1, . . . , d,
where p¯i is a survival probability. Recall that the ith obligor survives until t < T if and only if

i  t or Ui  p¯i(t).

(16)

So instead of determining the joint default probability of i one can specify a joint distribution of Ui by a copula
C{p¯1(t), . . . , p¯d(t)} = P{U1  p¯1(t), . . . , Ud  p¯d(t)},
where the margins satisfy P{Ui  p¯i(t)} = p¯i(t). Hence, the default times are calculated as i = p¯i-1(Ui), where Ui have a joint distribution of the Archimedean copula.
If Ui, i = 1, . . . , d, have the same unconditional survival probability p¯ and the number of obligors d is very large, then the limiting loss distribution is
log(1 - x) P (L  x) = FY - -1(p¯) ,
where FY is a distribution of the mixing variable Y . For more details we refer to Scho¨nbucher (2003), Chapter 10.

From (16) we see that a default occurs when Ui is large. Since for the credit portfolios we are mostly interested in modeling the joint defaults, in the applications we will use

9

a Gumbel copula as it exhibits an upper tail dependence. Namely, it assigns a positive

probability to a simultaneous occurrence of positive extreme values. The Gumbel copula

is given by


d

-1

C(u1, . . . , ud; ) = exp -

(- log uj)  ,

j=1

where the generator

(x; ) = exp {-x1/}, 1   < , x  [0, ).

is a Laplace transform of an -stable distribution with  = 1/.

A generalization of the one-parameter Archimedean copulae are hierarchical Archimedean copulae (HAC), see Okhrin, Okhrin & Schmid (2010). HAC are flexible copulas that define the dependency structure in a recursive way using multiple parameters. Choro´s-Tomczyk, H¨ardle & Okhrin (2010) apply HAC to CDO pricing.

3.2 Time dynamics

The copula structures of the above presented models contain only one parameter. This parameter reflects the strength of the dependence between the entities. As the market conditions change over time, the relation between the companies also change. In this study we investigate the evolution of the parameters over time and calculate their forecasts. The parameters' forecasts are used to compute the Value-at-Risk (VaR) measures for spreads. VaR states the maximum expected loss of a particular investment for a defined time horizon and for a given confidence level. It is especially useful if we are interested in assessing the tail risk. Big portfolios of financial assets are characterised by a high risk of joint extreme outcomes. The ability of a pricing model to describe the joint downward and upward movements is crucial for assigning the correct CDO prices.

In this paper we calculate a one-day VaR for the CDO tranches. The forecast of the next day tranche spreads are computed from the forecasted models' parameters.

Let j(t) denote a copula parameter implied from a tranche j = 1, . . . , J at time t using a copula-based CDO model. By calibrating the models to data day by day we construct time series of parameters. The econometric analysis is further conducted on the first difference Xj(t) = j(t) - j(t - 1) or on the first difference of logarithms of the parameters Xj(t) = log j(t) - log j(t - 1). The stationarity is checked using the augmented Dickey-Fuller test and the heteroskedasticity is detected by the Engle test for residual heteroscedasticity. Afterwards, we choose a time series model to describe the dynamic behavior of Xj(t). The models considered are ARMA(R, M )-GARCH(P, Q)

Xj(t) = µj(t) + j(t),
RM
Xj(t) = Cj + j,iXj(t - i) + j,lj(t - l) + j(t),
i=1 l=1

(17)

10

where j(t) = j(t)Zj(t) are innovations, Zj(t) are standardised innovations that follow the standard normal or t distribution, and

PQ
j2(t) = Kj + Gj,ij2(t - i) + Aj,lj2(t - l)
i=1 l=1

(18)

with the following constraints

P i=1

Gj,i

+

Q l=1

Aj,l

<

1,

Kj

>

0,

Gj,i



0,

and

Aj,i



0.

The following investigation is carried out in moving windows. A moving window procedure
is used when only the most recent data are considered to be relevant for estimation. Here,
a static window of h = 250 elements is applied. Then for every time t0 between h and T~, the end of the period considered, we look at {Xj(t)}tt0=-t01-h. In our work the size of the interval is fixed, however, more advance methods estimate the size of the window
adaptively, see Giacomini, Ha¨rdle & Spokoiny (2009).

We restrict the orders in (17) and (18) to be R, M , P , Q  {0, 1, 2}. The selection is done in the first window {Xj(t)}th=1 using AIC and BIC criteria. Then the orders of the time series models are fixed but in each following window we re-estimate the parameters.
The normality of the standardised residuals is also checked in the first window. If the
normality is rejected, we impose a t distribution.

After selecting a model and fitting it in the moving window we forecast the conditional mean {µ^j(t)}tT~=h+1 and the conditional standard deviations {^j(t)}tT~=h+1 of the residuals of the process Xj(t). Using these results we calculate predictions of X^j(t) and then transform them into ^j(t) which are needed for calculating VaR measures for tranche spreads.
For a given level  and a time horizon, the VaR is an -quantile of a profit-loss distribution. A profit-loss process is the first difference of the spread process sj(t) = sj(t) - sj(t - 1) and the probability that sj(t) exceeds the value of VaRs1j-(t) or is smaller than VaRsj (t) is equal :

P {sj(t) > VaRs1j-(t)} = , P {sj(t) < VaRsj (t)} = .
The calculation of VaRsj (t) requires prediction of s^j(t) as the value of sj(t - 1) is known. The spread is a function of correlations and default probabilities and both have to be forecasted in order to get the next day spread. As the predictor of the intensity ^(t) (7) one applies a forecast of the conditional mean of an ARMA model.
The spread of a tranche j = 2, . . . , J depends on two dependence parameters ^j(t) and ^j-1(t). Therefore, we investigate the dependence of Xj(t) and Xj-1(t). Because of the representation (17-18) we determine the join distribution of the innovations. A pair [j(t), j-1(t)] follows a certain distribution Gj. As the marginals are known j  Fj , the join distribution can be modeled with a copula function

CGj

(u1,

u2)

=

Gj{F-j 1(u1),

F -1
j-1

(u2

)}.

The copula chosen in this instance are a bivariate Gaussian, Gumbel and Clayton.

11

Afterwards, we generate N random bivariate vectors from a copula CGj and then we transform them into bivariate vectors ^j(t), ^j-1(t) . The set of theta parameters gives

us a vector of possible next day spreads. Finally, VaRsj (t) is calculated as a sample quantile from N values.

The adequacy of VaR is examined by performing a backtesting on historical spreads. The basic technique is to calculate an exceedances ratio of the number of exceedances to the number of observations

^ju

=

1 T~

1-

T~ - h t=h+1 1{sj(t) > VaRsj (t)},

^jl

=

1 T~ - h

T~


1{sj(t) < VaRsj (t)}.

t=h+1

(19) (20)

If a model works well, the exceedances ratio is close to the confidence level. The test that checks the frequency of exceedances is Kupiec's likelihood ratio test with the statistics

LRj = -2 log{(1 - j)T~-h-njn} + 2 log{(1 - ^j)T~-h-n^jn}  2(1),

(21)

where j is either ju or jl and n is a number of corresponding exceedences.

The model applied to compute VaR is correctly specified if the exceedances happen only
in the effect of unpredictable events. Moreover, the exceedances should not cluster over
time. However, this dependence in time is not taken into account by the Kupiec test.
Therefore, we use a dynamic quantile (DQ) test proposed by Engle & Manganelli (2004). Define a hit Hj as {Hju(t)}tT~=h+1 for the upper VaR or {Hjl(t)}tT~=h+1 for the lower VaR such that

Hju(t)

=

1-
1{sj(t) > VaRsj (t)} - ,

Hjl (t)

=


1{sj(t) < VaRsj (t)} - .

(22)

The test regresses the hits on their lags and the other variables. The statistics is given

by

DQj = HjVj (Vj Vj)-1Vj Hj/{(1 - )(T~ - h - n)},

(23)

where Vj is a vector of explanatory variables. Following Engle & Manganelli (2004) we

include in Vj a constant, Hj, VaRsj , defined for the upper VaR as {VaRs1j-(t)}tt=0-t01-h and

for

the

lower

VaR

as


{VaRsj

(t)}tt=0-t01-h,

and

also

their

four

lagged

values.

The

test's

null

hypothesis states that Hj and Vj are orthogonal. Under the null hypothesis the statistics

DQj  2(qj), where qj = rank(Vj).

4 Empirical Results
The empirical research of this study was performed using iTraxx Euro indices with a maturity of 5 years for a time period between 20 September 2006 and 2 February 2009.
12

This time interval is especially interesting as it covers time before and during the global financial crisis. In the 4th quarter of 2008 the European market of credit derivatives suffered from lack of demand. In the first quarter of 2009 the iTraxx tranches became highly illiquid. Many missing data made the analysis for the year 2009 impossible.
The time series are constructed from the on-the-run indices of series 6, 7, 8 and 9 consecutively plus observations of series 9 till the end of the term considered. The construction of our data set is motivated by the fact that the latest series of the index are the most liquid. However, we observed that at the end of 2008 the tranches on Series 9 of iTraxx, as well as of the American equivalent index CDX, were more liquid than the tranches of the on-the-run Series 10. For that reason Series 10 was not included in this study.
The following series and time periods are concatenated as follows: 1. Series 6: 20060920-20070322
2. Series 7: 20070323-20070919
3. Series 8: 20070920-20080320
4. Series 9: 20080321-20090202
In total we have 619 days. We assume a flat correlation structure, deterministic LGD of 60%, and constant intensity parameters derived from iTraxx indices. The discount curve is calculated from rates of Euribor and Euro Swaps.
11

t t t t

0.8
0.6
0.4 20061226 20070809 20080324 20081105
1 Time

0.8
0.6
0.4
0.2 20061226 20070809 20080324 20081105
1.6 Time

0.8 1.4
0.6
1.2 0.4

0.2 20061226 20070809 20080324 20081105 Time

1 20061226 20070809 20080324 20081105 Time

Figure 1: Square root of the base correlation from the Gaussian (upper panel, left), the

NIG (upper panel, right), the double-t (lower panel, left) model. Gumbel base parameter

(lower panel, right). Tranches: 1 (blue), 2 (black), 3 (red), 4 (pink), 5 (green).

13

Model Gaussian
NIG
Double-t
Gumbel

Tranche 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5

Mean 0.573 0.652 0.704 0.745 0.846 0.635 0.629 0.634 0.645 0.703 0.630 0.636 0.647 0.664 0.733 1.124 1.124 1.129 1.138 1.202

Std. Dev 0.097 0.096 0.088 0.082 0.067 0.158 0.148 0.135 0.127 0.116 0.119 0.135 0.138 0.140 0.146 0.078 0.082 0.085 0.090 0.132

Maximum 0.798 0.845 0.867 0.898 0.959 0.866 0.867 0.864 0.864 0.890 0.857 0.866 0.870 0.873 0.938 1.337 1.350 1.352 1.360 1.561

Minimum 0.400 0.453 0.494 0.529 0.610 0.375 0.354 0.354 0.324 0.371 0.427 0.366 0.338 0.328 0.288 1.017 1.011 1.011 1.007 1.014

Table 1: Summary statistics for the implied square root of the base parameter for the Gaussian, NIG, double-t, and Gumbel model.

Figure 1 shows the implied square root of the base parameter from the Gaussian, the NIG, the double-t, and the Gumbel model. Table 1 presents their summary statistics. We see that the parameters for mezzanine and senior tranches are much lower for the NIG and the double-t model than for the Gaussian copula. In addition, the tranches of the benchmark model are the least volatile.
In general, the NIG distribution has four free parameters. In the model (12) and (13) two parameters are chosen in such a way that the NIG distributions have zero mean and unit variance. In consequence, the copula has two free parameters  and . As it was shown by Kalemanova et al. (2007), setting  to zero does not significantly affect the results. However,  has to be calibrated to data. Since from one market value of a tranche spread we can imply only one parameter, and this is in our case the correlation, other parameters have to be determined through a preliminary investigation. Therefore, we calibrate the NIG model to data of all five tranches simultaneously by minimizing the sum of the relative difference between the calculated and historical spread. Hence, at any time t > 0 two parameters (t) and (t) price all tranches, see Figure 2. {(t)}tT~=0 obtained with this procedure are afterwards used in the individual calibration of the tranches.
In the double-t model (14) one has to choose the number of degrees of freedom for the common and the idiosyncratic factor. We follow Hull & White (2004) and set both to be equal four.

14

1 1.5

t t

0.8 1
0.6
0.5 0.4

0.2 20061226 20070809 20080324 20081105 Time

0 20061226 20070809 20080324 20081105 Time

Figure 2: Square root of the base correlation (left) and  parameter (right) calibrated for all tranches using the NIG model.

All integrals in (6) are computed using the Legendre-Gauss quadrature.

The ARMA-GARCH models (17)-(18) are estimated for the first difference and for the first difference of logarithms of the parameters. The model type is chosen in the first window using AIC and BIC criteria and then estimated for the whole period. The final model selection for parameters is also based on the Kupiec test (21) and the dynamic quantile test (23). The parameters' forecasts that are incorporated in the calculations of the spreads' forecasts are computed from the ARMA-GARCH models that provide the best quantiles predictions. We do not present these intermediate results as we find them of minor interest to the reader.

Table 2 depicts the exceedance ratios and Tables 3-10 present the p-values of the Kupiec test (21) and the dynamic quantile test (23) for the VaR at the level  equal 0.05 and 0.95. Each table shows the results of the CDO valuation approaches: the Gaussian, NIG, double-t, and Gumbel copula model. In Tables 2-10 the columns refer to the bivariate copulae (Gaussian, Gumbel and Clayton) that are applied to model the parameters of the neighbor tranches. A sample from which the VaR is calculated has a size of 1000 elements. Figure 3 illustrates selected results.

The results of the Kupiec and the DQ tests are different in nearly 38% of the cases which confirms that it is important to check the time series dependence of the exceedances. The number of statistically significant results for every CDO pricing model for both tests are shown in Table 11. The results detect that the bivariate Gumbel copula was most often the optimal choice for modelling the dependence between the parameters according to the Kupiec test and the bivariate Gaussian copula according to the DQ test.

Tables 12 and 13 present the highest computed p-values for every tranche and every pricing method. Here each column refers to a different CDO valuation approach but each item is a largest p-value out of three obtained from the models where the bivariate Gaussian, Gumbel and Clayton copulae were used to model the parameters. Using the Kupiec test for the lower VaR the Gumbel copula was the optimal model for the equity tranche and NIG for other tranches from 2 to 5. For the tranche 3 also the Gaussian model can be selected. According to the Kupiec test for the upper VaR the Gumbel model is the best

15

choice for the equity tranche, the double-t for the tranches 2 and 4, the Gaussian model for the tranches 3 and 5. Regarding the DQ test, which is more reliable, both lower and upper VaR is best calculated by the double-t model for the tranches 2 and 4, and by the NIG model for the tranches 3 and 5. The optimal result for the equity tranche for the lower VaR was achieved by the Gaussian model and for the upper VaR by the Gumbel model. As we see from both tests, against all the odds, the simple Gaussian copula is not entirely outperformed by the more sophisticated models. However, the Gaussian and the Gumbel model perform better for the equity tranche, and the NIG and the double-t are rather better for more senior tranches.

Tranche 1 2
Gaussian 3 4 5 1 2
NIG 3 4 5 1 2
Double-t 3 4 5 1 2
Gumbel 3 4 5

Gauss
0.038 0.033 0.016 0.019
0.054 0.068 0.030 0.038
0.030 0.033 0.022 0.008
0.024 0.019 0.011 0.011

VaR(5%) Gumbel Clayton
0.035 0.038 0.014 0.038 0.016 0.008 0.003 0.027 0.005 0.019 0.087 0.011 0.120 0.038 0.038 0.016 0.087 0.008 0.033 0.033 0.016 0.033 0.011 0.024 0.008 0.022 0.000 0.044 0.030 0.014 0.014 0.003 0.005 0.003 0.024 0.003

Gauss
0.065 0.068 0.016 0.060
0.052 0.082 0.043 0.038
0.038 0.060 0.054 0.035
0.027 0.011 0.014 0.005

VaR(95%) Gumbel Clayton
0.054 0.054 0.046 0.052 0.052 0.008 0.003 0.071 0.052 0.030 0.087 0.046 0.120 0.035 0.057 0.016 0.057 0.016 0.057 0.049 0.041 0.065 0.043 0.046 0.030 0.038 0.030 0.046 0.030 0.033 0.008 0.008 0.008 0.005 0.008 0.005

Table 2: Backtesting results. Exceedance ratios for all VaR models.

Tranche
1 2 3 4 5

Gauss
0.273 0.103 0.001 0.002

VaR(5%) Gumbel Clayton
0.174 0.273 0.000 0.273 0.001 0.000 0.000 0.028 0.000

VaR(95%) Gauss Gumbel Clayton
0.706 0.200 0.706 0.735 0.133 0.886 0.886 0.001 0.000 0.000 0.403 0.086 0.886

Table 3: Backtesting results for the Gaussian copula model. Kupiec test's p-values.

16

Tranche
1 2 3 4 5

Gauss
0.735 0.513 0.188 0.244

VaR(5%) Gumbel Clayton
0.755 0.723 0.111 0.425 0.184 0.037 0.009 0.474 0.019

VaR(95%) Gauss Gumbel Clayton
0.011 0.000 0.008 0.005 0.000 0.003 0.000 0.063 0.037 0.009 0.000 0.000 0.000

Table 4: Backtesting results for the Gaussian copula model. DQ test's p-values.

Tranche
1 2 3 4 5

Gauss
0.706 0.133 0.056 0.273

VaR(5%) Gumbel Clayton
0.002 0.003 0.000 0.000 0.273 0.273 0.001 0.003 0.000

VaR(95%) Gauss Gumbel Clayton
0.056 0.886 0.003 0.735 0.011 0.000 0.174 0.557 0.543 0.001 0.273 0.543 0.001

Table 5: Backtesting results for the NIG copula model. Kupiec test's p-values.

Tranche
1 2 3 4 5

Gauss
0.119 0.126 0.014 0.484

VaR(5%) Gumbel Clayton
0.137 0.000 0.068 0.000 0.780 0.165 0.026 0.000 0.037

VaR(95%) Gauss Gumbel Clayton
0.068 0.009 0.000 0.000 0.000 0.000 0.067 0.001 0.000 0.122 0.335 0.052 0.165

Table 6: Backtesting results for the NIG copula model. DQ test's p-values.

Tranche
1 2 3 4 5

Gauss
0.056 0.103 0.005 0.000

VaR(5%) Gumbel Clayton
0.103 0.103 0.001 0.103 0.000 0.013 0.000 0.005 0.000

VaR(95%) Gauss Gumbel Clayton
0.543 0.273 0.924 0.401 0.403 0.200 0.557 0.706 0.735 0.056 0.174 0.273 0.056

Table 7: Backtesting results for the double-t copula model. Kupiec test's p-values.

Tranche
1 2 3 4 5

Gauss
0.574 0.740 0.382 0.036

VaR(5%)

Gumbel Clayton

0.653

0.741 0.182

0.738 0.054

0.221 0.033

0.019

-

VaR(95%) Gauss Gumbel Clayton
0.020 0.223 0.001 0.240 0.000 0.000 0.005 0.001 0.089 0.510 0.269 0.031 0.114

Table 8: Backtesting results for the double-t copula model. DQ test's p-values. 17

Tranche
1 2 3 4 5

Gauss
0.013 0.002 0.000 0.000

VaR(5%) Gumbel Clayton
0.558 0.056 0.000 0.000 0.000 0.000 0.000 0.013 0.000

VaR(95%) Gauss Gumbel Clayton
0.735 0.028 0.056 0.103 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000

Table 9: Backtesting results for the Gumbel copula model. Kupiec test's p-values.

Tranche
1 2 3 4 5

Gauss
0.482 0.137 0.055 0.050

VaR(5%) Gumbel Clayton
0.605 0.596 0.110 0.118 0.009 0.018 0.009 0.251 0.009

VaR(95%) Gauss Gumbel Clayton
0.085 0.001 0.132 0.003 0.009 0.035 0.033 0.026 0.034 0.018 0.018 0.033 0.017

Table 10: Backtesting results for the Gumbel copula model. DQ test's p-values.

Test (Significance Level)
Kupiec (0.01) Kupiec (0.05)
DQ (0.01) DQ (0.05)

Gauss 7 8 1 3

VaR(5%) NIG double-t
77 78 31 64

Gumbel 9 11 3 5

Gauss 3 3 10 12

VaR(95%) NIG double-t
40 50 75 77

Gumbel 9 10 3 11

Table 11: Number of statistically significant results.

Tranche
1 2 3 4 5

Gauss 0.174 0.273 0.273 0.001 0.028

VaR(5%) NIG double-t 0.002 0.103 0.706 0.103 0.273 0.103 0.273 0.013 0.273 0.005

Gumbel 0.557 0.056 0.002 0.000 0.013

Gauss 0.706 0.735 0.886 0.001 0.886

VaR(95%) NIG double-t 0.056 0.543 0.886 0.924 0.174 0.557 0.557 0.735 0.543 0.273

Gumbel 0.735 0.103 0.000 0.000 0.000

Table 12: Backtesting results for all VaR models. Kupiec test's p-values.

Tranche
1 2 3 4 5

Gauss 0.755 0.735 0.513 0.188 0.474

VaR(5%) NIG double-t 0.137 0.653 0.119 0.741 0.780 0.740 0.165 0.382 0.484 0.036

Gumbel 0.605 0.596 0.137 0.055 0.251

Gauss 0.011 0.008 0.003 0.063 0.000

VaR(95%) NIG double-t 0.068 0.020 0.009 0.240 0.067 0.005 0.122 0.510 0.335 0.269

Gumbel 0.085 0.132 0.035 0.034 0.033

Table 13: Backtesting results for all VaR models. DQ test's p-values.

18

15 80

7.5 60

s
t

s
t

s s s s
tttt

0 40

-7.5

20

-15 20071031 20080313 20080725 20081208
200 Time

0 20071031 20080313 20080725 20081208

1700

Time

100 1275

0 850

-100

425

-200 20071031 20080313 20080725 20081208
150 Time

0 20071031 20080313 20080725 20081208

1000

Time

75 750

s
t

0 500

-75 250

-150 20071031 20080313 20080725 20081208
60 Time

0 20071031 20080313 20080725 20081208
200 Time

30 150

s
t

0 100

-30 50

-60 20071031 20080313 20080725 20081208 Time

0 20071031 20080313 20080725 20081208 Time

Figure 3: VaR for the tranche 1 calculated with the Gumbel model (first row), for the

tranche 2 calculated with the double-t model with the inner Gumbel copula (second

row), for the tranche 3 calculated with the NIG model with the inner Gaussian copula

(third row), and for the tranche 5 calculated with the Gaussian model with the inner Gumbel copula (fourth row). Left: spread difference (blue), VaRsj (red), VaR1sj- (pink), exceedances (black). Right: market spreads (blue), spread predictions (green, dashed

black)

19

Tranche 1 2 3 4 5
Total

Gauss 5.474 1524.232 510.229 219.462 33.687 2293.083

NIG 5.485 1510.394 563.453 274.981 39.228 2393.541

double-t 5.179
1474.177 508.706 211.025 33.199 2232.286

Gumbel 5.379
1532.652 646.390 495.953 46.246 2726.620

Table 14: Mean squared error of the spread predictions.

In addition to the VaR, Table 14 shows the mean squared error of the spread predictions. The best next day spread was forecasted by the double-t model for all the tranches.

5 Conclusions
This paper investigates the dynamic changes in the dependence structure in Collateralized Debt Obligations (CDOs). The CDO valuation procedure and four risk models, Gaussian, NIG, double-t, Gumbel, are presented. The empirical study is conducted using iTraxx Europe tranches for the time period between 20 September 2006 and 2 February 2009. We imply base correlations and analyse their evolution in time. By applying time series models we forecast the implied parameters. Afterwards, the predictions are used to compute the spread forecasts. The forecasting ability of the CDO models is exploited by calculating Value-at-Risk measures for spreads and carrying out a backtesting using the Kupiec test and the dynamic quantile test. The performance of the Gaussian and the more advance models are comparable. The empirical results do not confirm that the benchmark approach is entirely inefficient in assessing risk. Therefore, the simple Gaussian model should not be excluded from the analysis of the iTraxx spreads.

References
Amato, J. & Gyntelberg, J. (2005). CDS index tranches and the pricing of credit risk correlations, BIS Quarterly Review (March): 73­87.
Andersen, L. & Sidenius, J. (2004). Extensions of the Gaussian copula: : Random recovery and random factor loadings, Journal of Credit Risk 1(1): 29­70.
Bennani, N. (2005). The forward loss model: A dynamic term structure approach for the pricing of portfolio credit derivatives, Working paper, the Royal Bank of Scotland.
Bluhm, C. & Overbeck, L. (2006). Structured Credit Portfolio Analysis, Baskets and CDOs, CRC Press LLC.
Bluhm, C., Overbeck, L. & Wagner, C. (2010). An Introduction to Credit Risk Modelling, Chapman and Hall/CRC.
20

Burtshell, X., Gregory, J. & Laurent, J.-P. (2008). A comparative analysis of CDO pricing models, in G. Meissner (ed.), The Definitive Guide to CDOs, Risk Books, pp. 389­ 427.
Choro´s-Tomczyk, B., Ha¨rdle, W. & Okhrin, O. (2010). CDO and HAC, Discussion paper, SFB 649, Humboldt Universit¨at zu Berlin, submitted to the Journal of Empirical Finance.
Duffie, D. & Ga^rleanu, N. (2001). Risk and valuation of collateralized debt obligations, Financial Analysts Journal 57(1): 41­59.
Engle, R. & Manganelli, S. (2004). CAViaR: Conditional autoregressive value at risk by regression quantiles, Journal of Business and Economic Statistics 22(4): 367­381.
Fender, I., Tarashev, N. & Zhu, H. (2008). Credit fundamentals, ratings and value-at-risk: CDOs versus corporate exposures, BIS Quarterly Review (March): 87­101.
Filipovic, D., Overbeck, L. & Schmidt, W. (2011). Dynamic CDO term structure modeling, Mathematical Finance 21(1): 53­71.
Giacomini, E., Ha¨rdle, W. K. & Spokoiny, V. (2009). Inhomogeneous dependency modelling with time-varying copulae, Journal of Business and Economic Statistics 27(2): 224­234.
Hull, J. & White, A. (2004). Valuation of a CDO and an nth to default CDS without Monte Carlo simulation, Journal of Derivatives 12(2): 8­23.
Jorion, P. (2006). Value at Risk: The New Benchmark for Managing Financial Risk, 3 edn, McGraw-Hill.
Kalemanova, A., Schmid, B. & Werner, R. (2007). The normal inverse Gaussian distribution for synthetic CDO pricing, Journal of Derivatives 14(3): 80­93.
Li, D. X. (1999). Creditmetrics monitor, Technical document, RiskMetrics.
Li, D. X. (2000). On default correlation: a copula function approach, The Journal of Fixed Income 9(4): 43­54.
Marshall, A. & Olkin, J. (1988). Families of multivariate distributions, Journal of the American Statistical Association 83(403): 834­841.
McGinty, L. & Ahluwalia, R. (2004). A model for base correlation calculation, Technical report, JP Morgan.
Nelsen, R. (2006). An Introduction to Copulas, Springer Verlag, New York.
O'Kane, D. & Schlo¨gl, L. (2005). A note on the large homogeneous portfolio approximation with the Student-t copula, Finance and Stochastics 9(4): 577­584.
Okhrin, O., Okhrin, Y. & Schmid, W. (2010). Determining the structure and estimation of hierarchical Archimedean copulas, Journal of Econometrics . Under revision.
21

Overbeck, L. & Schmidt, W. (2005). Modeling default dependence with threshold models, Journal of Derivatives 12(4): 10­19.
Scho¨nbucher, P. (2003). Credit Derivatives Pricing Models: Model, Pricing and Implementation, John Wiley & Sons.
Sch¨onbucher, P. (2005). Portfolio losses and the term structure of loss transition rates: A new methodology for the pricing of portfolio credit derivatives, Working paper, ETH Zu¨rich.
Sidenius, J., Piterbarg, V. & Andersen, L. (2008). A new framework for dynamic credit portfolio loss modelling, International Journal of Theoretical and Applied Finance 11(2): 163­197.
Vasicek, O. (1987). Probability of loss on loan portfolio, Technical report, KMV Corporation.
Willemann, S. (2005). An evaluation of the base correlation framework for synthetic CDOs, Journal of Credit Risk 1(4): 180­190.
Zhou, C. (2001). An analysis of default correlations and multiple defaults, Review of Financial Studies 14(2): 555­576.
22

SFB 649 Discussion Paper Series 2012
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "HMM in dynamic HAC models" by Wolfgang Karl Härdle, Ostap Okhrin and Weining Wang, January 2012.
002 "Dynamic Activity Analysis Model Based Win-Win Development Forecasting Under the Environmental Regulation in China" by Shiyi Chen and Wolfgang Karl Härdle, January 2012.
003 "A Donsker Theorem for Lévy Measures" by Richard Nickl and Markus Reiß, January 2012.
004 "Computational Statistics (Journal)" by Wolfgang Karl Härdle, Yuichi Mori and Jürgen Symanzik, January 2012.
005 "Implementing quotas in university admissions: An experimental analysis" by Sebastian Braun, Nadja Dwenger, Dorothea Kübler and Alexander Westkamp, January 2012.
006 "Quantile Regression in Risk Calibration" by Shih-Kang Chao, Wolfgang Karl Härdle and Weining Wang, January 2012.
007 "Total Work and Gender: Facts and Possible Explanations" by Michael Burda, Daniel S. Hamermesh and Philippe Weil, February 2012.
008 "Does Basel II Pillar 3 Risk Exposure Data help to Identify Risky Banks?" by Ralf Sabiwalsky, February 2012.
009 "Comparability Effects of Mandatory IFRS Adoption" by Stefano Cascino and Joachim Gassen, February 2012.
010 "Fair Value Reclassifications of Financial Assets during the Financial Crisis" by Jannis Bischof, Ulf Brüggemann and Holger Daske, February 2012.
011 "Intended and unintended consequences of mandatory IFRS adoption: A review of extant evidence and suggestions for future research" by Ulf Brüggemann, Jörg-Markus Hitz and Thorsten Sellhorn, February 2012.
012 "Confidence sets in nonparametric calibration of exponential Lévy models" by Jakob Söhl, February 2012.
013 "The Polarization of Employment in German Local Labor Markets" by Charlotte Senftleben and Hanna Wielandt, February 2012.
014 "On the Dark Side of the Market: Identifying and Analyzing Hidden Order Placements" by Nikolaus Hautsch and Ruihong Huang, February 2012.
015 "Existence and Uniqueness of Perturbation Solutions to DSGE Models" by Hong Lan and Alexander Meyer-Gohde, February 2012.
016 "Nonparametric adaptive estimation of linear functionals for low frequency observed Lévy processes" by Johanna Kappus, February 2012.
017 "Option calibration of exponential Lévy models: Implementation and empirical results" by Jakob Söhl und Mathias Trabs, February 2012.
018 "Managerial Overconfidence and Corporate Risk Management" by Tim R. Adam, Chitru S. Fernando and Evgenia Golubeva, February 2012.
019 "Why Do Firms Engage in Selective Hedging?" by Tim R. Adam, Chitru S. Fernando and Jesus M. Salas, February 2012.
020 "A Slab in the Face: Building Quality and Neighborhood Effects" by Rainer Schulz and Martin Wersing, February 2012.
021 "A Strategy Perspective on the Performance Relevance of the CFO" by Andreas Venus and Andreas Engelen, February 2012.
022 "Assessing the Anchoring of Inflation Expectations" by Till Strohsal and Lars Winkelmann, February 2012.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2012
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
023 "Hidden Liquidity: Determinants and Impact" by Gökhan Cebiroglu and Ulrich Horst, March 2012.
024 "Bye Bye, G.I. - The Impact of the U.S. Military Drawdown on Local German Labor Markets" by Jan Peter aus dem Moore and Alexandra Spitz-Oener, March 2012.
025 "Is socially responsible investing just screening? Evidence from mutual funds" by Markus Hirschberger, Ralph E. Steuer, Sebastian Utz and Maximilian Wimmer, March 2012.
026 "Explaining regional unemployment differences in Germany: a spatial panel data analysis" by Franziska Lottmann, March 2012.
027 "Forecast based Pricing of Weather Derivatives" by Wolfgang Karl Härdle, Brenda López-Cabrera and Matthias Ritter, March 2012.
028 "Does umbrella branding really work? Investigating cross-category brand loyalty" by Nadja Silberhorn and Lutz Hildebrandt, April 2012.
029 "Statistical Modelling of Temperature Risk" by Zografia Anastasiadou, and Brenda López-Cabrera, April 2012.
030 "Support Vector Machines with Evolutionary Feature Selection for Default Prediction" by Wolfgang Karl Härdle, Dedy Dwi Prastyo and Christian Hafner, April 2012.
031 "Local Adaptive Multiplicative Error Models for High-Frequency Forecasts" by Wolfgang Karl Härdle, Nikolaus Hautsch and Andrija Mihoci, April 2012.
032 "Copula Dynamics in CDOs." by Barbara Choro-Tomczyk, Wolfgang Karl Härdle and Ludger Overbeck, Mai 2012.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

