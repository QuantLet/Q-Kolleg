BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2014-067
Bootstrap confidence sets under model
misspecification
Vladimir Spokoiny*, ** Mayya Zhilova**
* Humboldt-Universität zu Berlin, Germany ** Weierstrass-Institute
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Bootstrap confidence sets under model misspecification

Vladimir Spokoiny

Mayya Zhilova

Weierstrass-Institute, Humboldt University Berlin, Moscow Institute of Physics and Technology,
Mohrenstr. 39, 10117 Berlin, Germany, spokoiny@wias-berlin.de

Weierstrass-Institute,
Mohrenstr. 39, 10117 Berlin, Germany, zhilova@wias-berlin.de

November 17, 2014

Abstract
A multiplier bootstrap procedure for construction of likelihood-based confidence sets is considered for finite samples and a possible model misspecification. Theoretical results justify the bootstrap consistency for a small or moderate sample size and allow to control the impact of the parameter dimension p: the bootstrap approximation works if p3/n is small. The main result about bootstrap consistency continues to apply even if the underlying parametric model is misspecified under the so called Small Modeling Bias condition. In the case when the true model deviates significantly from the considered parametric family, the bootstrap procedure is still applicable but it becomes a bit conservative: the size of the constructed confidence sets is increased by the modeling bias. We illustrate the results with numerical examples for misspecified constant and logistic regressions.
JEL classification codes: C13, C15 Keywords: likelihood-based bootstrap confidence set, misspecified model, finite sample size, multiplier bootstrap, weighted bootstrap, Gaussian approximation, Pinsker's inequality
The author is partially supported by Laboratory for Structural Methods of Data Analysis in Predictive Modeling, MIPT, RF government grant, ag. 11.G34.31.0073. Financial support by the German Research Foundation (DFG) through the Research Unit 1735 is gratefully acknowledged
Financial support by the German Research Foundation (DFG) through the Collaborative Research Center 649 "Economic Risk" is gratefully acknowledged
1

2 Bootstrap confidence sets under model misspecification
1 Introduction
Since introducing in 1979 by Efron (1979) the bootstrap procedure became one of the most powerful and common tools in statistical confidence estimation and hypothesis testing. Many versions and extensions of the original bootstrap method have been proposed in the literature; see e.g. Wu (1986); Newton and Raftery (1994); Barbe and Bertail (1995); Horowitz (2001); Chatterjee and Bose (2005); Ma and Kosorok (2005); Chen and Pouzo (2009); Lavergne and Patilea (2013); Chen and Pouzo (2014) among many others. This paper focuses on the multiplier bootstrap procedure which attracted a lot of attention last time due to its nice theoretical properties and numerical performance. We mention the papers Chatterjee and Bose (2005), Arlot et al. (2010) and Chernozhukov et al. (2013) for the most advanced recent results. Chatterjee and Bose (2005) showed some results on asymptotic bootstrap consistency in a very general framework: for estimators obtained by solving estimating equations. Chernozhukov et al. (2013) presented a number of non-asymptotic results on bootstrap validity with applications to special problems like testing many moment restrictions or parameter choice for a LASSO procedure. Arlot et al. (2010) constructed a non-asymptotical confidence bound in s norm ( s  [1, ] ) for the mean of a sample of high dimensional i.i.d. Gaussian vectors (or with a symmetric and bounded distribution), using the generalized weighted bootstrap for resampling of the quantiles.
This paper makes a further step in studying the multiplier bootstrap method in the problem of confidence estimation by a quasi maximum likelihood method. For a rather general parametric model, we consider likelihood-based confidence sets with the radius determined by a multiplier bootstrap. The aim of the study is to check the validity of the bootstrap procedure in situations with a large parameter dimension, a limited sample size, and a possible misspecification of the parametric assumption. The main result of the paper explicitly describes the error term of the bootstrap approximation. This particularly allows to track the impact of the parameter dimension p and of the sample size n in the quality of the bootstrap procedure. As one of the corollaries, we show bootstrap validity under the constraint " p3/n -small". Chatterjee and Bose (2005) stated results under the condition " p/n -small" but their results only apply to low dimensional projections of the MLE vector. In the likelihood based approach, the construction involves the Euclidean norm of the MLE which leads to completely different tools and results. Chernozhukov et al. (2013) allowed a huge parameter dimension with " log(p)/n small" but they essentially work with a family of univariate tests which again differs essentially from the maximum likelihood approach.
Another interesting and important issue is the impact of the model misspecification

spokoiny, v. and zhilova, m.

3

on the accuracy of bootstrap approximation. A surprising corollary of our error bounds is that the bootstrap confidence set can be used even if the underlying parametric model is slightly misspecified under the so called small modeling bias (SmB) condition. If the modeling bias becomes large, the bootstrap confidence sets are still applicable, but they become more and more conservative. (SmB) condition is given in Section 4 and it is consistent with classical bias-variance relation in nonparametric estimation.
Our theoretical study uses the square-root Wilks (sq-Wilks) expansion from Spokoiny (2012a), Spokoiny (2013) which approximates the square root likelihood ratio statistic by the norm of the standardized score vector. Further we extend the sq-Wilks expansion to the bootstrap log-likelihood and adopt the Gaussian approximation theory (GAR) to the special case when the distribution of the Euclidean norm of a non-Gaussian vector is approximated by the distribution of the norm of a Gaussian one with the same first and second moments. The Gaussian comparison technique based on the Pinsker inequality completes the study and allows to bridge the real unknown coverage probability and the conditional bootstrap coverage probability under (SmB) condition. In the case of a large modeling bias we state a one-sided bound: the bootstrap quantiles are uniformly larger than the real ones. This effect is nicely confirmed by our simulation study.
Now consider the problem and the approach in more detail. Let the data sample Y = (Y1, . . . , Yn) consist of independent random observations and belong to the probability space (, F, IP ) . We do not assume that the observations Yi are identically distributed, moreover, no specific parametric structure of IP is being required. In order to explain the idea of the approach we start here with a parametric case, however the assumption (1.1) below is not required for the results. Let IP belong to some known regular parametric family {IP} d=ef {IP µ0,     IRp} . In this case the true parameter    is such that

IP  IP  {IP},

(1.1)

and the initial problem of finding the properties of unknown distribution IP is reduced to the equivalent problem for the finite-dimensional parameter  . The parametric family {IP} induces the log-likelihood process L() of the sample Y :

L() = L(Y , ) d=ef log dIP (Y ) dµ0
and the maximum likelihood estimate (MLE) of  :

 d=ef argmax L().

(1.2)

The asymptotic Wilks phenomenon Wilks (1938) states that for the case of i.i.d. observations with the sample size tending to the infinity the likelihood ratio statistic converges

4 Bootstrap confidence sets under model misspecification

in distribution to p2/2 , where p is the parameter dimension: 2 L() - L() -w 2p, n  .
Define the likelihood-based confidence set as E(z) d=ef  : L() - L()  z2/2 ,

(1.3)

then the Wilks phenomenon implies

IP   E(z, p2 )  , n  ,

where z2 , 2p is the (1-) -quantile for the 2p distribution. This result is very important and useful under the parametric assumption, i.e. when (1.1) holds. In this case the limit distribution of the likelihood ratio is independent of the model parameters or in other words it is pivotal. By this result a sufficiently large sample size allows to construct the confidence sets for  with a given coverage probability. However, a possibly low speed of convergence of the likelihood ratio statistic makes the asymptotic Wilks result hardly applicable to the case of small or moderate samples. Moreover, the asymptotical pivotality breaks down if the parametric assumption (1.1) does not hold (see Huber (1967)), and, therefore, the whole approach may be misleading if the model is considerably misspecified. If the assumption (1.1) does not hold, then the "true" parameter is defined by the projection of the true measure IP on the parametric family {IP} :

 d=ef argmax IEL().

(1.4)

The recent results by Spokoiny (2012a), Spokoiny (2013) provide a non-asymptotic version of square-root Wilks phenomenon for the case of misspecified model. It holds with an exponentially high probability

2 L() - L() -   W

p , n

(1.5)

where  d=ef D0-1L() , D02 d=ef -2 IEL() . The bound is non-asymptotical, the approximation error term W has an explicit form (the precise statement is given in Theorem A.2, Section A.1, and it depends on the parameter dimension p , sample size
n , and the probability of the random set on which the result holds.
Due to this bound, the original problem of finding a quantile of the LR test statistic L() - L() is reduced to a similar question for the approximating quantity  .
The difficulty here is that in general  is non-pivotal, it depends on the unknown distribution IP and the target parameter  . Another result by Spokoiny (2012b) gives

spokoiny, v. and zhilova, m.

5

the following non-asymptotical deviation bound for  2 : for some explicit constant

C

>0

it

holds for

x



 p

IP  2  IE  2 + Cx  2e-x

(the precise statement is given in Theorem A.3. This is a non-asymptotic deviation bound, sharp in leading approximating terms, however, the critical values yielded by it are too conservative for a valuable confidence set.
In the present work we study the multiplier bootstrap (or weighted bootstrap) procedure for estimation of the quantiles of the likelihood ratio statistic. The idea of the procedure is to mimic a distribution of the likelihood ratio statistic by reweighing its summands with random multipliers independent of the data:

L

ba ()

d=ef

n
log
i=1

dIP dµ0

(Yi)

ui.

Here the probability distribution is taken conditionally on the data Y , which is denoted by the sign ab . The random weights u1, . . . , un are i.i.d. with continuos c.d.f., indepen-
dent of Y and it holds for them: IE(ui) = 1 , Var(ui) = 1 , IE exp(ui) <  . Therefore,
the multiplier bootstrap induces the probability space conditional on the data Y . A simple but important observation is that IE ba L ab()  IE L ba () Y = L() , and hence,

ab ab argmax IE L () = argmax L() = .

This means that the target parameter in the bootstrap world is precisely known and it

coincides with the maximum likelihood estimator  conditioned on Y , therefore, the

bootstrap

likelihood

ratio

statistic

L

ba (

ba )

-

L

ab()

d=ef

sup L ba () - L ba ()

is

fully

computable and leads to a simple computational procedure for the approximation of the

distribution of L() - L() .

The goal of the present study is to show in a non-asymptotic way the consistency of the

described multiplier bootstrap procedure and to obtain an explicit bound on the error of

coverage probability. In other words, we are interested in non-asymptotic approximation

of the distribution of

L() - L() 1/2 with the distribution of

L

ba (

ba )

-

L

ba ()

1/2 .

So far there exist very few theoretical non-asymptotic results about bootstrap validity.

Important contributions are given in the works by Chernozhukov et al. (2013) and Arlot

et al. (2010). Finite sample methods for study of the bootstrap validity are essentially dif-

ferent from the asymptotic ones which are mainly based on weak convergence arguments.

6 Bootstrap confidence sets under model misspecification

The main steps of our theoretical study are illustrated by the following scheme:

Y -world:

sq-Wilks theorem

Gauss. approx.

2L() - 2L()   w





w

Gauss. compar.

Bootstrap world:

2L

ba (

ba )

-

2L

ba ()



ab 

w

ab ,

(1.6)

where

ba 

d=ef

D0-1

L ba () - IE

L ba ()

Y

; compare with the definition (1.5) of ba

the vector  in the Y -world. The vectors  and  are zero mean Gaussian and

they mimic the covariance structure of the vectors



and

 ab :

  N(0, Var ) ,

ab 

N 0, Var{ ba Y } .

The upper line of the scheme corresponds to the Y -world, the lower line - to the

bootstrap world. In both lines we apply two steps for approximating the corresponding

likelihood ratio statistics. The first approximating step is the non-asymptotic square-root

Wilks theorem: the bound (1.5) for the Y case and a similar statement for the bootstrap

case, which is obtained in Theorem A.4, Section A.2.

The next step is called Gaussian approximation (GAR) which means that the dis-

tribution of the Euclidean norm  of a centered random vector  is close to the

distribution of the similar norm of a Gaussian vector  with the same covariance maab
trix as  . A similar statement holds for the vector  . Thus, the initial problem of

comparing the distributions of the likelihood ratio statistics is reduced to the comparison ab
of the distributions of the Euclidean norms of two centered normal vectors  and 

(Gaussian comparison). This last step links their distributions and encloses the approx-

imating scheme. The Gaussian comparison step is done by computing the Kullback-

Leibler divergence between two multivariate Gaussian distributions (i.e. by comparison of the covariance matrices of L() and L ba () ) and applying Pinsker's inequality

(Lemma 5.7). At this point we need to introduce the "small modeling bias" condition

(SmB) from Section 4.2. It is formulated in terms of the following nonnegative-definite

p × p symmetric matrices:

H02 d=ef B02 d=ef

n
IE
i=1

 i() i()

,

n
i=1 IE [

i()] IE [

i()]

,

(1.7) (1.8)

so that Var {L()} = H02 - B02 . If the parametric assumption (1.1) is true or if the data Y are i.i.d., then it holds IE [ i()]  0 and B02 = 0 . The (SmB) condition roughly means that the bias term B02 is small relative to H02 . Below we show
that the Kullback-Leibler distance between the distributions of two Gaussian vectors

spokoiny, v. and zhilova, m.

7



and

ab 

is bounded by

p

H0-1B02H0-1

2/2 .

The

(SmB)

condition precisely means

that this quantity is small. We consider two situations: when the condition (SmB)

is fulfilled and when it is not. Theorem 2.1 in Section 2 deals with the first case, it

provides the cumulative error term for the coverage probability of the confidence set

(1.3), taken at the (1 - ) -quantile computed with the multiplier bootstrap procedure.

The proof of this result (see Section A.3) summarizes the steps of scheme (1.6). The

biggest term in the full error is induced by Gaussian approximation and requires the

ratio p3/n to be small. In the case of a "large modelling bias" i.e., when (SmB) does

not hold, the multiplier bootstrap procedure continues to apply. It turns out that the

bootstrap quantile increases with the growing modelling bias, hence, the confidence set

based on it remains valid, however, it may become conservative. This result is given in

Theorem 2.4 of Section 2. The problems of Gaussian approximation and comparison for

the Euclidean norm are considered in Sections 5.2 and 5.4 in general terms independently

of the statistical setting of the paper, and might be interesting by themselves. Section

5.4 presents also an anti-concentration inequality for the Euclidean norm of a Gaussian

vector. This inequality shows how the deviation probability changes with a threshold.

The general results on GAR are summarized in Theorem 5.1 and restated in Proposition

A.9 for the setting of scheme (1.6). These results are also non-asymptotic with explicit

errors and apply under the condition that the ratio p3/n to be small.

In Theorem 2.3 we consider the case of a scalar parameter p = 1 with an improved error term. Furthermore in Section 2.1 we propose a modified version of a quantile function based on a smoothed probability distribution. In this case the obtained error term is also better, than in the general result.

Notations: · denotes Euclidean norm for vectors and spectral norm for matrices; C is a generic constant. The value x > 0 describes our tolerance level: all the results will be valid on a random set of probability ( 1 - Ce-x ) for an explicit constant C . Everywhere we give explicit error bounds and show how they depend on p and n for the case of the i.i.d. observations Y1, . . . , Yn and x  C log n . More details on it are given in Section 4.3.

The paper is organized as follows: the main results are stated in Section 2, their proofs are given in Sections A.3, A.4 and A.5; Section 3 contains numerical results for misspecified constant and logistic regressions. In Section 4 we give all the necessary conditions and provide an information about dependency of the involved terms on n and p . Section 5 collects some useful statements on Gaussian approximation and Gaussian comparison.

8 Bootstrap confidence sets under model misspecification

2 Multiplier bootstrap procedure

Let i() denote the parametric log-density of the i -th observation:

i() d=ef log

dIP dµ0

(Yi)

,

then L() =

n i=1

i().

Consider i.i.d.

scalar random variables

ui

independent of

Y

with continuous c.d.f., IEui = 1 , Var ui = 1 , IE exp(ui) <  for all i = 1, . . . , n .

Multiply the summands of the likelihood function L() with the new random variables:

L

ab ()

d=ef

n
i=1 i()ui,

then it holds IE ba L ba () = L() , where IE ba stands for the conditional expectation given

Y:

IE ab(·) d=ef IE(·|Y ),

IP ba (·) d=ef IP (·|Y ).

Therefore, the quasi MLE for the Y -world is a target parameter for the bootstrap world:

ba ab argmax IE L () = argmax L() = . The corresponding quasi MLE under the conditional measure IP ba is defined



ba

d=ef

argmax

L

ba ().

The

likelihood

ratio

statistic

in

the

bootstrap

world

is

equal

to

L

ab(

ba )

-

L

ba ()

,

where

all

the

elements:

the

function

L ba ()

and

the

arguments

ba ,



are

known and available

for computation.

Let 1 -   (0, 1) be a fixed desirable confidence level of the set E(z) :

IP (  E(z))  1 - .

(2.1)

Here the parameter z  0 determines the size of the confidence set. Usually we are interested in finding a set of the smallest possible diameter satisfying this property. This leads to the problem of fixing the minimal possible value of z such that (2.1) is fulfilled. Let z denote the upper  -quantile of the square-root likelihood ratio statistic:

z d=ef min z  0 : IP L() - L() > z2/2   .

(2.2)

This means, that z is exactly the value of our interest. Estimation of z leads to recovering of the distribution of L() - L() . The multiplier bootstrap procedure consists
of generating a large number of independent samples {u1, . . . , un} and computing from

spokoiny, v. and zhilova, m.

9

them

the

empirical

distribution

function

of

L

ab(

ba )

-

L

ab()

.

By

this

procedure

we

can

estimate zab , the upper  -quantile of

2L

ba (

ba )

-

2L

ab()

:

ba z

d=ef

min

z  0 : IP ab

L

ba (

ab )

-

L

ba ()

>

z2/2

=

.

(2.3)

Theorem 2.1 (Validity of the bootstrap under a small modeling bias). Let the conditions

of Section 4 be fulfilled. 

It holds with probability

 1 - 12e-x

for

zab

 max{2, p} +

C(p + x)/ n :

IP

L()

-

L()

>

ab (z

)2/2

-

 full,

(2.4)

where full  C{(p + x)3/n}1/8 in the case 4.3. An explicit definition of the error term full is given in the proof (see (A.26), (A.27) in Section A.3).

The term full can be viewed as a sum of the error terms corresponding to each step in the scheme (1.6). The largest error term equal to C{(p+x)3/n}1/8 is induced by GAR. This error rate is not always optimal for GAR, e.g. in the case of p = 1 or for the i.i.d. observations (see Remark 5.2). In Theorems 2.3 and 2.5 the rate is C{(p + x)3/n}1/2 .
In view of definition (1.3) of the likelihood-based confidence set Theorem 2.1 implies the following

Corollary 2.2 (Coverage probability error). Under the conditions of Theorem 2.1 it holds:
|IP {  E (zba )} - (1 - )|  full.

Remark 2.1 (Critical dimension). The error term full depends on the ratio p3/n . The bootstrap validity can be only stated if this ratio is small. The obtained error bound seems to be mainly of theoretical interest, because the condition " (p3/n)1/8 is small" may require a huge sample. However, it provides some qualitative information about the bootstrap behavior as the parameter dimension grows. Our numerical results show that the accuracy of bootstrap approximation is very reasonable in a variety of examples.

In the following theorem we consider the case of a scalar parameter p = 1 . The

obtained

error

rate

is

 1/ n ,

which

is

sharper,

than

1/n1/8 .

Instead

of

the

GAR

for

the

Euclidean norm from Section 5 we use here Berry-Esseen theorem (see also Remark 5.2).

Theorem 2.3 (The case of p = 1 , using Berry-Esseen theorem). Let the conditions of

Section

4

be

fulfilled.

It

holds

with probability

 1 - 12e-x

for

zab



1

+

C(1

+

 x)/ n

:

IP

L()

-

L()

>

ab (z

)2/2

-   B.E., full,

(2.5)

 where B.E., full  C(1 + x)/ n in the case 4.3. An explicit definition of B.E., full is

given in (A.28) in Section A.3.

10 Bootstrap confidence sets under model misspecification

Remark 2.2 (Bootstrap validity and weak convergence). The standard way of proving the bootstrap validity is based on weak convergence arguments; see e.g. Mammen (1992), van ver Vaart and Wellner (1996), Janssen and Pauls (2003), Chatterjee and Bose (2005). If the statistic L() - L() weakly converges to a 2 -type distribution, one can state an asymptotic version of the results (2.4), (2.5). Our way is based on a kind of nonasymptotic Gaussian approximation and Gaussian comparison for random vectors and allows to get explicit error terms.
Remark 2.3 (Use of Edgeworth expansion). The classical results on confidence sets for the mean of population states the accuracy of order 1/n based on the second order Edgeworth expansion Hall (1992). Unfortunately, if the considered parametric model can be misspecified, even the leading term is affected by the modeling bias, and the use of Edgeworth expansion cannot help in improving the bootstrap accuracy.
Remark 2.4 (Choice of the weights). In our construction, similarly to Chatterjee and Bose (2005), we apply a general distribution of the bootstrap weights ui under some moment conditions. One particularly can use Gaussian multipliers as suggested by Cher-
ab nozhukov et al. (2013). This leads to the exact Gaussian distribution of the vectors  and is helpful to avoid one step of Gaussian approximation for these vectors.
Now we discuss the impact of modeling bias, which comes from a possible misspecification of the parametric model. As explained by the approximating diagram (1.6), the distance between the distributions of the likelihood ratio statistics can be characterized via the distance between two multivariate normal distributions. To state the result let us recall the definition of the full Fisher information matrix D02 d=ef -2 IEL() . For the matrices H02 and B02 , given in (1.7) and (1.8), it holds H02 > B02  0 . If the parametric assumption (1.1) is true or in the case of an i.i.d. sample Y , B02 = 0 . Under the condition (SmB) H0-1B02H0-1 enters linearly in the error term full in Theorem 2.1.
The first statement in Theorem 2.4 below says that the effective coverage probability of the confidence set based on the multiplier bootstrap is larger than the nominal coverage probability up to the error term b, full  C{(p+x)3/n}1/8 . The inequalities in the second part of Theorem 2.4 prove the conservativeness of the bootstrap quantiles: the quantity
tr{D0-1H02D0-1} - tr{D0-1(H02 - B02)D0-1}  0 increases with the growing modeling bias.

Theorem 2.4 (Performance of the bootstrap for a large modeling bias). Under the

conditions of Section 4 except for (SmB) it holds with probability  1 - 14e-x for

z,

zab



max{2,

p}

+

C(p

+

 x)/ n

1.

IP

L() - L() > z2/2

 IP ba

L

ba (

ab )

-

L

ba ()

>

z2/2

+ b, full.

spokoiny, v. and zhilova, m.

11

2.

ba z



z(+b, full)

+ tr{D0-1H02D0-1} -

ab z



z(-b, full)

tr{D0-1(H02 - B02)D0-1} - qf,1,

+ tr{D0-1H02D0-1} - tr{D0-1(H02 - B02)D0-1} + qf,2.

The term b, full  C{(p + x)3/n}1/8 is given in (A.30) in Section A.4. The positive

values above

witqhf,1(,a2 q+f,2a2Ba)r(eg8ixvpen+i6nx)(Af.o3r4)t,he(Ac.o3n3s)tainntsSeact2i,oan2B

A.4, >0

they are bounded from conditions

from (I) ,

(IB) .

Remark 2.5. There exists some literature on robust (and heteroscedasticity robust) bootstrap procedures; see e.g. Mammen (1993), Aerts and Claeskens (2001), Kline and Santos (2012). However, up to our knowledge there are no robust bootstrap procedures for the likelihood ratio statistic, most of the results compare the distribution of the estimator obtained from estimating equations, or Wald / score test statistics with their bootstrap counterparts in the i.i.d. setup. In our context this would correspond to the noise misspecification in the log-likelihood function and it is addressed automatically by the multiplier bootstrap. Our notion of modeling bias includes the situation when the target value  from (1.4) only defines a projection (the best parametric fit) of the data distribution. In particularly, the quantities IE i() for different i do not necessarily vanish yielding a significant modeling bias. Similar notion of misspecification is used in the literature on Generalized Method of Moments; see e.g. Hall (2005). Chapter 5 therein considers the hypothesis testing problem with two kinds of misspecification: local and non-local, which would correspond to our small and large modeling bias cases.
An interesting message of Theorem 2.4 is that the multiplier bootstrap procedure ensures a prescribed coverage level for this target value  even without small modeling bias restriction, however, in this case the method is somehow conservative because the modeling bias is transferred into the additional variance in the bootstrap world. The numerical experiments in Section 3 agree with this result.

2.1 Smoothed version of a quantile function
This section briefly discusses the use of a smoothed quantile function. The (1 - ) quantile of 2L() - 2L() is defined as
z d=ef min z  0 : IP L() - L() > z2/2   = min z  0 : IE 1I L() - L() > z2/2   .

12 Bootstrap confidence sets under model misspecification

Introduce for x  0 and z,  > 0 the following function

g(x, z) d=ef g

1 x2 - z2 2z

,

(2.6)

where g(·)  C2(IR) is a non-negative function, which grows monotonously from 0 to 1 , g(x) = 0 for x  0 and g(x) = 1 for x  1 , therefore:

1I {x  1}  g(x)  1I {x  0}  g(x + 1).

An example of such function is given in (5.9). In (5.10) it is shown

1I{x - z  }  g(x, z)  1I(x - z  0)  g(x, z + ).

This approximation is used in the proofs of Theorems 2.1 and 2.4 in the part of Gaussian approximation of Euclidean norm of a sum of independent vectors (see Section 5.2) yielding the error rate (p3/n)1/8 in the final bound (Theorems 2.1, 5.1). The next result shows that the use of a smoothed quantile function helps to improve the accuracy of bootstrap approximation: it becomes (p3/n)1/2 instead of (p3/n)1/8 . The reason is that we do not need to account for the error induced by a smooth approximation of the indicator function.

Theorem 2.5 (Validity of the bootstrap in the smoothed case under (SmB) condition).

Let the conditions of Section 4 be fulfilled. It holds with probability  1 - 12e-x for

z



max{2,

p}

+

C(p

+

 x)/ n

and

  (0, 0.22] :

IEg

2L() - 2L(), z

-

IE

ab g

2L

ba (

ba )

-

2L

ab(),

z

 sm,

where sm  C{(p + x)3/n}1/2-3 in the case 4.3. An explicit definition of sm is given in (A.38), (A.39) in Section A.5.

The modified bootstrap quantile function reads as

ba z, 

d=ef

min

z



0:

IE

ab g

2L

ab(

ab )

-

2L

ab(),

z

=

.

3 Numerical results
This section illustrates the performance of the multiplier bootstrap for some artificial examples. We especially aim to address the issues of noise misspecification and of increasing modeling bias. In all the experiments we took 104 data samples for estimation

spokoiny, v. and zhilova, m.

13

of empirical c.d.f. of 2L() - 2L() , 104 {u1, . . . , un} samples and 104 data samples for the estimation of the quantiles of 2L ba ( ba ) - 2L ba () . All sample sizes are n = 50 . It should be mentioned that the obtained results are nicely consistent with the theoretical statements.

3.1 Computational error

Here we check numerically, how well the multiplier procedure works in the case of the

correct model. Let the i.i.d. data follow the distribution Yi  N(2, 1) , i = 1, . . . , n . The

true likelihood function is L() = -

n i=1

(Yi

-

)2/2.

Table 1 shows the effective coverage probabilities of the quantiles estimated using the

multiplier bootstrap. The second line contains the range of the nominal confidence levels:

0.99, . . . , 0.75 . The first left column describes the distribution of the bootstrap weights:

N(1, 1) or exp(1) . The 3-d and the 4-th lines show the frequency of the event: "the real

likelihood ratio  the quantile of the bootstrap likelihood ratio".

Table 1: Coverage probabilities for the correct i.i.d. model

L(ui) exp(1) N(1, 1)

0.99 0.99 0.99

Confidence levels 0.95 0.90 0.85 0.80 0.94 0.89 0.83 0.78 0.95 0.89 0.84 0.80

0.75 0.73 0.75

3.2 Constant regression with misspecified heteroscedastic errors
Here we show on a constant regression model that the quality of the confidence sets obtained by the multiplier bootstrap procedure is not significantly deteriorated by misspecified heteroscedastic errors. Let the data be defined as Yi = 2 + ii , i = 1, . . . , n . The i.i.d. random variables i  Lap(0, 2-1/2) are s.t. IE(i) = 0 , Var(i) = 1 . The coefficients i are deterministic: i d=ef 0.5 {4 - i (mod 4)} . The quasi-likelihood function is the same as in the previous experiment: L() = - in=1(Yi - )2/2 , i.e. it is misspecified, since it corresponds to the i.i.d. standard normal distribution. Table 2 describes the 2 -nd experiment's results similarly to the Table 1.

14 Bootstrap confidence sets under model misspecification

Table 2: Coverage probabilities for the misspecified heteroscedastic noise

L(ui) exp(1) N(1, 1)

0.99 0.98 0.98

Confidence levels 0.95 0.90 0.85 0.80 0.93 0.87 0.82 0.77 0.94 0.88 0.83 0.78

0.75 0.72 0.73

3.3 Biased constant regression with misspecified errors
In the third experiment we consider biased regression with misspecified i.i.d. errors:
Yi =  sin(Xi) + i, i  Lap(0, 2-1/2), i.i.d,
Xi are equidistant in [0, 2].
Taking the likelihood function L() = - in=1(Yi - )2/2 yields  = 0 . Therefore, the larger is the deterministic amplitude  > 0 , the bigger is bias of the mean constant regression. We consider two cases:  = 0.25 with fulfilled (SmB) condition and  = 1.25 when (SmB) does not hold. Table 3 shows that for the large bias quantiles yielded by the multiplier bootstrap are conservative. This conservative property of the
Table 3: Coverage probabilities for the misspecified biased regression
Confidence levels L(ui)  0.99 0.95 0.90 0.85 0.80 0.75
0.25 0.98 0.94 0.89 0.84 0.79 0.74 N(1, 1)
1.25 1.0 0.99 0.97 0.94 0.91 0.87
multiplier bootstrap quantiles is also illustrated with the graphs in Figure 3.1. They show the empirical distribution functions of the likelihood ratio statistics L() - L() and L ab( ba ) - L ba () for  = 0.25 and  = 1.25 . On the right graph for  = 1.25 the empirical distribution functions for the bootstrap case are smaller than the one for the Y case. It means that for the large bias the bootstrap quantiles are bigger than the Y quantiles, which increases the diameter of the confidence set based on the bootstrap quantiles. This confidence set remains valid, since it still contains the true parameter with a given confidence level.
Figure 3.2 shows the growth of the difference between the quantiles of L ab( ab)-L ba () and L()-L() with increasing  for the range of the confidence levels: 0.75, 0.8, . . . , 0.99 .

spokoiny, v. and zhilova, m. Figure 3.1: Empirical distribution functions of the likelihood ratios

15

Yi = 0.25 sin(Xi) + Lap(0, 2-1/2), n = 50 Yi = 1.25 sin(Xi) + Lap(0, 2-1/2), n = 50 empirical distribution function of L() - L() estimated with 104 Y samples 50 empirical distribution functions of L ab( ba ) - L ba () estimated with 104 {ui}  exp(1) samples
Figure 3.2: The difference "Bootstrap quantile" - " Y -quantile" growing with modeling bias

16 Bootstrap confidence sets under model misspecification

3.4 Logistic regression with bias
In this example we consider logistic regression. Let the data come from the following distribution:

Yi  Bernoulli(Xi), Xi are equidistant in [0, 2],   (0, 1/2].

Consider the likelihood function corresponding to the i.i.d. observations:

L() =

n i=1

Yi - log(1 + e)

.

By definition (1.4)  = log{/(1 - )} , bigger values of  induce larger modeling
bias. The graphs below demonstrate the conservativeness of bootstrap quantiles. Here
we consider two cases:  = 0.1 and  = 0.5 . Similarly to the Example 3.3 in the case
of the bigger  on the right graph in Figure 3.3 the empirical distribution functions of L ab( ba ) - L ba () are smaller than the one for L() - L() .

Figure 3.3:

Yi  Bernoulli(0.1Xi), n = 50

Yi  Bernoulli(0.5Xi), n = 50

empirical distribution function of L() - L() estimated with 104 Y samples 50 empirical distribution functions of L ab( ab) - L ba () estimated with 104
{ui}  exp(1) samples

4 Conditions
Here we state the conditions necessary for the main results. The conditions in Section 4.1 come from the general finite sample theory by Spokoiny (2012a), they are required

spokoiny, v. and zhilova, m.

17

for the results of Sections A.1 and A.2. Spokoiny (2012a) considers the examples of i.i.d. setup, generalized linear model and linear median regression providing a check of conditions from Section 4.1. The conditions in Section 4.2 are necessary to prove the results on multiplier bootstrap from Section 2.

4.1 Basic conditions

Introduce the stochastic part of the likelihood process: () d=ef L() - IEL() , and its marginal summand: i() d=ef i() - IE i() .

(ED0) There exist a positive-definite symmetric matrix V02 and constants g > 0, 0  1 such that Var {()}  V02 and

sup

log IE exp

 

  ( )

 IRp

V0

 022/2,

||  g.

(ED2) There exists a constant   0 and for each r > 0 a constant g2(r) such that it holds for all   0(r) and for j = 1, 2

sup log IE exp
j IRp
j 1

 

1

D0-1 2  ()D0-1  2

 022/2,

||  g2(r).

(L0) For each r > 0 there exists a constant (r)  0 such that for r  r0 ( r0 comes from condition (A.1) of Theorem A.1 in Section A.1) (r)  1/2 , and for all   0(r) it holds
D0-1D2()D0-1 - Ip  (r), where D2() d=ef -2 IEL() , 0(r) d=ef { : D0( - )  r} .
(I) There exists a constant a > 0 s.t. a2D02  V02 .

(Lr) For each r  r0 there exists a value b(r) > 0 s.t. rb(r)   for r   and  : D0( - ) = r it holds
-2 {IEL() - IEL()}  r2b(r).

4.2 Conditions required for the bootstrap validity
(SmB) There exists a constant s2mb  [0, 1/8] such that it holds for all i = 1, . . . , n and the matrices H02 , B02 defined in (1.7) and (1.8).
H0-1B02H0-1  s2mb  Cpn-1/2,

18 Bootstrap confidence sets under model misspecification

(ED2m) For each r > 0 , i = 1, . . . , n , j = 1, 2 and for all   0(r) it holds for the values   0 and g2(r) from the condition (ED2) :

sup log IE exp
j IRp
j 1

 

1

D0-1 2 i ()D0-1  2

 022 , 2n

||  g2(r),

(L0m) For each r > 0 , i = 1, . . . , n and for all   0(r) there exists a constant Cm(r)  0 such that
D0-12 IE i()D0-1  Cm(r)n-1.

(L3m) For all    and i = 1, . . . , n it holds D0-13IE i()D0-1  C . (IB) There exists a constant aB2 > 0 s.t. a2BD02  B02 .
(SD1) There exists a constant 0  v  Cp/n. such that it holds for all i = 1, . . . , n with exponentially high probability

H0-1  i() i() - IE  i() i() H0-1  v2.

(Eb) The i.i.d. bootstrap weights ui have continuous c.d.f., and it holds for all i =

1, . . . , n :

IE abui = 1 ,

ba Var ui = 1 ,

log IE ba exp {(ui - 1)}  022/2,

||  g.

4.3 Dependence of the involved terms on the sample size and parameter dimension

Here we consider the case of the i.i.d. observations Y1, . . . , Yn and x = C log n in order

to specify the dependence of the non-asymptotic bounds on n and p . Example 5.1 in



Spokoiny (2012a) demonstrates that in this situation g = C n and  = C/ n . then

 Z(x) = C p + x

for some constant

C  1.85 , for the function

Z(x)

given in (A.3) in

Section A.1. Similarly it can be checked that g2(r) from condition (ED2) is proportional 
to n : due to independency of the observations

log IE exp

 

1

D0-12 ()D0-12

=

n
log IE exp
i=1

 n

1 / n

1

d-0 12i()d0-12



2 nC
n

 for ||  g2(r) n,

spokoiny, v. and zhilova, m.

19

where i() d=ef i() - IE i() , d20 d=ef -2 IE i() and D02 = nd20 in the i.i.d. case.

Function g2(r) denotes the marginal analog of g2(r) .



Let us show, that for the value (r) from the condition (L0) it holds (r) = Cr/ n .

For some 

D0-1D2()D0-1 - Ip = D0-1( - ) 3IEL()D0-1

= D0-1( - ) D0D0-13 IEL()D0-1

 r D0-1

D0-1 3 IE L()D0-1

  Cr/ n

(by condition (L3m) ).

Similarly Cm(r)  Cr in condition (L0m) . 
If (r) = Cr/ n is sufficiently small, then the value b(r) from condition (Lr) can be taken as C{1 - (r)}2 . Indeed, by (L0) and (Lr) for  : D0( - ) = r

-2 {IEL() - IEL()}  r2 1 - 2(r) .

Therefore, if (r) is small, then b(r) d=ef C{1 - 2(r)}  const . Due to the obtained orders the conditions (A.1) and (A.17) of Theorems A.1 and A.6 on concentration of the
ab  MLEs ,  require r0  C p + x .

5 Approximation of distributions of 2 norms of sums random vectors

Consider two samples 1, . . . , n and 1, . . . , n , each consists of centered independent random vectors in IRp with nearly the same second moments. This section explains how one can quantify the closeness in distribution between the norms of  = i i and of  = i i . Suppose that
IEi = IEi = 0, Var i = i, Var i = i, i = 1, . . . , n.

Let also

 d=ef  d=ef Var  =

n
i=1 i,
n
i=1 i,

 d=ef

n
i=1 i,

 d=ef Var  =

n i=1

i.

(5.1) (5.2)

Also introduce multivariate Gaussian vectors i, i which are mutually independent for i = 1, . . . , n and

 d=ef

i  N (0, i),
n
i=1 i  N (0, ),

i  N (0, i),

 d=ef

n i=1

i



N (0, ).

(5.3)

20 Bootstrap confidence sets under model misspecification

The bar sign for a vector stands here for a normal distribution. The following theorem gives the conditions on  and  which ensure that  and  are close to each

other in distribution. It also presents a general result on Gaussian approximation of 

with  .

Introduce the following deterministic values, which are supposed to be finite:

n

d=ef

1 2

n

IE

i=1

i 3 + i 3 ,

n

d=ef

1 2

n

IE

i=1

i 3 + i 3 .

(5.4)

Theorem 5.1. Assume for the covariance matrices defined in (5.2) that

-1/2-1/2 - Ip  1/2, and tr -1/2-1/2 - Ip 2  2

(5.5)

for some 2  0 . The sign · for matrices denotes the spectral norm. Let also for z, z  2 and some z  0 |z - z|  z , then it holds for all 0 <   0.22

1.1.

IP (   z) - IP

 z



16n-3

+



+ z

z

p/2 + /2



16n-3

+

(

+

 z)/ 2

+

 /2

for

z



 p,

1.2.

|IP ( 

 z) - IP ( 

 z)|  16-3 n + n

+ 2 + z z

p/2 + /2

 16-3 n + n

 + (2 + z)/ 2 + /2

for

z



 p.

Moreover, if z, z  max{2, p} and max{n1/4, n1/4}  0.11 , then
 2.1. IP (   z) - IP   z  1.55n1/4 + z/ 2 + /2,
 2.2. |IP (   z) - IP (   z)|  1.55 n1/4 + n1/4 + z/ 2 + /2.

Proof of Theorem 5.1. The inequality 1.1 is based on the results of Lemmas 5.3, 5.6 and

5.7:

by L. 5.3
IP (   z)  IP

  z -  + 16-3n

by L. 5.7
 IP

  z -  + 16-3n + /2

by L. 5.6
 IP

  z + 16-3n + /2 + (z + )z-1 p/2.

The inequality 1.2 is implied by the triangle inequality and the sum of two bounds: the bound 1.1 for IP (   z) - IP   z and the bound

IP (   z) - IP   z  16n-3 + z-1 p/2,

spokoiny, v. and zhilova, m.

21

which also follows from 1.1 by taking  :=  , z := z . In this case  =  and  = z = 0 .
The second part of the statement follows from the the first part by balancing the 
error term 16n-3 + / 2 w.r.t.  .

Remark 5.1. The approximation error in the statements of Theorem 5.1 includes three terms, each of them is responsible for a step of derivation: Gaussian approximation, Gaussian comparison and anti-concentration. The value  bounds the relation between covariance matrices, z corresponds to the difference between quantiles. n1/4 comes from the Gaussian approximation, under certain conditions this is the biggest term in the expressions 2.1, 2.2 (cf. the proof of Theorem 2.1).

Remark 5.2. Here we briefly comment how our results can be compared with what is

available in the (2003) obtained

literature. In the case

the

rate

IE

i

3

 /n

of i.i.d. for the

vectors error of

i and Var i  Ip Bentkus approximation supAA IP ( 

A) - IP (  A) , where A is a class of all Euclidean balls in IRp . G¨otze (1991) showed

for independent vectors i and their standardized sum  :

GAR



 C1

p

C2p

n i=1

IE

i

3/n,

n i=1

IE

i

3/n,

p  [2, 5], p  6,

where GAR d=ef supBB IP (  B) - IP (  B) and B is a class of all measurable convex sets in IRp , the constants C1, C2 > 150 . Bhattacharya and Holmes (2010)

argued that the results by Go¨tze (1991) might require more thorough derivation, they

obtained the rate in the i.i.d. case).

p5/2 Chen

n i=1

IE

i

and Fang

3 for (2011)

the previous bound prove that GAR 

(and p5/2IE 1 3/n1/2

 115 p

n i=1

IE

i

3

for

independent vectors i with a standardized sum. Go¨tze and Zaitsev (2014) obtained the

rate IE i 4/n for i.i.d. vectors i with a standardized sum but only for p  5 . See also

Prokhorov and Ulyanov (2013) for the review of the results about normal approximation

of quadratic forms.

Our results ensure the error of the Gaussian approximation of order 1.55n1/4 

1.31

n i=1

IE

i 3 + i 3

1/4 . The technique used here is much simpler than in the

previous works, and the obtained bounding terms are explicit and only use independence

of the i and i . However, for some special cases, the use of more advanced results on Gaussian approximation may lead to sharper bounds. For instance, for an i.i.d. sample, the GAR error rate GAR = p3/n by Bentkus (2003) is better then ours (p3/n)1/8 , and in the one-dimensional case Berry-Esseen's theorem would also work better (see

Section 5.1). In those cases one can improve the overall error bound of the bootstrap 
approximation by putting GAR in place of the sum 16n-3 + / 2 . Section 5.3

22 Bootstrap confidence sets under model misspecification

comments how our results can be used to obtain the error rate p3/n by using a smoothed quantile function.

5.1 The case of p = 1 using Berry-Esseen theorem

Let us consider how the results of Theorem 5.1 can be refined in the case p = 1 using Berry-Esseen theorem. Introduce similarly to n and n from (5.4) the bounded values

n,B.E. d=ef

n i=1

IE|i|3,

n,B.E. d=ef

n i=1

IE|

i|3.

(5.6)

Due to Berry-Esseen theorem by Berry (1941) and Esseen (1942) it holds

sup IP (||  z) - IP ||  z
zIR
sup IP (||  z) - IP ||  z
zIR



2C0

n,B.E. (Var )3/2

,



2C0

n,B.E. (Var )3/2

,

(5.7)

for the constant C0  [0.4097, 0.560] by Esseen (1956) and Shevtsova (2010).

Lemma 5.2. Under the conditions of Theorem 5.1 it holds

1.

IP (||  z) - IP ||  z



2C0

n,B.E. (Var )3/2

+

 2

+

z 1 2z



2C0

n,B.E. (Var )3/2

+

 2

+

z 2

for z  1,

2. |IP (||  z) - IP (||  z)|

 2C0  2C0

n,B.E. + n,B.E. (Var )3/2 (Var )3/2
n,B.E. + n,B.E. (Var )3/2 (Var )3/2

+  + z 1 2 2z
+  + z for z  1. 22

Proof of Lemma 5.2. Similarly to the proof of Theorem 5.1:

(5.8)

by (5.7)
IP (||  z)  IP ||  z + 2C0(Var )-3/2n,B.E.

by L. 5.7
 IP

||  z

+ 2C0(Var )-3/2n,B.E. +  /2

by L. 5.6
 IP

||  z

+ 2C0(Var )-3/2n,B.E. +  /2 + zz-12-1/2.

The analogous chain in the inverse direction finishes the proof of the first part of the statement. The second part is implied by the triangle inequality applied to the first part and again to it with  :=  and z := z .

spokoiny, v. and zhilova, m.

23

5.2 Gaussian approximation of 2 norm of a sum of independent vectors
Lemma 5.3 (GAR with equal covariance matrices). For the random vectors  and  defined in (5.1), (5.3), s.t. Var  = Var  , and for n given in (5.4), it holds for all z  2 and   (0, 0.22] :

IP (   z)  IP   z -  + 16-3n, IP (   z)  IP   z +  - 16-3n.

Proof of Lemma 5.3. It holds for z  IR IP (   z) = IE 1I {   z} . The main idea of the proof is to approximate the discontinuous function 1I {   z} by a smooth function f(, z) and then to apply the Lindeberg's telescopic sum device. Let us introduce a non-negative function g(·)  C2(IR) , which grows monotonously from 0 to 1:



0,







16x3/3,





g(x)

d=ef

 0.5

+

2(x

-

0.5)

-

16(x

-

0.5)3/3,

  1 + 16(x - 1)3/3,     1,

x  0, x  [0, 1/4], x  [1/4, 3/4], x  [3/4, 1], x  1.

(5.9)

It holds for all x  IR 1I {x  1}  g(x)  1I {x  0} . Hence, for the function f(, z) d=ef g (  2 - z2)/(2z) with z,  > 0 , it holds due to 1I {   z} = 1I  2 - z2 /2  0 :

1I {   z + }  1I  2  z2 + 2z  f(, z)  1I {   z} .

(5.10)

Due to Lemma 5.4 one can apply the Lindeberg's telescopic sum device (see Lindeberg (1922)) in order to approximate IEf(, z) with IEf(, z) . Define for k = 2, . . . , n-1 the following random sums

k-1 n

Sk d=ef

i +

i,

i=1 i=k+1

n

S1 d=ef

i,

i=2

n-1

Sn d=ef

i.

i=1

The difference f(, z) - f(, z) can be represented as the telescopic sum:

f(, z) - f(, z) =

n
k=1 f(Sk + k, z) - f(Sk + k, z) .

Due to Lemma 5.4 and the third order Taylor expansions of f(Sk + k, z) and f(Sk +

24 Bootstrap confidence sets under model misspecification

k, z) w.r.t. the first argument at Sk , it holds for each k = 1, . . . , n :

f(Sk + k, z) - f(Sk + k, z) - f(Sk, z) (k - k)

-

1 2

(k

-

k )

2 f(Sk, z)(k + k)

 C(, z)

k 3 + k 3 /6,

where the value C(, z) is defined in (5.14). As Sk and k - k are independent, IEk = IEk = 0 and Var k = Var k , we derive

IEf(, z) - IEf(, z) =

n
k=1 IEf(Sk + k, z) - IEf(Sk + k, z)

 C(, z)

n
IE
k=1

k 3 + k 3 /6

(by Def. (5.4)) = C(, z)n/3.

(5.11)

Combining the derived bounds, we obtain:

IP ( 



z

+ )

by

(5.10) 

IEf(, z)

by

(5.11) 

IEf(, z) +

C(, 3

z)

n

by (5.10)  IP



z

+

C(, 3

z) n,

or IP (   z)  IP   z -  + C(, z - )n/3. Interchanging the arguments  and  implies the inequality in the inverse direction:

IP (   z)  IP   z +  - C(, z)n/3.

Let us bound the constants C(, z) and C(, z - ) for the function g(x) given above in (5.9). |g (x)|  8 and |g (x)|  32 for all x  IR . By definition (5.14) it holds for 0 <   0.22 and z  2 :

C(, z)  C(, z - )  -348.

(5.12)

Lemma 5.4 (A property of the smooth approximant of the indicator). Let a function g(·)  C2(IR) be non-negative, monotonously increasing from 0 to 1 s.t. g(x) = 0 for x < 0, g(x) = 1 for x  1 . It holds for all , 0  IRp , z,   0 , for the Euclidean norm · and for the function

f(, z) d=ef g

1 (  2 - z2) 2z

(5.13)

f(0 + , z) - f(0, z) -  f(0, z) -  2f(0, z)/2  C(, z)  3/3!,

spokoiny, v. and zhilova, m.

25

where

C(, z)

d=ef

1 3

 1/2 1+2
z

 1+2
z

 g +3z g  .

Proof of Lemma 5.4. By the Taylor formula:

(5.14)

f(0 + , z) = f(0, z) +  f(0, z) +  2 f(0, z)/2 + R3,

where R3 is the 3-d order remainder term. Consider for   IRp :  = 1 and t  R

the function f(0 + t, z) = g

1 2z

(

0 + t

2 - z2)

. It holds

|R3| 

3 sup
3! IRp, 

sup
=1 tIR

d3f(0 + t, z) dt3

.

Now let us bound the third derivative

d3 dt3

f

(

+

t

,

z

)

:

df( + t, z) = 

( + t) g

1 (  + t 2 - z2) ,

dt z 2z

d2f( + t, z) dt2

=

{

( + t)}2

(z)2

g

1 (  + t 2 - z2) 2z

1 +g

1 (  + t 2 - z2) ,

z 2z

d3f( + t, z) dt3

=

{

( + t)}3

(z)3

g

1 (  + t 2 - z2) 2z

 ( + t) + 3 (z)2 g

1 (  + t 2 - z2) . 2z

Now we use that g (x) and g

(x) vanish if x < 0 or x  1 . The inequality

1 2z

(

+

t 2 - z2)  1 implies in view of  = 1 that

 ( + t)   + t  (2z + z2)1/2.

Therefore

d3f(0 + t, z) dt3



1 3

 1/2 1+2
z

 1+2
z

 g +3z g  .

26 Bootstrap confidence sets under model misspecification

5.3 Results for the smoothed indicator function

Theorem 5.5 (Theorem 5.1 for a smoothed indicator function). Under the conditions of Theorem 5.1 it holds for all z  [0, 1] and the function f(, z) defined in (5.13):

1.

IEf(, z) - IEf(, z)



16 3 n

+

 2p

z

z

+

 p

z2 z2

+ 



16 3 n

 + 5z

+ 

for

z



 p.

2.

|IEf(, z) - IEf(, z)|



16 3

n + n

+

 2p

z

z

+

 p

z2 z2

+





16 3

 n + n + 5z + 

for

z



 p.

Remark 5.3. The approximating bounds above do not contain the term proportional

to  unlike the bound in Theorem 5.1. This yields the smaller error terms for the case

of the smoothed indicator.

Proof of Theorem 5.5. The following inequality is proved in Lemma 5.3 (see the expres-

sion (5.11)): IEf(, z) - IEf(, z)  C(, z)n/3 . The function f(, z) is non-increasing in z :

df(, z) = - 1 dz 2

2 1 + z2

g

1 2z

 2 - z2

 0.

The definition of f(, z) yields for z  z , a d=ef z/z  1 and any 

f(, z)  f(, z)  f(a, z), 0  f(, z) - f(, z)  f(a, z) - f(, z).

(5.15)

Lemma 5.8 yields for z  z( 3/2 - 1) :

IEf(a, z) - IEf(, z)

 p

z2 z2

-1



 2p

z

z

+

 p

z2 z2



 (1 + 3/2)z  5z for z  p.

Inequalities similar to (5.15) hold for z  z and a d=ef z/z , therefore, by triangle inequality, bound (5.12) on C(, z) and Lemma 5.8:

IEf(, z) - IEf(, z)



16 3 n

+

 2p

z

z

+

 p

z2 z2

+ 



16 3 n

 + 5z

+ 

for

z



 p.

The second part of the statement follows from triangle inequality applied to the first inequality and again to the same one with  :=  and z := z .

spokoiny, v. and zhilova, m.

27

5.4 Gaussian anti-concentration and comparison by Pinsker's inequality

Lemma 5.6 (Anti-concentration bound for 2 norm of a Gaussian vector). Let   N (0, ) ,   IRp , then it holds for all z > 0 and 0    z :

 IP   z +  - IP   z   p/(z 2)

  / 2

for

z



 p.

Proof of Lemma 5.6. It holds IP

  z +  = IP



z

,

where



d=ef



z z+

.

The Kullback-Leibler divergence between

IP1 d=ef N (0, )

and

IP2 d=ef N

0,



z2 (z+)2

is

equal to

KL(IP1, IP2) = p (/z)2 + 2(/z) - 2 log(1 + /z) /2  p(/z)2 for 0    z.

We use Pinsker's inequality in the following form (see the book by Tsybakov (2009), pp. 88, 132): for a measurable space (, F ) and two measures on it IP1, IP2 :

sup |IP1(A) - IP2(A)|  KL(IP1, IP2)/2.
AF

(5.16)

Therefore, it holds:

IP

  z +  - IP

 z



KL(IP1,

IP2)/2



  p/(z 2).

Lemma 5.7 (Comparison of the Euclidian norms of Gaussian vectors). Let 1  N (0, 1) and 2  N (0, 2) belong to IRp , and
2-1/212-1/2 - Ip  1/2, and tr 2-1/212-1/2 - Ip 2  2 ,
for some 2  0 . Then it holds
sup IP 1  z - IP 2  z  /2.
zR
Proof of Lemma 5.7. Let IP1 = N (0, 1) and IP2 = N (0, 2) . Denote G d=ef 2-1/212-1/2 , then the Kullback-Leibler divergence between IP1 and IP2 is equal to
KL(IP1, IP2) = -0.5 log{det(G)} + 0.5 tr{G - Ip}
p
= 0.5 j=1 {j - log(j + 1)} ,

28 Bootstrap confidence sets under model misspecification

where p  · · ·  1 are the eigenvalues the matrix G - Ip . By conditions of the lemma |1|  1/2 , and it holds:

KL(IP1, IP2)  0.5

p j=1

j2

=

0.5 tr{(G

-

I p )2 }



2 /2,

(5.17)

which finishes the proof due to the Pinsker's inequality (5.16).

Lemma 5.8 (Gaussian comparison, smoothed version). Let 1  N (0, 1) and 2  N (0, 2) belong to IRp , and for some 2  0 :
2-1/212-1/2 - Ip  1/2, and tr 2-1/212-1/2 - Ip 2  2 .
Then it holds for any function f (x) : IRp  IR s.t. |f (x)|  1 :

IEf (1) - IEf (2)  .
Proof of Lemma 5.8. Let IP1 = N (0, 1) and IP2 = N (0, 2) . Due to |f (x)|  1 and Pinsker's inequality (5.16) it holds:

IEf (1) - IEf (2)  |f (x)| · |dIP1(x) - dIP2(x)|
IRp

 |dIP1(x) - dIP2(x)|  2
IRp
Finally, as in (5.17), 2 KL(IP1, IP2)/2  .

KL(IP1, IP2)/2.

A Appendix
This section contains proofs of the main results from Section 2. Due to the scheme (1.6) the key ingredients are:
· the square-root Wilks approximation for the Y -world (Theorem A.2),
· the square-root Wilks approximation for the bootstrap world (Theorem A.4),
· the statement about closeness in distribution of the approximating terms  and ab
 (Proposition A.9).
In Section A.1 we recall some results from the general finite sample theory by Spokoiny (2012a), Spokoiny (2012b) and Spokoiny (2013), including the square-root Wilks approximation in Y case. In Section A.2 we derive the necessary results from the finite sample theory for the bootstrap world (including the square-root Wilks approximation). In Section A.3 we adapt Theorem 5.1 (GAR for 2 norm of a sum of independent vectors) to the setting of maximum likelihood estimation (Proposition A.9). The proofs of the main results are given in Sections A.3, A.4 and A.5.

spokoiny, v. and zhilova, m.

29

A.1 Finite sample theory
Let us use the notations given in the introduction: L() is the log-likelihood process, which depends on the data Y and corresponds to the regular parametric family of probability distributions {IP} . The general finite sample approach by Spokoiny (2012a) does not require the true measure IP to belong to {IP} . The target parameter  is defined as in (1.4) by projection of the true measure IP on {IP} . D02 denotes the full Fisher information p × p matrix, which is deterministic, symmetric and positive-definite:
D02 d=ef -2 IEL().

A centered p -dimensional random vector  denotes the normalised score:  d=ef D0-1L().
Introduce the following elliptic vicinity around the true point  : 0(r) d=ef { : D0( - )  r} .

The non-asymptotic Wilks approximating bound by Spokoiny (2012a), Spokoiny (2013)
requires that the maximum likelihood estimate  gets into the local vicinity 0(r0) of some radius r0 > 0 with probability  1 - 3e-x , x > 0 . This is guaranteed by the following concentration result:

Theorem A.1 (Concentration of MLE, Spokoiny (2013)). Let the conditions (ED0) , (ED2) , (L0) , (I) and (Lr) be fulfilled. If for the constant r0 > 0 and for the function b(r) from (Lr) :

b(r)r  2 Zqf(x, IB) + 60Z(x + log(2r/r0)) , r > r0

(A.1)

where the functions Z(x) and Zqf(x, IB) are defined respectively in (A.3) and (A.4), then it holds

IP  / 0(r0)  3e-x.

The constants , 0 and a come from the imposed conditions (ED0) ­ (I) (from Section 
4). In the case 4.3 r0  C p + x .
The following result is one of the central in the general finite sample theory and is crucial for the present study due to the scheme (1.6):

30 Bootstrap confidence sets under model misspecification

Theorem A.2 (Wilks approximation, Spokoiny (2013)). Under the conditions of Theorem A.1 for some r0 > 0 s.t. (A.1) is fulfilled, it holds with probability  1 - 5e-x
2 L() - L() -  2  W 2(r0, x), 2 L() - L() -   W(r0, x)

for

W(r, x) d=ef 3r {(r) + 60Z(x)} ,

W 2 (r, x)

d=ef

2 3

2r + Zqf(x, IB)

W(r, x),

Z(x)

d=ef

 2p

+

 2x

+

4p(xg-2

+

1)g-1.

(A.2) (A.3)

In the case 4.3 it holds for r  r0 :

W(r,

x)

=

C

p+ x n

,

W 2(r, x) = C

(p + x)3 . n

The constants g and (r) come from the imposed conditions (ED0) , (L0) (from Section 4), and the function Zqf(x, IB) , defined in (A.4), corresponds to the quantile function of deviations of the random value  (see Theorem A.3 below).

The following theorem characterizes the tail behaviour of the approximating term  2 . It means that with a bounded exponential moment of the vector  (condition (ED0) ) its squared Euclidean norm  2 has three regimes of deviations: sub-Gaussian, Poissonian and large-deviations' zone.

Theorem A.3 (Deviation bound for a random quadratic form, Spokoiny (2012b)). Let condition (ED0) be fulfilled, then for g  2 tr(IB2) it holds:

IP  2  Zq2f(x, IB)  2e-x + 8.4e-xc ,

where IB2 d=ef D0-1V02D0-1 , (IB) is a maximum eigenvalue of IB2 ,



 tr(IB2) + 8 tr(IB4)x,





Zq2f(x, IB)

d=ef

 tr(IB2)

+

6x(IB),



 |zc

+

2(x

-

xc

)/gc

|2

(IB

),

x  2 tr(IB4)/{18(IB)}, 2 tr(IB4)/{18(IB)} < x  xc,
x > xc,

(A.4)

spokoiny, v. and zhilova, m.

31

2xc d=ef 2xc(IB) d=ef µczc2 + log det Ip - µcIB2/(IB) , zc2 d=ef g2/µ2c - tr (IB2)/µc /(IB), gc d=ef g2 - µc tr (IB2)/ (IB), µc d=ef 2/3.

(A.5)

The matrix V02 comes from condition (ED0) and can be defined as V02 d=ef Var {L()} .

By condition (I) tr(IB2)  a2p , tr(IB4)  a4p and (IB)  a2 . In the case 4.3 
g = C n , hence xc = Cn , and for x  xc it holds:

Zq2f(x, IB)  a2(p + 6x).

(A.6)

A.2 Finite sample theory for the bootstrap world

Let us introduce the bootstrap score vector at a point    :

 ab() d=ef D0-1 ba ()
n
= D0-1 i()(ui - 1).
i=1

Theorem A.4 (Bootstrap Wilks approximation). Under the conditions of Theorems A.1 and A.6 for some r02  0 s.t. (A.1) and (A.17) are fulfilled, it holds with IP -probability  1 - 5e-x

ba IP

sup 2


L

ab ()

-

L

ba ()

-

ab  ()

2



ab W

2 (r0,

x)

 1 - 4e-x,

ba IP

sup 2

L ab() - L ab()

-

ba  ()



ba W(r0,

x)



where the error terms Wba (r, x), Wab 2(r, x) are deterministic and

 1 - 4e-x.

ba W(r, x)

d=ef

2W(r, x) +

360r1(r)Z(x),

ba W 2 (r, x)

d=ef

1 18

ab 12rW(r,

x)

+

ab W(r,

x)2

.

W(r, x) and Z(x) are defined respectively in (A.2) and (A.3), and 1(r) is given in (A.12). For the case 4.3 and r  r0 it holds:

ab W

(r,

x)

=

C

p+ x n

 x,

ab W 2(r, x) = C

(p

+

x)3

 x.

n

32 Bootstrap confidence sets under model misspecification

Proof of Theorem A.4. Let us consider the following random process in the bootstrap

world for , 1  0(r) :

A

ab (, 1)

d=ef

L

ab ()

-

L

ba (1)

-

(

-

1)

 L

ab (1)

+

1 2

D0( - 1)

2.

It holds A ab(1, 1) = 0 . Taylor expansion w.r.t.  around 1 implies :

A ba (, 1) = ( - 1)



A

ba (1,

1),

where 1 is some convex combination of the vectors  and 1 . Therefore,

|A

ab (,

1)|



D0( - 1)

sup

D0-1  A

ba (,

1)

0(r)

 2r sup

D0-1

A

ab (,

1)

.

0(r)

Now let us consider the normalized gradient process:

D0-1  A

ab (,

1)

=

D0-1

{ L

ab ()

-

 L

ba (1)}

+

D0(

-

1).

(A.7) (A.8)

The deterministic part of it reads as:

D0-1  IE

ba A

ab (,

1)

=

D0-1

{ L()

-

 L(1 )}

+

D0(

-

1).

Proposition 3.1 in Spokoiny (2013) implies due to the conditions (L0) , (ED2) , that the following random event holds with IP -probability at least 1 - e-x for all , 1  0(r) and r  r0 :

D0-1IE ba A ab(, 1)

= D0-1 {L() - L(1)} + D0( - 1)



2 3 W(r, x),

(A.9)

where the deterministic error term W(r, x) is given in (A.2). Denote the stochastic part of D0-1A ba (, 1) as follows:

Y

ab (, 1)

d=ef

D0-1

{ A

ab (, 1)

-

 IE

abA

ab (,

1)}

n
= D0-1 { i() -  i(1)} (ui - 1).
i=1

In order to bound its norm's supremum w.r.t.   0(r) for r  r0 we use the idea from
the proof of Proposition 3.1 in Spokoiny (2013). Let us introduce the new parameters  d=ef D0( - ) and 1 d=ef D0(1 - ) , then

 Y

ab (, 1)

=

n
D0-12 i()D0-1(ui - 1).

i=1

spokoiny, v. and zhilova, m.

33

Thus, we obtain a proper normalisation for Y ba (, 1) . Independency of u1, . . . , un and Lemma A.5 imply with probability  1 - e-x for j = 1, 2 and 1(r) given in
(A.12):

ba sup log IE exp
j IRp
j =1

 1(r)



1



Y

ab (,

1)

2

 202 , 2

||  g2(r).

This allows to apply Theorem A.3 from Spokoiny (2013) on a uniform bound for the norm of stochastic process to 1-1(r)Y ba (, 1) . By the triangle inequality it holds for
r  r0 :

ab IP

sup

Y

ab (,

1)

 120r1(r)Z(x)

 1 - e-x,

,10(r)

(A.10)

where Z(x) is defined in (A.3). Collecting together the bounds (A.8), (A.9) and (A.10) we obtain that the following bound holds with IP -probability at least 1 - e-x :

ba IP

sup

|A

ab (,

1)|



4r

{W(r,

x)/3

+

60r1(r)Z(x)}

 1 - e-x

,10(r)

for r  r0 .

ba

Theorems A.6 and A.1 say that the maximum likelihood estimators  and  get

into

the

local

vicinity

0(r0)

with

exponentially ba

high

IP

ab -

and

IP -probabilities

corre-

spondingly. Therefore, taking  =  and 1 =  in the last bound, we obtain with

dominating probability:

L

ba (

ab )

-

L

ba ()

-

(

ab

-

)

 L

ab ()

+

1 2

ba D0( - )

2

 4r {W(r0, x)/3 + 60r01(r)Z(x)} .

Similarly bounds (A.9) and (A.10) imply:

1 2

ba  ()

ba 2 - 2( - )



L

ab ()

+

ab D0( - ) 2

=

1 2

D0-1

L

ab ()

-

D0(

ab

-

)

2

 2 {W(r0, x)/3 + 60r01(r)Z(x)}2 .

Therefore it holds with IP -probablity at least 1 - 4e-x :

ab IP

L

ba (

ba )

-

L

ba ()

-

1

2

ba  ()

2



ba W

2

(r0,

x)

 1 - 4e-x,

ab W 2 (r0,

x)

d=ef

4r

{W(r0,

x)/3

+

60r01(r)Z(x)}

+ 2 {W(r0, x)/3 + 60r01(r)Z(x)}2 .

(A.11)

34 Bootstrap confidence sets under model misspecification

For the second bound of the statement we use the similar approach as in Theorem 2.3 in Spokoiny (2013).

2

L

ab(

ab )

-

L

ab()

-

ba D0( - )

2

L

ab(

ab )

-

L

ab()

-

ba D0( - ) 2

 ab

D0( - )

=

2A

ab(,



ba )

ab D0( - )

 sup

|2A ab(, 1)|

,10(r0) D0( - 1)

by (A.7)


sup

2

D0-1



A

ba (,

1)

,10(r0)

by (A.9),
(A.10)
 4W(r0, x)/3 + 240r01(r)Z(x).

This together with (A.11) imply the final statement.

Lemma A.5 (Check of the bootstrap equivalent of (ED2) ). Conditions (Eb) , (L0m)
and (ED2m) imply for each r > 0 ,   0(r) , j = 1 , j = 1, 2 and all ||  g2(r) with probability  1 - e-x :

n ab sup log IE exp
j IRp i=1 j =1

 1(r)

1

D0-12

i()D0-12(ui - 1)

 202 . 2

where

1(r)

=

1

d=ef

Cm(r) n

 + 20 2x

(A.12)

In the case 4.3 it holds for r  r0 1(r) = Cr/n + C x/n .

Proof of Lemma A.5. Introduce the independent random scalar values for i = 1, . . . , n and j = 1, 2 :
µi(, j ) d=ef 1 D0-12 i()D0-12.

spokoiny, v. and zhilova, m.

35

It holds

n ab log IE exp
i=1

 1

1

D0-12

i()D0-12(ui - 1)

n ba = log IE exp
i=1

 1

µi(,

 j )(ui

-

1)



202 212

n
µ2i (, j),
i=1

(A.13)

here the inequality (A.13) follows from condition (Eb) if µi(, j)  g1 for all i = 1, . . . , n , which is true due to the arguments below. Let us consider µi(, j) , for each   0(r) , i = 1, . . . n it holds:

µi(, j)  D0-12 IE i()D0-1 + D0-1 2 i() - 2IE i() D0-1 .

(A.14)

Condition (ED2m) , which is a stronger version of (ED2) , implies that for all i = 1, . . . , n ,   0(r) and each r > 0 it holds with IP -probability  1 - e-x

D0-1 2 i() - 2 IE i() D0-1  20

2x

1/2
.

n

(A.15)

Indeed, by the exponential Chebyshev inequality for  > 0

IP -1 D0-1 2 i() - 2IE i() D0-1  t

 IE exp -t + -1 D0-1 2 i() - 2 IE i() D0-1

by (ED2m)
 exp

-t + 202/(2n)

,

0 <  < g2(r)

 exp{-x},

here the last inequality holds under the assumption, that g2(r) is large enough. In the case 4.3 it holds g2(r) = Cn1/2 ,  = Cn-1/2 and x = C log(n) ; t2 := 802x/n implies t-202/(2n)-x  0 for 0 <  < g2(r) . For the deterministic term in (A.14) condition (L0m) reads as:

D0-12IE i()D0-1

 Cm(r) . n

(A.16)

36 Bootstrap confidence sets under model misspecification

Collecting the inequalities (A.13), (A.14), (A.15) and (A.16), we obtain:

n ba log IE exp
i=1



202 1 2 12

 1

1

D0-12

i()D0-12(ui - 1)

Cm(r) n

+

 20 2x

2

Taking 1 = 1(r) as in (A.12) implies the necessary statement.

Theorem A.6 (Concentration of bootstrap MLE). Let the conditions of Theorems A.1 and A.8, (L0m) and (ED2m) be fulfilled. If the following holds for 1(r) defined in (A.12) and the IP -random matrix B2 d=ef D0-1 Var ba {L ab()} D0-1

b(r)r  2 Zqf(x, IB) + Zqf(x, B) + 60Z(x)1(r0)r0

(A.17)

+ 120( + 1(r))Z(x + log(2r/r0)) for r > r0,

then it holds with IP -probability  1 - 3e-x

ba IP

ab  / 0(r0)

 3e-x.

Proof of Theorem A.6. We use the idea by Spokoiny (2013): if

sup\0 bootstrap

(orb0)jecLt(s: )L-ab(L()-)L

<0 ba ()

, then and 

ab

 .

 0(r0) . Denote the

We apply it stochastic part

here for the the of the bootstrap

likelihood

process

as



ab ()

d=ef

L ab() - IE abL ba () .

It

holds

L

ba ()

-

L

ab ()

=



ba ()

-



ba ()

+

IE

ba L

ab ()

-

IE

ba L

ba ()

=



ba ()

-



ab ()

+

L()

-

L()

=



ba ()

-



ba ()

+

L() - L()

+

L() - L() .

Here the last summand L() - L() is non-positive by definition (1.2) of  . The following bound follows from the proof of Theorem 2.1 in Spokoiny (2013):

IP sup L() - L() < (r, x)r + rZqf(x, IB) - r2b(r)/2  1 - 3e-x,
\0(r0)
(r, x) d=ef 60Z(x + log(2r/r0)).
Due to Lemma A.5 the process  ab() -  ba () satisfies the necessary conditions of Theorem A.1 in Spokoiny (2013), and it holds for r  r0

ba IP

sup



ab ()

-



ba ()

-

(

-

)

 

ab ()



1(r, x)r

 1 - e-x,

0(r)

1(r, x) d=ef 60Z(x + log(2r/r0))1(r).

spokoiny, v. and zhilova, m.

37

By Lemma A.7 and Theorem A.8 it holds with dominating probability

sup

( - )

 

ab ()

r

ab  ()

0(r)

r

 ba ()

+



ba ()

-



ba ()

 r Zqf(x, B) + 60Z(x)1(r0)r0 .

Finally we have:

sup

L

ba ()

-

L

ab ()

\0(r0)

 sup L() - L()
\0(r0)

+ sup



ab ()

-



ba ()

0(r), rr0

 rZqf(x, B) + rZqf(x, IB) + 1(r, x)r

+ (r, x)r - r2b(r)/2 + 60Z(x)1(r0)rr0,

which implies the condition (A.17) in the statement.

Remark A.1. Condition (A.17) imposed for the bootstrap MLE concentration result is stronger, than condition (A.1) for the concentration of Y - MLE, and (A.17) implies the latter one.
The following lemma had already been derived in the proof of Theorem A.4: see the bound (A.10). We formulate it separately, since it is used again in another statements.

Lemma A.7. Let the conditions of Lemma A.5 be fulfilled, then it holds with IP probability  1 - e-x

where

ba IP

sup



ba ()

-



ab()



ab  (r,

x)

 1 - e-x,

0(r)

ba  (r, x)

d=ef

60Z(x)1(r)r

In the case 4.3 it holds for the bounding term.

ab  (r0

,

x)



C

p+ x n

 x.

Theorem A.8 (Deviation bound for the bootstrap quadratic form). Let conditions (Eb) , (I) , (SD1) , (IB) be fulfilled, then for g  2 tr(B2) it holds:

ab IP

 ba () 2  Zq2f(x, B)  1 - 2e-x - 8.4e-xc(B),

38 Bootstrap confidence sets under model misspecification

where

B2 d=ef D0-1V2()D0-1, V2() d=ef Var ab L ab(),

(A.18)

Zqf(x, ·) and xc(·) are defined respectively in (A.4) and (A.5). Similarly to (A.6) it holds for x  xc(B) :

Z2qf(x, B)  a ab2(p + 6x) for a ab2 d=ef (1 + V2 )(a2 + a2B).

(A.19)

Proof of Theorem A.8. This result is the bootstrap equivalent of Theorem A.3. For the
Y -world it demands condition (ED0) to be fulfilled. Let us check whether the bootstrap equivalent of (ED0) holds. It reads as follows: there exist constants g ba > 0, 0ba  1 such that for the positive-definite symmetric matrix V2() it holds for all ||  g ba

ab sup log IE exp
 IRp

 

{L ab() - IE ba L ab()} V ( )

 0ba 22/2.

n
By definition V2() =  i() i() . Let us introduce the independent IP -

i=1

random variables si() d=ef   i()/ V() for i = 1, . . . , n . It holds

n i=1

si2 ( )

=

1 , hence max1in |si|  1 . Condition (Eb) implies:

ba log IE exp

 

{L ab() - IE ba L ab()} V ( )

= n log IE ab exp {si()(ui - 1)}
i=1

 022 2

n

s2i () = 022/2,

i=1

||  g.

Thus the bootstrap equivalent for the condition (ED0) is fulfilled with the same constants 0, g , and the theorem's statements holds as well as for Theorem A.3.
The inequality (A.19) follows from conditions (I) , (IB) , (SD1) and Bernstein matrix inequality by Tropp (2012) (see Section A.6):

D0-1V02()D0-1  D0-1H0 2(1 + V2 )  (1 + V2 )(a2 + aB2 ).

spokoiny, v. and zhilova, m.

39

A.3 Proofs of Theorems 2.1 and 2.3

In order to justify theoretically the multiplier bootstrap procedure it has to be shown

ab that the approximating terms  and  () from the Wilks Theorems A.2 and A.4

have nearly the same distributions. By Lemma A.7 the random values  ba () and

ab  ()

are close to each other within the error term  C(p+x)

x/n with exponentially

high probability, therefore, it is sufficient to compare the distributions of  ab() and

 . This is done in Proposition A.9 using the results on Gaussian approximation for

Euclidean norms from Section 5.

Let us introduce the multivariate normal vectors similarly to (5.3):

  N (0, Var ),  ba ()  N (0, Var ab{ ab()}).

(A.20)

Let us also represent the vectors  and  ab() as sums of the marginal score vectors i and iba () s.t. IEi = IE abiba = 0 :

i d=ef D0-1 { i() - IE i()} , iba () d=ef D0-1 i(){ui - 1}.

Their Gaussian analogs are

i  N (0, Var i)

and

ab i



N

(0,

Var

ab{iab()}).

Similarly to (5.4) denote

n

d=ef

1 2

n

IE

i=1

i 3 + i 3 ,

n

d=ef

1 2

n

IE ab

iab() 3 + iba () 3 .

i=1

(A.21)

Proposition A.9 (Closeness of the c.d.f. of  and  ba () ). If conditions (SmB)

and (SD1) are fulfilled, then it holds with probability  1 - e-x for all 0 <   0.22

and for all z, z > 2 s.t. |z - z|  z for some z  0 :

IP ( 

 z) - IP

ab (

 ab()

 z)

 16-3  16-3

n + n n + n

+ 2 + z z

p 2

+

p 2

V2 (x) + s2mb 1 - V2 (x)



+ 2+ z + 2 p 23

V2 (x) + s2mb

for

z



 p,

V2 (x)  1/4.

40 Bootstrap confidence sets under model misspecification Moreover, if z, z  max{2, p} and max{n1/4, n1/4}  0.11 , then

|IP ( 

 z) - IP

ab (

 ba ()

 z)|



 1.55 n1/4 + n1/4

+ z

2 +

p

23

V2 (x) + s2mb

.

(A.22)

Proof of Proposition A.9. We use Theorem 5.1 taking  :=  and  :=  ab() . Let us check that the conditions (5.5) on the covariance matrices are fulfilled. By definitions (1.7), (1.8) and (A.18)

Var  = D0-1H02D0-1 - D0-1B02D0-1, Var ba { ba ()} = D0-1V2()D0-1.

Due to Theorem A.13 by Tropp (2012) (see Section A.6) it holds with probability  1 - e-x

H0-1V2()H0-1 - Ip  V2 (x),

(A.23)

therefore, by Cauchy-Schwarz inequality

V-1()H02V-1() - Ip  V2 (x)(1 - V2 (x))-1.

Condition (SmB) says that H0-1B02H0-1  s2mb , therefore, by the triangle inequality it holds:

Var ab { ba ()} -1/2 Var{} Var ba { ab()} -1/2 - Ip



V2 (x) + s2mb 1 - V2 (x)

 1/2

for s2mb  1/8, V2 (x)  1/4.

Now we are ready to collect all the obtained bounds together for the following

spokoiny, v. and zhilova, m.

41

Proof of Theorem 2.1. On a random set of probability  1 - 12e-x it holds:

 (Def=. (2.3)) IP ab

2

L

ab(

ba )

-

L

ba ()

ba > z

(Th. A.4)


IP ab

ab ba ba  () > z + W(r0, x)

(L. A.7)


ab IP

 ba ()

ab ba

ba

> z + W(r0, x) +  (r0, x)

(Prop. A.9)
 IP ( 

>

ba z

-

W(r0,

x))

-

full

(Th. A.2)
 IP

2 L() - L()

ab > z

- full,

(A.24) (A.25)

where the value full comes from the bound (A.22) with z := W(r0, x) + Wab (r0, x) +

ab(r0, x) :

full d=ef 1.55 n1/4 + n1/4

 2p +
3

ab

V2 (x) + s2mb ab 

+ W(r0, x) + W(r0, x) +  (r0, x) / 2

(A.26)

By the similar arguments in the inverse direction we obtain the following inequality:

IP

2 L() - L()

ba > z

-   full.

Notice that inequality (A.22) from Proposition A.9, that we use here, requires max{n1/4, n1/4}  0.11 .

Let us quantify, how the error term full depends on p and n . In the case 4.3 random vectors i and iba () satisfy the conditions of Theorems A.3 and A.8 correspondingly. Hence i , iab()  C (p + x)/n and n, n  C (p + x)3/n . Finally we have in the case 4.3

full = C

(p + x)3 n

1/8

+

C

p+

x

 x

+

C

p+

x

.

nn

(A.27)

Remark A.2. It is clear from expression (A.27), that the impact of the error term, induced by the Gaussian approximation, is the biggest. The requirement for the ratio (p+x)3/n to be small is imposed by our Gaussian approximation results (see also Remark 5.2 about the multivariate GAR).

Let us introduce for p = 1 similarly to (5.6) and (A.21)

n

n,B.E. d=ef

IE|i|3,

i=1

n,B.E. d=ef n IE ab|iab()|3.
i=1

42 Bootstrap confidence sets under model misspecification

Proof of Theorem 2.3. On a random set of probability  1 - 12e-x it holds:



(Def. (2.3))

ba

= IP

2

L

ab(

ba )

-

L

ba ()

ba > z

(Th. A.4)


ba ba

ab ab

IP  () > z + W(r0, x)

(L. A.7)


ab IP

 ba ()

ba ab

ba

> z + W(r0, x) +  (r0, x)

(L. 5.2, Prop. A.9)
 IP ( 

>

ba z

-

W(r0,

x))

-

B.E.,

full

(Th. A.2)


IP

2 L() - L()

ab > z

- B.E., full,

ab where the value B.E., full comes from the bound (5.8) with z := W(r0, x)+W(r0, x)+ ab(r0, x) , C0  [0.4097, 0.560] and

Var ab{ ab()}  {1 - V2 (x)}IE Var ba { ab()}



3 4

D0-1H02D0-1

for V2 (x)  1/4

with probability  1 - e-x (due to the bound (A.23)):

B.E., full d=ef 2C0

n,B.E. (Var )3/2

+

(IE

Var ab{n,Bab.E(.)})3/2

2 3 3

+ 1 2

ba ab W(r0, x) + W(r0, x) +  (r0, x)

+2 3

V2 (x) + 2

 C 1+ x in the case 4.3. n

(A.28)

The similar inequalities in the inverse direction finish the proof with the error term

A.4 Proof of Theorem 2.4 (large modeling bias)
Lemma A.10 (Lower bound for deviations of a Gaussian quadratic form). Let   N (0, Ip) and  is any symmetric non-negative definite matrix, then it holds for any x>0
IP tr  - 1/2 2  2 x tr(2)  exp(-x).
Proof of Lemma A.10. It is sufficient to consider w.l.o.g. only the case of diagonal matrix  , since it can be represented as  = U diag{a1, . . . , ap}U for an orthogonal matrix U and the eigenvalues a1  · · ·  ap ; U   N (0, Ip) .

spokoiny, v. and zhilova, m.

43

By the exponential Chebyshev inequality it holds for µ > 0 ,  > 0

IP tr  - 1/2 2    exp(-µ/2)IE exp µ tr  - 1/2 2 /2 .

log IE exp

µ

tr  -

1/2 2

/2

1 2

p

{µaj - log(1 + ajµ)} ,

j=1

therefore





IP

tr  - 1/2 2  



exp

-

1 2

µ

+

p
{log(1 + ajµ) - µaj}

j=1

  exp - 1 µ - µ2
2

p

 a2j /2

j=1





p



 exp -2/ 4 a2j  .

 j=1 

If x := 2/ 4

p i=1

aj2

, then  = 2

x

p j=1

aj2

.

Proof of Theorem 2.4. Due to the bound (A.24) it holds for z  max{2, p} + C(p +

 x)/ n

with

probability

 1 - 5e-x

ab IP

2

L

ab(

ba )

-

L

ab()

>z

 IP ba

 ab()

ba ab > z + W(r0, x) +  (r0, x)

.

Let us introduce the random vector 0 d=ef (D0-1H02D0-1)1/2(Var )-1/2 . The bound (A.23) implies with probability  1 - e-x

tr (Var 0)-1/2 Var ab { ab()} (Var 0)-1/2 - Ip 2  pV4 (x).

(A.29)

Applying statement 2.2 of Theorem 5.1 to the vectors  ab() and 0 , we have with probability  1 - e-x

ab IP

 ba ()

ba ab > z + W(r0, x) +  (r0, x)

 IP ( 0 > z - W(r0, x)) - b, full

where



b,

full

d=ef +

1.55 n1/4 W(r0, x)

+ +

n1/4 + Wba (r0,

x)2p+V2(xab()r0,

x)

.

2

(A.30)

44 Bootstrap confidence sets under model misspecification

By the definition of 0 it holds 0   (Var )1/2(D0-1H02D0-1)-1/2 -1 . Consider the following matrix

V 2 d=ef (D0-1H02D0-1)-1/2(Var )(D0-1H02D0-1)-1/2 = (D0-1H02D0-1)1/2 D0H0-2V02H0-2D0 (D0-1H02D0-1)1/2  (D0-1H02D0-1)1/2 D0H0-2D0 (D0-1H02D0-1)1/2 = Ip,

(A.31)

here V02 d=ef Var{L()} ; the inequality (A.31) holds due to the definitions (1.7), (1.8) and V02 = H02 - B02 > 0 . Therefore V 2  1 and 0   . By (A.25)

ab IP

2

L

ba (

ab )

-

L

ba ()

>z

 IP (  > z - W(r0, x)) - b, full  IP 2 L() - L() > z - b, full

with probability  1 - 12e-x , which finishes the proof of the first part. For the second part let us introduce 0  N (0, D0-1H02D0-1) s.t. Var 0 = Var 0 . Applying statement 2.1 of Theorem 5.1 to the vectors  ab() and 0 , using the bound (A.29), we have with probability  1 - e-x

ba IP

 ba ()

ab ab > z + W(r0, x) +  (r0, x)

 IP 0 > z - G,1,

where

G,1

d=ef

1.55n1/4

+

Wab (r0,

x) + 

ba (r0,

x)

2

+



p 2

V2

(x).

By definition (A.20)   N (0, Var ) . Lemma A.10 and Theorem 1.2 by Spokoiny (2012b) imply

IP  - 0  tr(Var ) - tr(Var 0) + qf,1  2e-x, IP  - 0  tr(Var ) - tr(Var 0) - qf,2  2e-x,

(A.32)

spokoiny, v. and zhilova, m.

45

where

qf,1 d=ef 4x tr{(Var 0)2} 1/4 + max 2 2x tr{(Var )2}, 6x Var 

1/2
,

qf,2 d=ef 4x tr{(Var )2} 1/4 + max 2 2x tr{(Var 0)2}, 6x Var 0

1/2
.

(A.33)

By conditions (I) , (IB)

qf,1 

4xp(a2 + aB2 )

1/2
+ a max

qf,2  4xpa4 1/4 + a2 + aB2 max

1/2
8xp, 6x ,
1/2
8xp, 6x .

Further, it holds on a random set with probability  1 - 2e-x

(A.34)

IP 0 > z - G,1

= IP
(by (A.32))
 IP

 > z +  - 0 - G,1  > z + tr(Var ) - tr(Var 0) + qf,1 - G,1

(Th. 5.1)
 IP  > z - W(r0, x) + tr(Var ) - tr(Var 0) + qf,1

-G,1 - G,2

(Th. A.2)
 IP

2 L() - L() > z + tr(Var ) - tr(Var 0) + qf,1

- b, full,

where

G,2

d=ef

1.55n1/4 +

W(r0, x) , 2

b, full = G,1 + G,2.

Hence, we obtain

ab IP

2

L

ba (

ab )

-

L

ab()

>z

 IP 2 L() - L() > z + tr(Var ) - tr(Var 0) + qf,1 - b, full.

46 Bootstrap confidence sets under model misspecification

By definition (2.2) of (1 - ) -quantile z it holds:

z(+b, full)



ba z

+

tr(Var ) -

tr(Var 0) + qf,1,

and in addition

tr(Var ) -

tr(Var 0)



- 2

tr(D0-1B02D0-1) tr(D0-1H02D0-1)



0.

The inverse inequalities are implied with the similar arguments:

IP ab

2

L

ab(

ba )

-

L

ab()

>z

And

 IP 2 L() - L() > z + tr(Var ) - tr(Var 0) - qf,2 + b, full.

z(-b, full)



ba z

+

tr(Var ) -

tr(Var 0) - qf,2.

A.5 Proof of Theorem 2.5 (the smoothed version)
Lemma A.11. For the function g(x, z) defined in (2.6), all 1  [0, x] and all C  1 it holds

g(x - 1, z)  g(x, z + 1C)

Proof of Lemma A.11. By definition (5.9) of g(x)

max
x0

{g(x

-

1,

z)

=

0}

=

z

+

1,

max
x0

{g(x,

z

+

1 C )

=

0}

=

z

+

1C.

For x  z + 1C it holds

g(x - 1, z) = g

1 2z

(x - 1)2 - z2

g

1 2(z + 1C)

x2 - (z + 1C)2

= g(x, z + 1C).

(A.35)

spokoiny, v. and zhilova, m.

47

Indeed, the comparison in (A.35) reads as

(z + 1C)(x - 1 + z)(x - 1 - z)  z(x + z + 1C)(x - z - 1C).

(A.36)

Since C  1 , (x - 1 - z)  (x - 1C - z) and it holds for the left side of (A.36):
(z + 1C)(x - 1 + z) = (zx + z2 + 21C) + 1(xC - 1C - z)  (zx + z2 + 21C),

which is equal to the multiplier z(x + 1C + z) in right side.

Proposition A.12 (Smooth analog of Proposition A.9). If conditions (SmB) and
(SD1) are fulfilled, then it holds for all 0 <   0.22 and for all z, z > 2 s.t. |z - z|  z for some z  [0, 1] with probability  1 - e-x :

IEg (



,

z)

-

IE

ba g

(

 ba ()

, z)



16 3

n + n

+

 2p

z

z

+

 p

z2 z2

+

 p

V2 (x) 1-

+ V2

s2mb (x)



16 3

n + n

 4p + 5z + 3

V2 (x) + s2mb

for

z



 p,

V2 (x)  1/4.

(A.37)

Proof of Proposition A.12. The conditions of Theorem 5.5 are fulfilled with the value

  = p

V2 (x) + s2mb

/

1 - V2 (x)

due to the proof of Proposition A.9.

Proof of Theorem 2.5. The following holds on a random set of probability  1 - 12e-x :

ab IE g

2

L

ba (

ab )

-

L

ba ()

,z

(Th. A.4)


ab IE g

ba  ()

-

ba W(r0,

x),

z

(L. A.7)

(L. A.11)

(Prop. A.12)


ab IE g

 ab()

-

ab W(r0,

x)

-

ba  (r0,

x),

z

ab IE g

 ba ()

ba ba , z + W(r0, x) +  (r0, x)

IEg (  , z - W(r0, x)) - sm

(Th. A.2, L. A.11)
 IEg

2 L() - L() , z - sm,

48 Bootstrap confidence sets under model misspecification

where the term sm comes from (A.37) with z := W(r0, x) + Wba (r0, x) + ab(r0, x) :



sm d=ef163

n + n

4p +
3

V2 (x) + s2mb

ba ba

+ 5 W(r0, x) + W(r0, x) +  (r0, x) .

(A.38)

By the similar inequalities in the inverse direction we get the statement proved. Due to the arguments in the end of the proof of Theorem 2.1 it holds in the case 4.3

1 sm = C 3

(p + x)3 n

1/2

+

C

p+

x

 x

+

C

p+

x

.

nn

(A.39)

A.6 Bernstein matrix inequality

Consider the following symmetric p × p IP -random matrix and its expected value:

V2() d=ef Var ba (L ab()) = n  i() i() ,
i=1

n
H02 d=ef IEV2() = IE  i() i()
i=1

.

Matrix V2() equals to a sum of the independent random matrices  i() i() . Assuming the condition (SD1) to be fulfilled, we can refer to the result by Tropp (2012) in order to get the concentration bound below. Let us previously introduce some notations.

vi2() d=ef H0-1  i() i() - IE  i() i() H0-1,

then

n
H0-1V2()H0-1 = vi2().
i=1

Define also

n

v2 d=ef

IEvi4() .

i=1

Theorem A.13 (Bernstein inequality for V2() ). Let the condition (SD1) be fulfilled, then it holds with probability  1 - e-x :

H0-1V2()H0-1 - Ip  V2 (x),

spokoiny, v. and zhilova, m.

where the error term is defined as

V2 (x) d=ef

2v2

{log(p)

+

x}

+

2 3

v2

{log(p)

+

x}

and is proportional to {log(p) + x}/n in the case 4.3.

Proof. Due to Theorem 1.4 by Tropp (2012):

For it holds:

IP

H0-1V2()H0-1 - Ip  t  p exp

-t2 2v2 + 2v2t/3

.

x

=

2v2

t2 + 2v2t/3

-

log(p)

IP H0-1V2()H0-1 - Ip  V2 (x)  e-x.

49

References
Aerts, M. and Claeskens, G. (2001). Bootstrap tests for misspecified models, with application to clustered binary data. Computational statistics & data analysis, 36(3):383­401.
Arlot, S., Blanchard, G., and Roquain, E. (2010). Some nonasymptotic results on resampling in high dimension. I. Confidence regions. Ann. Statist., 38(1):51­82.
Barbe, P. and Bertail, P. (1995). The weighted bootstrap, volume 98. Springer.
Bentkus, V. (2003). On the dependence of the Berry­Esseen bound on dimension. Journal of Statistical Planning and Inference, 113(2):385­402.
Berry, A. C. (1941). The accuracy of the Gaussian approximation to the sum of independent variates. Transactions of the american mathematical society, 49(1):122­136.
Bhattacharya, R. and Holmes, S. (2010). An exposition of G¨otze's estimation of the rate of convergence in the multivariate central limit theorem. arXiv preprint arXiv:1003.4254.
Chatterjee, S. and Bose, A. (2005). Generalized bootstrap for estimating equations. The Annals of Statistics, 33(1):414­436.
Chen, L. H. and Fang, X. (2011). Multivariate normal approximation by Stein's method: The concentration inequality approach. arXiv preprint arXiv:1111.4073.

50 Bootstrap confidence sets under model misspecification
Chen, X. and Pouzo, D. (2009). Efficient estimation of semiparametric conditional moment models with possibly nonsmooth residuals. Journal of Econometrics, 152(1):46­ 60.
Chen, X. and Pouzo, D. (2014). Sieve Wald and QLR Inferences on semi/nonparametric conditional moment models.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2013). Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors. The Annals of Statistics, 41(6):2786­2819.
Efron, B. (1979). Bootstrap methods: Another look at the jackknife. The Annals of Statistics, 7(1):1­26.
Esseen, C.-G. (1942). On the Liapounoff limit of error in the theory of probability. Almqvist & Wiksell.
Esseen, C. G. (1956). A moment inequality with an application to the central limit theorem. Scandinavian Actuarial Journal, 1956(2):160­170.
Go¨tze, F. (1991). On the rate of convergence in the multivariate CLT. The Annals of Probability, pages 724­739.
G¨otze, F. and Zaitsev, A. Y. (2014). Explicit rates of approximation in the CLT for quadratic forms. The Annals of Probability, 42(1):354­397.
Hall, A. R. (2005). Generalized method of moments. Oxford University Press Oxford.
Hall, P. (1992). The bootstrap and Edgeworth expansion. Springer.
Horowitz, J. L. (2001). The bootstrap. Handbook of econometrics, 5:3159­3228.
Huber, P. (1967). The behavior of maximum likelihood estimates under nonstandard conditions. Proc. 5th Berkeley Symp. Math. Stat. Probab., Univ. Calif. 1965/66, 1, 221-233 (1967).
Janssen, A. and Pauls, T. (2003). How do bootstrap and permutation tests work? Annals of statistics, pages 768­806.
Kline, P. and Santos, A. (2012). Higher order properties of the wild bootstrap under misspecification. Journal of Econometrics, 171(1):54­70.
Lavergne, P. and Patilea, V. (2013). Smooth minimum distance estimation and testing with conditional estimating equations: uniform in bandwidth theory. Journal of Econometrics, 177(1):47­59.

spokoiny, v. and zhilova, m.

51

Lindeberg, J. W. (1922). Eine neue Herleitung des Exponentialgesetzes in der Wahrscheinlichkeitsrechnung. Mathematische Zeitschrift, 15(1):211­225.
Ma, S. and Kosorok, M. R. (2005). Robust semiparametric M-estimation and the weighted bootstrap. Journal of Multivariate Analysis, 96(1):190­217.
Mammen, E. (1992). When does bootstrap work?, volume 77. Springer.
Mammen, E. (1993). Bootstrap and wild bootstrap for high dimensional linear models. The Annals of Statistics, pages 255­285.
Newton, M. A. and Raftery, A. E. (1994). Approximate bayesian inference with the weighted likelihood bootstrap. Journal of the Royal Statistical Society. Series B (Methodological), pages 3­48.
Prokhorov, Y. V. and Ulyanov, V. V. (2013). Some approximation problems in statistics and probability. In Limit Theorems in Probability, Statistics and Number Theory, pages 235­249. Springer.
Shevtsova, I. (2010). An improvement of convergence rate estimates in the Lyapunov theorem. In Doklady Mathematics, volume 82, pages 862­864. Springer.
Spokoiny, V. (2012a). Parametric estimation. Finite sample theory. The Annals of Statistics, 40(6):2877­2909.
Spokoiny, V. (2012b). Supplement to "Parametric estimation. Finite sample theory".
Spokoiny, V. (2013). Bernstein-von Mises Theorem for growing parameter dimension. arXiv preprint arXiv:1302.3430.
Tropp, J. A. (2012). User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389­434.
Tsybakov, A. B. (2009). Introduction to Nonparametric Estimation. Springer, New York.
van ver Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence and Empirical processes. Springer, New York.
Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hypotheses. The Annals of Mathematical Statistics, 9(1):60­62.
Wu, C. F. J. (1986). Jackknife, bootstrap and other resampling methods in regression analysis. The Annals of Statistics, 14(4):1261­1295+.

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Principal Component Analysis in an Asymmetric Norm" by Ngoc Mai Tran, Maria Osipenko and Wolfgang Karl Härdle, January 2014.
002 "A Simultaneous Confidence Corridor for Varying Coefficient Regression with Sparse Functional Data" by Lijie Gu, Li Wang, Wolfgang Karl Härdle and Lijian Yang, January 2014.
003 "An Extended Single Index Model with Missing Response at Random" by Qihua Wang, Tao Zhang, Wolfgang Karl Härdle, January 2014.
004 "Structural Vector Autoregressive Analysis in a Data Rich Environment: A Survey" by Helmut Lütkepohl, January 2014.
005 "Functional stable limit theorems for efficient spectral covolatility estimators" by Randolf Altmeyer and Markus Bibinger, January 2014.
006 "A consistent two-factor model for pricing temperature derivatives" by Andreas Groll, Brenda López-Cabrera and Thilo Meyer-Brandis, January 2014.
007 "Confidence Bands for Impulse Responses: Bonferroni versus Wald" by Helmut Lütkepohl, Anna Staszewska-Bystrova and Peter Winker, January 2014.
008 "Simultaneous Confidence Corridors and Variable Selection for Generalized Additive Models" by Shuzhuan Zheng, Rong Liu, Lijian Yang and Wolfgang Karl Härdle, January 2014.
009 "Structural Vector Autoregressions: Checking Identifying Long-run Restrictions via Heteroskedasticity" by Helmut Lütkepohl and Anton Velinov, January 2014.
010 "Efficient Iterative Maximum Likelihood Estimation of HighParameterized Time Series Models" by Nikolaus Hautsch, Ostap Okhrin and Alexander Ristig, January 2014.
011 "Fiscal Devaluation in a Monetary Union" by Philipp Engler, Giovanni Ganelli, Juha Tervala and Simon Voigts, January 2014.
012 "Nonparametric Estimates for Conditional Quantiles of Time Series" by Jürgen Franke, Peter Mwita and Weining Wang, January 2014.
013 "Product Market Deregulation and Employment Outcomes: Evidence from the German Retail Sector" by Charlotte Senftleben-König, January 2014.
014 "Estimation procedures for exchangeable Marshall copulas with hydrological application" by Fabrizio Durante and Ostap Okhrin, January 2014.
015 "Ladislaus von Bortkiewicz - statistician, economist, and a European intellectual" by Wolfgang Karl Härdle and Annette B. Vogt, February 2014.
016 "An Application of Principal Component Analysis on Multivariate TimeStationary Spatio-Temporal Data" by Stephan Stahlschmidt, Wolfgang Karl Härdle and Helmut Thome, February 2014.
017 "The composition of government spending and the multiplier at the Zero Lower Bound" by Julien Albertini, Arthur Poirier and Jordan RoulleauPasdeloup, February 2014.
018 "Interacting Product and Labor Market Regulation and the Impact of Immigration on Native Wages" by Susanne Prantl and Alexandra SpitzOener, February 2014.
SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasrecahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
019 "Unemployment benefits extensions at the zero lower bound on nominal interest rate" by Julien Albertini and Arthur Poirier, February 2014.
020 "Modelling spatio-temporal variability of temperature" by Xiaofeng Cao, Ostap Okhrin, Martin Odening and Matthias Ritter, February 2014.
021 "Do Maternal Health Problems Influence Child's Worrying Status? Evidence from British Cohort Study" by Xianhua Dai, Wolfgang Karl Härdle and Keming Yu, February 2014.
022 "Nonparametric Test for a Constant Beta over a Fixed Time Interval" by Markus Reiß, Viktor Todorov and George Tauchen, February 2014.
023 "Inflation Expectations Spillovers between the United States and Euro Area" by Aleksei Netsunajev and Lars Winkelmann, March 2014.
024 "Peer Effects and Students' Self-Control" by Berno Buechel, Lydia Mechtenberg and Julia Petersen, April 2014.
025 "Is there a demand for multi-year crop insurance?" by Maria Osipenko, Zhiwei Shen and Martin Odening, April 2014.
026 "Credit Risk Calibration based on CDS Spreads" by Shih-Kang Chao, Wolfgang Karl Härdle and Hien Pham-Thu, May 2014.
027 "Stale Forward Guidance" by Gunda-Alexandra Detmers and Dieter Nautz, May 2014.
028 "Confidence Corridors for Multivariate Generalized Quantile Regression" by Shih-Kang Chao, Katharina Proksch, Holger Dette and Wolfgang Härdle, May 2014.
029 "Information Risk, Market Stress and Institutional Herding in Financial Markets: New Evidence Through the Lens of a Simulated Model" by Christopher Boortz, Stephanie Kremer, Simon Jurkatis and Dieter Nautz, May 2014.
030 "Forecasting Generalized Quantiles of Electricity Demand: A Functional Data Approach" by Brenda López Cabrera and Franziska Schulz, May 2014.
031 "Structural Vector Autoregressions with Smooth Transition in Variances ­ The Interaction Between U.S. Monetary Policy and the Stock Market" by Helmut Lütkepohl and Aleksei Netsunajev, June 2014.
032 "TEDAS - Tail Event Driven ASset Allocation" by Wolfgang Karl Härdle, Sergey Nasekin, David Lee Kuo Chuen and Phoon Kok Fai, June 2014.
033 "Discount Factor Shocks and Labor Market Dynamics" by Julien Albertini and Arthur Poirier, June 2014.
034 "Risky Linear Approximations" by Alexander Meyer-Gohde, July 2014 035 "Adaptive Order Flow Forecasting with Multiplicative Error Models" by
Wolfgang Karl Härdle, Andrija Mihoci and Christopher Hian-Ann Ting, July 2014 036 "Portfolio Decisions and Brain Reactions via the CEAD method" by Piotr Majer, Peter N.C. Mohr, Hauke R. Heekeren and Wolfgang K. Härdle, July 2014 037 "Common price and volatility jumps in noisy high-frequency data" by Markus Bibinger and Lars Winkelmann, July 2014 038 "Spatial Wage Inequality and Technological Change" by Charlotte Senftleben-König and Hanna Wielandt, August 2014 039 "The integration of credit default swap markets in the pre and postsubprime crisis in common stochastic trends" by Cathy Yi-Hsuan Chen, Wolfgang Karl Härdle, Hien Pham-Thu, August 2014
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
040 "Localising Forward Intensities for Multiperiod Corporate Default" by Dedy Dwi Prastyo and Wolfgang Karl Härdle, August 2014.
041 "Certification and Market Transparency" by Konrad Stahl and Roland Strausz, September 2014.
042 "Beyond dimension two: A test for higher-order tail risk" by Carsten Bormann, Melanie Schienle and Julia Schaumburg, September 2014.
043 "Semiparametric Estimation with Generated Covariates" by Enno Mammen, Christoph Rothe and Melanie Schienle, September 2014.
044 "On the Timing of Climate Agreements" by Robert C. Schmidt and Roland Strausz, September 2014.
045 "Optimal Sales Contracts with Withdrawal Rights" by Daniel Krähmer and Roland Strausz, September 2014.
046 "Ex post information rents in sequential screening" by Daniel Krähmer and Roland Strausz, September 2014.
047 "Similarities and Differences between U.S. and German Regulation of the Use of Derivatives and Leverage by Mutual Funds ­ What Can Regulators Learn from Each Other?" by Dominika Paula Galkiewicz, September 2014.
048 "That's how we roll: an experiment on rollover risk" by Ciril Bosch-Rosa, September 2014.
049 "Comparing Solution Methods for DSGE Models with Labor Market Search" by Hong Lan, September 2014.
050 "Volatility Modelling of CO2 Emission Allowance Spot Prices with RegimeSwitching GARCH Models" by Thijs Benschop, Brenda López Cabrera, September 2014.
051 "Corporate Cash Hoarding in a Model with Liquidity Constraints" by Falk Mazelis, September 2014.
052 "Designing an Index for Assessing Wind Energy Potential" by Matthias Ritter, Zhiwei Shen, Brenda López Cabrera, Martin Odening, Lars Deckert, September 2014.
053 "Improved Volatility Estimation Based On Limit Order Books" by Markus Bibinger, Moritz Jirak, Markus Reiss, September 2014.
054 "Strategic Complementarities and Nominal Rigidities" by Philipp König, Alexander Meyer-Gohde, October 2014.
055 "Estimating the Spot Covariation of Asset Prices ­ Statistical Theory and Empirical Evidence" by Markus Bibinger, Markus Reiss, Nikolaus Hautsch, Peter Malec, October 2014.
056 "Monetary Policy Effects on Financial Intermediation via the Regulated and the Shadow Banking Systems" by Falk Mazelis, October 2014.
057 "A Tale of Two Tails: Preferences of neutral third-parties in three-player ultimatum games" by Ciril Bosch-Rosa, October 2014.
058 "Boiling the frog optimally: an experiment on survivor curve shapes and internet revenue" by Christina Aperjis, Ciril Bosch-Rosa, Daniel Friedman, Bernardo A. Huberman, October 2014.
059 "Expectile Treatment Effects: An efficient alternative to compute the distribution of treatment effects" by Stephan Stahlschmidt, Matthias Eckardt, Wolfgang K. Härdle, October 2014.
060 "Are US Inflation Expectations Re-Anchored?" by Dieter Nautz, Till Strohsal, October 2014.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
061 "Why the split of payroll taxation between firms and workers matters for macroeconomic stability" by Simon Voigts, October 2014.
062 "Do Tax Cuts Increase Consumption? An Experimental Test of Ricardian Equivalence" by Thomas Meissner, Davud Rostam-Afschar, October 2014.
063 "The Influence of Oil Price Shocks on China's Macro-economy : A Perspective of International Trade" by Shiyi Chen, Dengke Chen, Wolfgang K. Härdle, October 2014.
064 "Whom are you talking with? An experiment on credibility and communication structure" by Gilles Grandjean, Marco Mantovani, Ana Mauleon, Vincent Vannetelbosch, October 2014.
065 "A Theory of Price Adjustment under Loss Aversion" by Steffen Ahrens, Inske Pirschel, Dennis J. Snower, November 2014.
066 "TENET: Tail-Event driven NETwork risk" by Wolfgang Karl Härdle, Natalia Sirotko-Sibirskaya, Weining Wang, November 2014.
067 "Bootstrap confidence sets under model misspecification" by Vladimir Spokoiny, Mayya Zhilova, November 2014.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

